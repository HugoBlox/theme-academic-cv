---
title: "Wait Times at Walt Disney World"
author: 'Mikayla'
date: '2019-04-22'
slug: wait-times-at-walt-disney-world
tags: []
categories: []
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
The data used for this analysis was originally published by [Touring Plans](https://touringplans.com/blog/2018/06/25/disney-world-wait-times-available-for-data-science-and-machine-learning/). 

I used the data for six rides: two from the Magic Kingdom (Seven Dwarfs Mine Train & Splash Mountain), two from Hollywood Studios (Toy Story Mania & The Rockin Rollercoaster), and two from Epcot (Soarin & Spaceship Earth).

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(lubridate)
library(corrplot)
library(caret)
library(skimr)
library(ModelMetrics)
library(janitor)
library(broom)
library(forcats)
library(kableExtra)
```

There was a separate csv file for each ride. I originally merged & cleaned this data using Python. I then created a new file (merged_rides), which I load here. I also load the metadata file, which contains additional information about each observed date (e.g. holidays, WDW events, etc.). This will later be merged with the ride wait time data for modeling.

```{r,warning=FALSE,message=FALSE}
meta<-read_csv("metadata.csv")
merged_rides<- read_csv("merged_rides_.csv")
merged_rides$datetime <- as_datetime(merged_rides$datetime)
```

```{r}
kable(head(merged_rides)) %>%   kable_styling(font_size = 9,bootstrap_options = "striped", full_width = F)
```

```{r}
merged_rides %>% gather(key="ride",value="wait_time",-datetime) %>% 
ggplot(aes(ride, wait_time, group = ride)) +
  geom_boxplot(colour = "black", fill = "#56B4E9",notch = T) +
  ylab("Wait time")
```
This shows the distribution of wait times by ride. We see, for example, that Seven Dwarfs has the highest median wait time, while Spaceship Earth has the lowest.
```{r}
rides1<-merged_rides%>%
  mutate(hour=hour(datetime),
         date=date(datetime)) %>%
  group_by(hour,date)%>% filter(hour<21 & hour>8) %>%
  summarize_at(
    vars(toy_story_mania:soarin),
    ~mean(.,na.rm =TRUE)
  )
avg_by_month <- rides1 %>%
  mutate(month = month(date, label = TRUE)) %>%
  group_by(month) %>%
  summarize_at(vars(toy_story_mania:soarin),
  ~ mean(., na.rm = TRUE))

avg_by_month %>% gather(key="ride",value="avg_wait",-month,) %>% 
ggplot(aes(x=month,y=avg_wait,color=ride)) + geom_line(aes(group = ride)) +
geom_point() + xlab("Month")
```
All rides have the lowest average wait in September. Splash Mountain sees the largest decline. 
```{r,message=FALSE,warning=FALSE}
rides1 %>%
  keep(is.numeric) %>% 
  gather(key="ride",value="wait",-hour) %>% 
ggplot(aes(x = wait, fill = as.factor(ride))) + 
  geom_density(alpha = .3)+xlim(5,220)+ labs(title="Wait Time Dist.")+scale_fill_discrete(name="Ride")
```
Spaceship Earth's distribution appeairs skewed leftward because wait times are consistenly low.

Now, we bring in the metadata file to examine additional features that can be used as we build various models to predict the ride wait times. 
```{r}
##selecting features to explore
meta1<-meta %>% select(DATE, contains('event'),-contains('eventN'),contains('HOLIDAY'),-contains('DAYS'),-contains('STREAK'),-HOLIDAYJ)

meta1$date<-as.character(meta1$DATE)
meta1$date <-ymd(mdy(meta1$date))
meta1$DATE<-NULL
meta1<-meta1 %>% clean_names()  
meta1$holidayn<-fct_explicit_na(meta1$holidayn, "no holiday")
```

```{r}
mrides<-rides1 %>% 
merge(meta1,by="date")%>% 
arrange(date,hour) %>% mutate(
  hour=as.factor(hour),  
  month=as.factor(month(date)),
  year=as.factor(year(date))) 

kable(head(mrides)) %>% kable_styling(bootstrap_options = "striped", full_width = F,font_size = 8)
```
With this data, we can look at how ride wait times are affected by holidays. 
```{r}
mrides %>% group_by(holiday,hour) %>% summarize_at(
  vars(toy_story_mania:soarin),
  ~mean(.)
) %>% 
  gather(key="ride",value="wait_time",toy_story_mania:soarin) %>% 
  ggplot(aes(x=hour,y=wait_time,color=as.logical(holiday)))+geom_line(aes(group=holiday))+facet_wrap(~ride,scales = "free_x")
```
This shows that the hourly average wait is higher on holidays but has a similar slope. 

#### Part I. Linear Model for Each Ride
Using the tidyverse list-column workflow, we can build a separate model for each ride within the same dataframe.  
```{r}
rides_nest<-mrides %>% gather(key="ride",value="wait_time",toy_story_mania:soarin)%>%  group_by(ride) %>% nest()
head(rides_nest)
```
The data column shown above is a <list> where each element is a tibble that stores the data corresponding to the ride variable. 
Now, we append a "model" variable to the data tibble for each ride. Using the broom package, we can easily extract model results. The "tidy" function, for example, retrieves the coefficient values of the linear model. 
```{r}
##building model for each ride
nest_model<-rides_nest %>% 
  mutate(model=map(data,~lm(formula =wait_time~.,data=.x)))

##retrieve coef         
coef<-nest_model %>% 
  mutate(coef=map(model,~tidy(.x))) %>% 
  unnest(coef)

coef %>% select(ride:estimate) %>% filter(term!='(Intercept)') %>%  group_by(ride) %>% top_n(5,estimate) %>%  ggplot(aes(x=term,y=estimate))+geom_bar(stat="identity") + facet_wrap(~ride,scales="free_y") +
  coord_flip()
```
This shows the variables with the highest coefficient estimate for each ride. We can see that "nye" (New Years Eve) is in the top 5 for every ride. For seven dwarfs, the mid-day hours are particularly significant. 

We can also compare different holidays, such as New Years Eve, Christmas Eve, and Independence Day. 
```{r,message=FALSE,warning=FALSE}
mrides %>% 
  filter(stringr::str_detect(holidayn, "nye|cme|ind") ) %>% gather(key="ride",value="wait_time",toy_story_mania:soarin) %>% group_by(ride,hour,holidayn) %>% summarize(avg_wait=mean(wait_time)) %>% 
  ggplot(aes(x=hour,y=avg_wait,group=holidayn,color=holidayn))+geom_line(aes(group=holidayn))+facet_wrap(~ride,scales="free_y","free_x")
```
NYE seems to have the highest wait times for all rides. This is consistent with the coefficient importance results shown above. For some rides,the wait time difference is more significant(e.g.soarin).This is also reflected in the coefficient estimate above. For splash mountain, however, independence day (July 4th) times are also very high. This makes sense given that it's a water ride whose popularity we would expect to be highest in summer. 

Now, we can compare the model performance for each ride. Broom has a "glance" function that provides a data frame with model performance statistics and an "augment" function through which we can derive model predictions. 
Here, I compare the mean absolute error (mae) for each ride model. 
```{r}
model_perf<-nest_model %>% 
  mutate(
    fit=map(model,~glance(.x)),
    augmented=map(model,~augment(.x))
  )

aug <- model_perf %>%
  unnest(augmented)
  ##calc mae for each ride model
  aug_mae <- aug %>% group_by(ride) %>%
  transmute(mae = mae(wait_time, .fitted)) %>% distinct()
  
  aug_mae %>% ggplot(aes(x = ride, y = mae)) + geom_segment(aes(
  x = ride ,
  xend = ride,
  y = 0,
  yend = mae
  ), color = "black") +
  geom_point(size = 3, color = "deepskyblue") + geom_text(aes(label = round(mae,2), y = mae + 0.75),
  position = position_dodge(0.9),
  vjust = 0) + coord_flip()
  
```
The highest mae (mean absolute error) is observed for seven dwarfs. Spaceship Earth has the lowest, which makes sense given its minimal variation. We can interpret the mae as being the average size of the prediction error (i.e. the absolute difference between the model prediction and the actual wait time). So the seven dwarfs model was off, on average, by about 17 minutes. 


```{r}
ggplot(aug,aes(wait_time,.fitted))+geom_smooth()+geom_abline(intercept=0,slope=1)+facet_wrap(~ride)
```
By plotting our predictions (y axis) against the actual time (x axis), we can see that out models were generally much better at predicting low wait times. They did not accurately capture the high wait times. This is particularly true for splash mountain, where our predictions were very low (50-75) despite high observed wait times (200-300).

#### Part II. XGboost Model
Instead of building a separate model for each ride, we could instead have a single model that uses ride as a feature in the dataset.  Here, I will use eXtreme Gradient Boosting (XGboost), an implementation of the gradient boosted decision tree algorithm. To give a short explanation, gradient boosting is an ensemble technique that sequentially creates new models to predict the residuals (errors) of previous models. The gradient descent algorithm is used to minimize the loss between subsequent models. The residuals are added together when the model makes a final prediction. 
```{r,message=FALSE}
library (xgboost)
set.seed(1234)

#select numeric features
mrides_long<-mrides %>% gather(key="ride",value="wait_time",toy_story_mania:soarin) %>% mutate_if(is.factor,as.character) 
mrides_long_numeric<-mrides_long %>% select(-date) %>% select_if(is.numeric)


#get vector of training labels
waitLabels <- mrides_long_numeric %>% mutate(wait_time=as.numeric(wait_time)) %>% select(wait_time)
waitLabels<-as.vector(waitLabels$wait_time)

#remove outcome variable
mrides_long_numeric<-mrides_long_numeric %>% select(-wait_time)

#one-hot encoding for categorical variables (holidayn,hour,month, etc. )
holiday_name <- model.matrix(~holidayn-1, mrides_long)
hour_mat <- model.matrix(~hour-1, mrides_long)
mon_mat <- model.matrix(~month-1, mrides_long)
year_mat <- model.matrix(~year-1, mrides_long)
ride_mat<- model.matrix(~ride-1, mrides_long)

#column bind existing numeric data with one-hot encoded categorical variables
mrides_long<- cbind(mrides_long_numeric, holiday_name,hour_mat,mon_mat,year_mat,ride_mat) 

#convert to numeric & create matrix
mrides_long<-mrides_long %>% mutate_if(is.integer,as.numeric)
mrides_matrix <- data.matrix(mrides_long)
```
To accurately measure the performance of our model, we can split the data to create training and testing sets. Below, I use a 70/30 train/test split, which means that 70% of the data is used to train the model. 
```{r}
# get the 70/30 training test split
numberOfTrainingSamples <- round(length(waitLabels) * .7)

# training data
train_data <- mrides_matrix[1:numberOfTrainingSamples,]
train_labels <- waitLabels[1:numberOfTrainingSamples]

# testing data
test_data <- mrides_matrix[-(1:numberOfTrainingSamples),]
test_labels <- waitLabels[-(1:numberOfTrainingSamples)]

# put testing & training data into seperate Dmatrix objects
dtrain <- xgb.DMatrix(data = train_data, label= train_labels)
dtest <- xgb.DMatrix(data = test_data, label= test_labels)
```

```{r}
xgb <- xgboost(data = dtrain,   
                nrounds = 200,print_every_n = 50, early_stop_round = 20, verbose = 1)

#model prediction
xgbpred <- predict (xgb,dtest)
xgb_mae<- mae(test_labels, xgbpred)
print(xgb_mae)
```

```{r}
## Plot the feature importance
importance_matrix <- xgb.importance(colnames(dtrain), model = xgb)
xgb.plot.importance(importance_matrix = importance_matrix[1:20])
```

