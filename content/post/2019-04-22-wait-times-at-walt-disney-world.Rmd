---
title: "Wait Times at Walt Disney World"
author: 'Mikayla'
date: '2019-04-22'
slug: wait-times-at-walt-disney-world
tags: []
categories: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
The data used for this analysis was originally published by [Touring Plans](https://touringplans.com/blog/2018/06/25/disney-world-wait-times-available-for-data-science-and-machine-learning/). 

I used the data for six rides: two from the Magic Kingdom (Seven Dwarfs Mine Train & Splash Mountain), two from Hollywood Studios (Toy Story Mania & The Rockin Rollercoaster), and two from Epcot (Soarin & Spaceship Earth).

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(lubridate)
library(skimr)
library(ModelMetrics)
library(janitor)
library(broom)
library(forcats)
library(kableExtra)
library(stringr)
```

There was a separate csv file for each ride. I originally merged & cleaned this data using Python, which can be found [here](https://github.com/mikaylaedwards/Disney-wait-times/blob/master/wait%20comparison.ipynb). 
I then created a new file (merged_rides), which I load below. I also load the metadata file, which contains additional information about each observed date (e.g. holidays, WDW events, etc.). This will later be merged with the ride wait time data for modeling.

```{r,warning=FALSE,message=FALSE}
meta<-read_csv("metadata.csv")
merged_rides<- read_csv("merged_rides_.csv")
merged_rides$datetime <- as_datetime(merged_rides$datetime)
```

```{r}
kable(head(merged_rides)) %>%   kable_styling(font_size = 9,bootstrap_options = "striped", full_width = F)
```

```{r}
merged_rides %>% gather(key="ride",value="wait_time",-datetime) %>% 
ggplot(aes(ride, wait_time, group = ride)) +
  geom_boxplot(colour = "black", fill = "#56B4E9",notch = T) +
  ylab("Wait time")
```
This shows the distribution of wait times by ride. We see, for example, that Seven Dwarfs has the highest median wait time, while Spaceship Earth has the lowest.
```{r}
rides1<-merged_rides%>%
  mutate(hour=hour(datetime),
         date=date(datetime)) %>%
  group_by(hour,date)%>% filter(hour<21 & hour>8) %>%
  summarize_at(
    vars(toy_story_mania:soarin),
    ~mean(.,na.rm =TRUE)
  )
avg_by_month <- rides1 %>%
  mutate(month = month(date, label = TRUE)) %>%
  group_by(month) %>%
  summarize_at(vars(toy_story_mania:soarin),
  ~ mean(., na.rm = TRUE))

avg_by_month %>% gather(key="ride",value="avg_wait",-month,) %>% 
ggplot(aes(x=month,y=avg_wait,color=ride)) + geom_line(aes(group = ride)) +
geom_point() + xlab("Month")
```
All rides have the lowest average wait in September. Splash Mountain sees the largest decline. 
```{r,message=FALSE,warning=FALSE}
rides1 %>%
  keep(is.numeric) %>% 
  gather(key="ride",value="wait",-hour) %>% 
ggplot(aes(x = wait, fill = as.factor(ride))) + 
  geom_density(alpha = .3)+xlim(5,220)+ labs(title="Wait Time Dist.")+scale_fill_discrete(name="Ride")
```
Spaceship Earth's distribution appeairs skewed leftward because wait times are consistenly low.

Now, we bring in the metadata file to examine additional features that can be used as we build various models to predict the ride wait times. 
```{r}
##selecting features to explore
meta1<-meta %>% select(DATE, contains('event'),-contains('eventN'),contains('HOLIDAY'),-contains('DAYS'),-contains('STREAK'),-HOLIDAYJ)

meta1$date<-as.character(meta1$DATE)
meta1$date <-ymd(mdy(meta1$date))
meta1$DATE<-NULL
meta1<-meta1 %>% clean_names()  
meta1$holidayn<-fct_explicit_na(meta1$holidayn, "no_holiday")
```

```{r}
mrides<-rides1 %>% 
merge(meta1,by="date")%>% 
arrange(date,hour) %>% mutate(
  hour=as.factor(hour),  
  month=as.factor(month(date)),
  year=as.factor(year(date))) 

kable(head(mrides)) %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,font_size = 8)
```
With this data, we can look at how ride wait times are affected by holidays. 
```{r}
mrides %>% group_by(holiday,hour) %>% summarize_at(
  vars(toy_story_mania:soarin),
  ~mean(.)
) %>% 
  gather(key="ride",value="wait_time",toy_story_mania:soarin) %>% 
  ggplot(aes(x=hour,y=wait_time,color=as.logical(holiday)))+geom_line(aes(group=holiday))+facet_wrap(~ride,scales = "free_x")
```
This shows that the hourly average wait is higher on holidays but has a similar slope. 

#### Part I. Linear Model for Each Ride
Using the tidyverse list-column workflow, we can build a separate model for each ride within the same dataframe.  
```{r}
rides_nest<-mrides %>% gather(key="ride",value="wait_time",toy_story_mania:soarin)%>%
  group_by(ride) %>% nest()
head(rides_nest)
```
The data column shown above is a list where each element is a tibble that stores the data corresponding to the ride variable. 
Now, we append a "model" variable to the data tibble for each ride. Using the broom package, we can easily extract model results. The "tidy" function, for example, retrieves the coefficient values of the linear model. 
```{r}
##building model for each ride
nest_model<-rides_nest %>% 
  mutate(model=map(data,~lm(formula =wait_time~.,data=.x)))

##retrieve coef         
coef<-nest_model %>% 
  mutate(coef=map(model,~tidy(.x))) %>% 
  unnest(coef)

coef %>% select(ride:estimate) %>% filter(term!='(Intercept)') %>%  group_by(ride) %>% top_n(5,estimate) %>%  ggplot(aes(x=term,y=estimate))+geom_bar(stat="identity") + facet_wrap(~ride,scales="free_y") +
  coord_flip()
```
This shows the variables with the highest coefficient estimate for each ride. We can see that "nye" (New Years Eve) is in the top 5 for every ride. For seven dwarfs, the mid-day hours are particularly significant. 

We can also compare different holidays, such as New Years Eve, Christmas Eve, and Independence Day. 
```{r,message=FALSE,warning=FALSE}
mrides %>% 
  filter(stringr::str_detect(holidayn, "nye|cme|ind") ) %>% gather(key="ride",value="wait_time",toy_story_mania:soarin) %>% group_by(ride,hour,holidayn) %>% summarize(avg_wait=mean(wait_time)) %>% 
  ggplot(aes(x=hour,y=avg_wait,group=holidayn,color=holidayn))+geom_line(aes(group=holidayn))+facet_wrap(~ride,scales="free_y","free_x")
```
NYE seems to have the highest wait times for all rides. This is consistent with the coefficient importance results shown above. For some rides,the wait time difference is more significant(e.g.soarin).This is also reflected in the coefficient estimate above. For splash mountain, however, independence day (July 4th) times are also very high. This makes sense given that it's a water ride whose popularity we would expect to be highest in summer. 

Now, we can compare the model performance for each ride. Broom has a "glance" function that provides a data frame with model performance statistics and an "augment" function through which we can derive model predictions. 
Here, I compare the mean absolute error (mae) for each ride model. 
```{r}
model_perf<-nest_model %>% 
  mutate(
    fit=map(model,~glance(.x)),
    augmented=map(model,~augment(.x))
  )

aug <- model_perf %>%
  unnest(augmented)
  ##calc mae for each ride model
  aug_mae <- aug %>% group_by(ride) %>%
  transmute(mae = mae(wait_time, .fitted)) %>% distinct()
  
  aug_mae %>% ggplot(aes(x = ride, y = mae)) + geom_segment(aes(
  x = ride ,
  xend = ride,
  y = 0,
  yend = mae
  ), color = "black") +
  geom_point(size = 3, color = "deepskyblue") + geom_text(aes(label = round(mae,2), y = mae + 0.75),
  position = position_dodge(0.9),
  vjust = 0) + coord_flip()
  
```
The highest mae (mean absolute error) is observed for seven dwarfs. Spaceship Earth has the lowest, which makes sense given its minimal variation. We can interpret the mae as being the average size of the prediction error (i.e. the absolute difference between the model prediction and the actual wait time). So the seven dwarfs model was off, on average, by about 17 minutes. 


```{r}
ggplot(aug,aes(wait_time,.fitted))+geom_smooth()+geom_abline(intercept=0,slope=1)+facet_wrap(~ride)
```
By plotting our predictions (y axis) against the actual time (x axis), we can see that out models were generally much better at predicting low wait times. They did not accurately capture the high wait times. This is particularly true for splash mountain, where our predictions were very low (50-75) despite high observed wait times (200-300).

#### Part II. XGboost Model
Instead of building a separate model for each ride, we could instead have a single model that uses ride as a feature in the dataset.  Here, I will use eXtreme Gradient Boosting (XGboost), an implementation of the gradient boosted decision tree algorithm.  To give a short explanation, gradient boosting is an ensemble technique that sequentially creates new models to predict the residuals (errors) of previous models. The gradient descent algorithm is used to minimize the loss between subsequent models. The residuals are added together when the model makes a final prediction. 

```{r,message=FALSE}
library (xgboost)
set.seed(1234)

#select numeric features
mrides_long<-mrides %>% gather(key="ride",value="wait_time",toy_story_mania:soarin) %>% mutate_if(is.factor,as.character) 
mrides_long_numeric<-mrides_long %>% select(-date) %>% select_if(is.numeric)


#get vector of training labels
waitLabels <- mrides_long_numeric %>% mutate(wait_time=as.numeric(wait_time)) %>% select(wait_time)
waitLabels<-as.vector(waitLabels$wait_time)

#one-hot encoding for categorical variables (holidayn,hour,month, etc. )
holiday_name <- model.matrix(~holidayn-1, mrides_long)
hour_mat <- model.matrix(~hour-1, mrides_long)
mon_mat <- model.matrix(~month-1, mrides_long)
year_mat <- model.matrix(~year-1, mrides_long)
ride_mat<- model.matrix(~ride-1, mrides_long)

#column bind existing numeric data with one-hot encoded categorical variables
mrides_long_numeric<- cbind(mrides_long_numeric, holiday_name,hour_mat,mon_mat,year_mat,ride_mat) 

#remove outcome variable
mrides_long_num_no_resp<-mrides_long_numeric %>% select(-wait_time)

#convert to numeric & create matrix
mrides_long_num_no_resp<-mrides_long_num_no_resp %>% mutate_if(is.integer,as.numeric)
mrides_matrix <- data.matrix(mrides_long_num_no_resp)
```
To accurately measure the performance of our model, we can split the data to create training and testing sets. Below, I use a 70/30 train/test split, which means that 70% of the data is used to train the model. 
```{r}
# get the 70/30 training test split
numberOfTrainingSamples <- round(length(waitLabels) * .7)

# training data
train_data <- mrides_matrix[1:numberOfTrainingSamples,]
train_labels <- waitLabels[1:numberOfTrainingSamples]

# testing data
test_data <- mrides_matrix[-(1:numberOfTrainingSamples),]
test_labels <- waitLabels[-(1:numberOfTrainingSamples)]

# put testing & training data into seperate Dmatrix objects
dtrain <- xgb.DMatrix(data = train_data, label= train_labels)
dtest <- xgb.DMatrix(data = test_data, label= test_labels)
```
Now we train an xgboost model on the training set. 
For this model, I use 200 rounds of boosting. The "early_stop_round" hyper-parameter (set to 20) specifies the number of rounds after which the algorithm will stop if performance has not improved. This is just a baseline to see the performance of the xgboost package. I will conduct hyper-parameter tuning later in this post with the mlr package. 

```{r}
xgb <- xgboost(data = dtrain,   
                nrounds = 200,print_every_n = 50, early_stop_round = 20, eval_metric='mae',verbose = 1)

#model prediction
xgbpred <- predict (xgb,dtest)
xgb_mae<- mae(test_labels, xgbpred)
print(xgb_mae)
```
The model performance on the training data steadily improved as the number of boosting rounds increased, although gains were diminishing. Overall, it had a mean absolute error of ~14.5 minutes on the test data. This is pretty good considering it was predicting across the entire span of rides (as opposed to only one ride like the separate linear models above). 

```{r}
## Plot the feature importance
importance_matrix <- xgb.importance(colnames(dtrain), model = xgb)
xgb.plot.importance(importance_matrix = importance_matrix[1:20],xlab="Gain")
```

The plot above shows the most important features for our xgboost model. Specifying the ride as a feature was obviously important for performance (which makes sense: we expect that we would need to know which ride we were making a prediction on in order to be accurate).

We can conduct hyper-parameter tuning to improve performance. I will use the mlr package for tuning. Unlike xgboost, which requires a matrix, mlr functions take in a dataframe. So I will use the mrides_long df again.
```{r,message=FALSE}
library(mlr)
library(caret)

set.seed(1)

# Split df into train/test

names(mrides_long_numeric)<-gsub(x=names(mrides_long_numeric),"\\|","_") 

idTrain=createDataPartition(y=mrides_long_numeric$wait_time,p=0.7,list=FALSE)
train_df=mrides_long_numeric[idTrain,]
test_df=mrides_long_numeric[-idTrain,]

```

#### Baseline performance with mlr: no hyper-parameter tuning
```{r,message=FALSE,warning=FALSE}

#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
test_task = makeRegrTask(data = test_df, target = "wait_time")

#learner
xgb_learner <- makeLearner(
  "regr.xgboost", predict.type = "response", eval_metric="mae")

xgb_model <- mlr::train(xgb_learner, task = train_task)

result <- predict(xgb_model, test_task)
performance(result,measures = mae)
```
The above model uses the default hyper-parameter for nrounds (1). It has a pretty high MAE and is off by almost 40 minutes on average. However, we did not specify any hyper-parameters and defaults were used.

We can improve model performance by tuning our parameters. Below we can see all available parameters. 
```{r}
# To see all the parameters of the xgboost regr
getParamSet("regr.xgboost")
```
Below we will tune the nrounds, max_depth, eta, and lambda hyper-parameters.
```{r,message=FALSE,warning=FALSE}

xgb_params <- makeParamSet(makeIntegerParam("nrounds", lower = 100, upper = 300),
  # number of splits in each tree
  makeIntegerParam("max_depth", lower = 1, upper = 10),
  # "shrinkage" - prevents overfitting
  makeNumericParam("eta", lower = .1, upper = .5),
  # L2 regularization - prevents overfitting
  makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
control <- makeTuneControlRandom(maxit = 1)

# Choose a resampling strategy
resample_desc = makeResampleDesc("CV", iters = 2)

# tuning 
tuned_params <- tuneParams(
  learner = xgb_learner,
  task = train_task,
  resampling = resample_desc,
  par.set = xgb_params,
  control = control
)
xgb_tuned_learner<- setHyperPars(
  learner = xgb_learner,
  par.vals = tuned_params$x
)
print(xgb_tuned_learner)

```
The optimal hyperparameter results from tuning are shown above. Now, we can re-train our model with these results. 
```{r}
# Re-train parameters using tuned hyperparameters 
xgb_tuned_model <- mlr::train(xgb_tuned_learner, train_task)

# Make a new prediction
result_tuned <- predict(xgb_tuned_model, test_task)
performance(result_tuned,measures = mae)
```

The performance improved significantly with hyper-parameter tuning. This model performs better than the one created originally with the xgboost package. 
```{r}
feat_imp<-getFeatureImportance(xgb_tuned_model)

#plot top 20 features
feat_imp$res %>% gather(key="feature",value="importance") %>% top_n(20,importance) %>% ggplot(aes(x=fct_reorder(feature,importance))) + geom_col(aes(y=importance))+coord_flip()
```
This feature importance plot is very similar to that found above using xgboost untuned. The ride type is the most important feature in this model as well.  Additionally, we see that the hour, holiday metric (holidaym), and month are significant.

