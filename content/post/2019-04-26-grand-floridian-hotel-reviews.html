---
title: Grand Floridian Hotel Reviews
author: ''
date: '2019-04-26'
slug: grand-floridian-hotel-reviews
categories: []
tags:
  - sentiment analysis
  - NLP
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>


<div id="grand-floridian-hotel-reviews-sentiment-analysis-and-nlp" class="section level2">
<h2>Grand Floridian Hotel Reviews: Sentiment Analysis and NLP</h2>
<p>The data for this analysis was obtained from the reviews posted on TripAdvisor. I used RSelenium and rvest to extract the data from the first 3000 dynamic web-pages on the TripAdvisor site, which was 2,350 reviews in total. The code used to do this is published on my GitHub. Here, I load the the Rda file I saved from that extraction.</p>
<pre class="r"><code>library(tidyverse)
library(zoo)
library(knitr)
library(tidytext)
library(SnowballC)
library(keras)
library(kableExtra)</code></pre>
<p>This is the raw version of the data as obtained from the site: it has the text content of the review, the star rating (1-5), and the date of stay (month/year).</p>
<pre class="r"><code>#load data
rdf&lt;-readRDS(&quot;first_3000.Rda&quot;)
str(rdf)</code></pre>
<pre><code>## &#39;data.frame&#39;:    2350 obs. of  3 variables:
##  $ rating : int  5 5 5 5 5 3 5 5 4 5 ...
##  $ id_date: Factor w/ 75 levels &quot;Date of stay: April 2019&quot;,..: 1 3 2 2 2 1 2 2 2 3 ...
##  $ reviews: Factor w/ 2344 levels &quot;Family Vacation dreams come true.  We have always wanted to stay at Grand Floridian and this time we finally we&quot;| __truncated__,..: 4 3 1 2 5 10 7 9 6 8 ...</code></pre>
<p>It requires a bit of cleaning to get in the form we want.</p>
<pre class="r"><code>#rename &amp; change variable type
rdf &lt;- rdf %&gt;% rename(stay_date = id_date,
review = reviews) %&gt;%
mutate_at(vars(stay_date, review),
~ as.character(.)) %&gt;%
mutate(rating = as.factor(rating))

#change stay_date variable from character to date type

rdf$stay_date &lt;- gsub(&quot;Date of stay: &quot;, &quot;&quot;, rdf$stay_date)
rdf$stay_date &lt;- as.yearmon(rdf$stay_date, &quot;%b %Y&quot;)
kable(head(rdf,3)) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F,font_size = 8) </code></pre>
<table class="table table-striped" style="font-size: 8px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
rating
</th>
<th style="text-align:left;">
stay_date
</th>
<th style="text-align:left;">
review
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
Apr 2019
</td>
<td style="text-align:left;">
We have stayed throughout the Deluxe resorts on Disney property. We are a Disney family and have “grown up” in our accommodation needs starting at Animal Kingdom, through the Epcot resorts and this year, our first year staying at the Grand Floridian. With older kids, we decided to make this our home for our stay this year and we couldn’t have made a better choice. The main lobby is impressive and part way through our stay the easter eggs made their impressive appearance for the next few weeks. Dumbo on a Marbled egg is simply amazing!We arrived by Uber this year and we were, as typical for the Deluxe Resorts, greeted immediately by name and handed off to a cast member that would help with the check-in process. We were staying on the “club level” which we have always done (in anyone wants to know why, ping me… it is a HUGE benefit when executed properly). We were there early, so our luggage was handled and we were brought directly to the Club building where we could start our day and informed that when our room was ready, we would be notified by text and through the Disney app. The family split up to pools, and monorail rides to enjoy Disney. And fairly early in the afternoon, our room was ready. The rooms are a bit larger in Grand Floridian than they are in some of the other resorts and they make good use of that space. Two queen beds and a single pullout bed made life easy for us. Later that evening, we were planning to head out to watch the fireworks from the grounds but realized we could stand on our balcony and have a clear view which was great! There are better views for sure, but you can’t beat getting back into your room at 9:33 pm literally seconds after the fireworks end.Access to transportation is great with so many options from this resort. Monorail, boat or bus (or the Minnie-Van’s ;) ) to get you to where you need to go. I’m not sure if we were just lucky this year, but we never waited more than 5 minutes for any type of transportation. We used all of the pools during our stay and although crowded, we always (eventually) found a spot to sit back and relax. Virgin Strawberry Daiquiris were a hit (and a snack credit!!). The pools were clean and well managed to say the least and the constant activities available at the smaller pool was so much fun to watch. The “Club Level” at Grand Floridian was without question the best of any that we have experienced. Although a bit smaller than some, the quality of the options was amazing. Even Mickey waffles in the mornings for those looking for a bit heartier start to the day. There is rarely anything but raves regarding the staff at the Deluxe resorts and this stay was no different if not a step up. I’m not sure if there is a process or approach to who goes where but everyone was awesome!There are plenty of shops in the resort to keep you in whatever you need and regardless, spend some time just enjoying the main lobby while you are there. The restaurant count is greater than any other resort (I think) although some on the high end of the range, but certainly options for everyone. We ended our stay with a trip back to the airport on Disney’s Magical Express and as has been with every time using this service, it was flawless! A great Disney Resort that deserves its reputation!
</td>
</tr>
<tr>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
Oct 2018
</td>
<td style="text-align:left;">
This was our second stay at the Grand Floridian. This stay we were in the Main Building Club Level. The resort’s attention to detail is extraordinary. Our room was clean and spacious with a wonderful view of Magic Kingdom. The staff was very accommodating during the stay. There is a fitness center and spa on property, and the location on the monorail makes it very convenient.
</td>
</tr>
<tr>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
Mar 2019
</td>
<td style="text-align:left;">
Family Vacation dreams come true. We have always wanted to stay at Grand Floridian and this time we finally were able to. The Grand Floridian was very clean and we were able to get a great nights rest thanks to the very comfortable beds. My children loved the hidden murphy bed so much they traded out each night on who would sleep on it. We loved that upon our arrival we went down to the dock to watch the fireworks from Magic Kingdom and we could even hear all the music from speakers on the dock. We can not be more happy about out stay here.
</td>
</tr>
</tbody>
</table>
<p>Above we see three example reviews.
Now we can plot the share each rating category (1-5) over time.</p>
<pre class="r"><code>rdf %&gt;% 
  ggplot(aes(x=stay_date))+geom_bar(aes(fill=rating))+scale_x_yearmon(format = &quot;%b-%y&quot;)</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-4-1.png" width="672" />
It seems like most reviews have been consistenly positive. We can also analyze the distribution of reviews by each rating category.</p>
<pre class="r"><code>#set color
colors&lt;-c(&quot;cadetblue1&quot;, &quot;deepskyblue&quot;, &quot;deepskyblue3&quot;,&quot;deepskyblue4&quot;,&quot;dodgerblue3&quot;)

rdf %&gt;% 
  ggplot(aes(x=rating,fill=rating))+geom_bar()+scale_fill_manual(values=colors)</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-5-1.png" width="672" />
##Sentiment Analysis
Using the tidytext package, we first transform the text of the reviews into tidy format. We remove stop words and filter unwanted characters. Then, we can visualize the most common words in the text of all reviews.</p>
<pre class="r"><code>rdf_tidy &lt;- rdf %&gt;% mutate(review_num = row_number()) %&gt;%
  unnest_tokens(word, review) %&gt;%
  anti_join(stop_words) %&gt;% filter(str_detect(word, &quot;^[a-z&#39;]+$&quot;)) %&gt;%
  distinct()

most_common_words &lt;- rdf_tidy %&gt;%
  group_by(word) %&gt;%
  summarize(freq = n()) %&gt;%
  arrange(desc(freq))

#rearrange by frequency, eliminate duplicates

most_common_words %&gt;%
  mutate(word = reorder(word, freq)) %&gt;%
  head(25) %&gt;%  ggplot(aes(x = word, y = freq)) + geom_segment(aes(
    x = word ,
    xend = word,
    y = 0,
    yend = freq
  ), color = &quot;black&quot;) +
  geom_point(size = 3, color = &quot;deepskyblue&quot;) +
  labs(title = &quot;Top 25 Most Common Words in Review Text&quot;, y = &quot;# times used&quot;, x =
         &quot;&quot;) + coord_flip()</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>If we want to compare word usage between good and bad reviews, we can use the inverse document frequency. In other words, which words appear frequently in high reviews but not low (and vice versa)?</p>
<pre class="r"><code>#group rating 1&amp;2 into &quot;low&quot; category; rating 3,4,&amp; 5 &amp; into &quot;high&quot; category
by_rate &lt;- rdf %&gt;%
mutate(rating_group = case_when(rating == &quot;1&quot; |
rating == &quot;2&quot; ~ &quot;low&quot;, TRUE ~ &quot;high&quot;))


by_rate_inverse &lt;- by_rate %&gt;% unnest_tokens(word, review) %&gt;%
count(rating_group, word, sort = TRUE) %&gt;% filter(!nchar(word) &lt;= 3) %&gt;%
bind_tf_idf(word, rating_group, n) %&gt;% group_by(rating_group) %&gt;%
mutate(rank = rank(desc(tf_idf), ties.method = &quot;first&quot;)) %&gt;%
arrange(rank)

by_rate_inverse %&gt;%
mutate(word = factor(word, levels = rev(unique(word)))) %&gt;%
group_by(rating_group) %&gt;% filter(rank &lt;= 10) %&gt;%
ungroup() %&gt;%
ggplot(aes(word, tf_idf, fill = rating_group)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = &quot;tf-idf&quot;) +
facet_wrap( ~ rating_group, ncol = 2, scales = &quot;free&quot;) +
coord_flip()</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>We can also visualize the most common “bigrams” (i.e. group of two words). To do this, we first separate the unnested bigram and filter for stop words. Then we put them back together and count the frequency of appearance.</p>
<pre class="r"><code>##bigram: separate--&gt;filter out stop words; put back together--&gt;count
bigram_filt&lt;-rdf %&gt;% 
  unnest_tokens(bigram, review, token = &quot;ngrams&quot;, n = 2) %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% 
  filter(!word1 %in% stop_words$word) %&gt;%
  filter(!word2 %in% stop_words$word) 


bigram_count &lt;- bigram_filt %&gt;%
  unite(bigram, word1, word2, sep = &quot; &quot;) %&gt;%
  count(bigram, sort = TRUE)

bigram_count %&gt;% filter(!str_detect(bigram,&quot;floridian&quot;)) %&gt;% head(25) %&gt;%  mutate(bigram = reorder(bigram, n)) %&gt;% 
  ggplot(aes(x=bigram,y=n))+ geom_segment(aes(x=bigram ,xend=bigram, y=0, yend=n),color=&quot;black&quot;) +
  geom_point(size=3, color=&quot;deepskyblue&quot;)+
  labs(title = &quot;Top 25 Most Common Bigrams in Review Text&quot;, y = &quot;# times used&quot;, x=&quot;&quot;) + coord_flip()</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-8-1.png" width="672" />
Some of the bigram results are particularly interesting, such as “1900 park” (the on-site restaraunt) and “sugar load” (the name of the club level). It also makes sense that “Magic Kingdom” (located right next to the GF) is the most commonly used.</p>
<p>The tidytext package has several lexicons (stored as “sentiments”) that can be used for sentiment analysis. Below, we explore the number of distint words in each lexicon. We exclude “loughran”, the finance lexicon.</p>
<pre class="r"><code>sentiment_lexicons &lt;- sentiments %&gt;% 
  filter(lexicon != &quot;loughran&quot;)%&gt;% 
  group_by(lexicon) %&gt;%
  mutate(words_in_lexicon = n_distinct(word)) %&gt;%
  ungroup()

sentiment_lexicons %&gt;%
  group_by(lexicon) %&gt;%
  summarise(distinct_words = n_distinct(word)) %&gt;%
  ungroup() </code></pre>
<pre><code>## # A tibble: 3 x 2
##   lexicon distinct_words
##   &lt;chr&gt;            &lt;int&gt;
## 1 AFINN             2476
## 2 bing              6785
## 3 nrc               6468</code></pre>
<p>We can see how many words in the review text are found in each lexicon.</p>
<pre class="r"><code>rdf_tidy %&gt;%
  mutate(words_in_reviews = n_distinct(word)) %&gt;%
  inner_join(sentiment_lexicons) %&gt;%
  group_by(lexicon, words_in_reviews, words_in_lexicon) %&gt;%
  summarise(num_match = n_distinct(word)) %&gt;%
  ungroup() %&gt;%
  mutate(match_ratio = num_match / words_in_reviews) %&gt;%
  select(lexicon, words_in_reviews, num_match, match_ratio)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre><code>## # A tibble: 3 x 4
##   lexicon words_in_reviews num_match match_ratio
##   &lt;chr&gt;              &lt;int&gt;     &lt;int&gt;       &lt;dbl&gt;
## 1 AFINN              10983       954      0.0869
## 2 bing               10983      1687      0.154 
## 3 nrc                10983      1923      0.175</code></pre>
<p>The bing lexicon classifies words as either positive or negative. Using this, we can see when the most negative words were used in reviews.</p>
<pre class="r"><code>#get negative words
bingnegative &lt;- get_sentiments(&quot;bing&quot;) %&gt;% 
  filter(sentiment == &quot;negative&quot;)

#normalize w/count of total words in reviews for given stay date
wordcounts &lt;- rdf_tidy %&gt;%
  group_by(stay_date) %&gt;%
  summarize(words = n())

most_neg&lt;-rdf_tidy %&gt;%
  semi_join(bingnegative) %&gt;%
  group_by(stay_date) %&gt;%
  summarize(negativewords = n()) %&gt;%
  left_join(wordcounts, by = c(&quot;stay_date&quot;)) %&gt;%
  mutate(ratio_neg = negativewords/words,ratio_pos=1-ratio_neg) %&gt;%
  ungroup()


most_neg %&gt;% ggplot(aes(x=stay_date,y=ratio_neg,color=ratio_neg))+scale_x_yearmon(format = &quot;%b-%y&quot;)+geom_line()</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>mean(most_neg$ratio_neg)</code></pre>
<pre><code>## [1] 0.04268044</code></pre>
<p>From this, we can see that 8% is the max percentage of negative words used in reviews over a given month. On average, it’s about 4%.
The AFINN lexicon provides a positivity score for each word, from -5 (most negative) to 5 (most positive). We can visualize how these positivity scores compare to the actual numeric rating (1-5) given.</p>
<pre class="r"><code>##gives word and score
afinn &lt;- sentiments %&gt;%
filter(lexicon == &quot;AFINN&quot;) %&gt;%
select(word, afinn_score = score)

reviews_sentiment &lt;- rdf_tidy %&gt;%
inner_join(afinn, by = &quot;word&quot;) %&gt;%
group_by(review_num, rating) %&gt;%
summarize(sentiment = mean(afinn_score))


ggplot(reviews_sentiment, aes(rating, sentiment, group = rating)) +
geom_boxplot(colour = &quot;black&quot;,
fill = &quot;#56B4E9&quot;,
notch = T) +
ylab(&quot;Average sentiment score&quot;)</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-12-1.png" width="672" />
Although there are outliers with low scores among 5 star reviews, we see that sentiment scores roughly correlate with the rating given. There is the largest gap between the 25th and 50th percentile in rating 1 group, indicating that some reviews here were quite negative.</p>
<p>Finally, we can visualize difference in word usage between positive and negative reviews by plotting the most frequently used words against average rating. To do this we obtain a per-word summary and find the average rating for its usage.</p>
<pre class="r"><code>#count number of times word used in each review
count_words &lt;- rdf_tidy %&gt;%
  count(review_num, rating, word) %&gt;%
  ungroup() 

#returns most negative words
summary_neg&lt;- count_words %&gt;%  group_by(word) %&gt;%
  summarize(reviews = n(),
            average_rat = mean(as.integer(rating))) %&gt;% filter(reviews&gt;10) %&gt;% arrange(average_rat)



head(summary_neg,15) %&gt;% ggplot(aes(x=reviews,y=average_rat))+ geom_point() +
  geom_label(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 0) +
  xlab(&quot;# of reviews&quot;) +
  ylab(&quot;Average Rating&quot;) + expand_limits(x = c(10,40))</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-13-1.png" width="672" />
Many of these are words we would expect in negative reviews. However, “waldorf” is interesting because it may reflect guest making a comparison or speaking highly of another resort.</p>
<pre class="r"><code>summary_pos&lt;- count_words %&gt;%  group_by(word) %&gt;%
  summarize(reviews = n(),
            average_rat = mean(as.integer(rating))) %&gt;% filter(reviews&gt;50) %&gt;% arrange(desc(average_rat))

head(summary_pos,15) %&gt;% ggplot(aes(x=reviews,y=average_rat))+ geom_point() +
  geom_label(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 0) +
  xlab(&quot;# of reviews&quot;) +
  ylab(&quot;Average Rating&quot;)</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-14-1.png" width="672" />
Similarly, many of these words are what we would expect in positive reviews–though “kitchen”,“pianist”,and “orchestra” are notable.</p>
<p>##Text Classification</p>
<p>Now, we will use keras to build a deep-learning model that classifies whether or not reviews are 5 star based on their text (i.e. the words used).</p>
<pre class="r"><code>#set seed
set.seed(42)
#shuffle dataframe
rows &lt;- sample(nrow(rdf))
rdf &lt;- rdf[rows, ]


rdf_bin &lt;- rdf %&gt;% mutate(review_num = row_number()) %&gt;%
  mutate(five_star = case_when(rating != 5 ~ 0, TRUE ~ 1)) %&gt;% select(-rating, -stay_date)

rdf_bin %&gt;% ggplot(aes(x = five_star)) +
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat = &quot;count&quot;) +
  scale_y_continuous(labels = scales::percent) + ylab(&quot;percent&quot;) +
  scale_x_discrete()</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-15-1.png" width="672" />
Almost 70% of the reviews are five star. This will be helpful as we evaluate model performance.</p>
<p>We tokenize the review text into a sequence of integers where each word will correspond to an integer. To get the 1000 most frequent words in our text, we set num_words to 1000.</p>
<pre class="r"><code># get text of review
text&lt;-rdf_bin$review
max_features &lt;- 1000
tokenizer &lt;- text_tokenizer(num_words = max_features)

#fit tokenizer to actual reviews
tokenizer %&gt;% 
  fit_text_tokenizer(text)

#num reviews
print(tokenizer$document_count)</code></pre>
<pre><code>## [1] 2350</code></pre>
<pre class="r"><code>#list of integers for reviews 
text_seqs&lt;- texts_to_sequences(tokenizer, text)</code></pre>
<p>To begin constructing a neural network, we first define training data to map input tensors to target tensors. However, we can’t feed the lists of integers (text_seqs) into the network–they must first be converted to tensors. This is accomplished by padding the lists to have the same length (with pad_sequences). Then the lists are converted into an integer tensor which has dimension num_reviews x max_length (2350x100). This is used to construct the first (“embedding”) layer.</p>
<pre class="r"><code>#cut reviews off after 100 words
maxlen &lt;- 100

#define train data
x_train &lt;- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)</code></pre>
<pre><code>## [1] 2350  100</code></pre>
<pre class="r"><code>y_train &lt;- rdf_bin$five_star
length(y_train)</code></pre>
<pre><code>## [1] 2350</code></pre>
<p>The network will learn 50-dimensional embeddings for each of the 1000 words.</p>
<pre class="r"><code># Network parameters
embedding_dims &lt;- 50
filters &lt;- 64
kernel_size &lt;- 5
hidden_dims &lt;- 50
epochs &lt;- 10</code></pre>
<div id="build-network-layers-hidden-units-activation-function" class="section level4">
<h4>Build Network: Layers, Hidden Units, Activation Function</h4>
<p>Below, we build the model.</p>
<pre class="r"><code>#input max_feat (total vocabulary count used for reviews:1000)

model &lt;- keras_model_sequential() %&gt;% 
  #embedding layer
  layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %&gt;%
  layer_dropout(0.2) %&gt;%
  layer_conv_1d(
    filters, kernel_size, 
    padding = &quot;valid&quot;, activation = &quot;relu&quot;, strides = 1
  ) %&gt;%
  layer_global_max_pooling_1d() %&gt;%
  layer_dense(units=hidden_dims, activation = &quot;relu&quot;) %&gt;%
  layer_dropout(0.2) %&gt;% 
  layer_dense(units=1,activation = &quot;sigmoid&quot;)

model %&gt;% summary()</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## embedding (Embedding)            (None, 100, 50)               50000       
## ___________________________________________________________________________
## dropout (Dropout)                (None, 100, 50)               0           
## ___________________________________________________________________________
## conv1d (Conv1D)                  (None, 96, 64)                16064       
## ___________________________________________________________________________
## global_max_pooling1d (GlobalMaxP (None, 64)                    0           
## ___________________________________________________________________________
## dense (Dense)                    (None, 50)                    3250        
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 50)                    0           
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 1)                     51          
## ===========================================================================
## Total params: 69,365
## Trainable params: 69,365
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<p>The model is built by sequentially stacking layers:</p>
<ol style="list-style-type: decimal">
<li><p>The embedding layer input dimension is the vocab size. It looks up the embedding vector for each word-index and adds a dimension to the output.</p></li>
<li><p>1D convolution layer using kernel size 5: recognize local patterns in sentence to learn words length &lt;5, can recognize in later context</p></li>
<li><p>Additional dense layer with relu activation</p></li>
<li><p>The final layer uses the sigmoid activation function to output a single node: a probability score between 0 and 1 indicating how likely a review is to be positive.</p></li>
</ol>
<p>Additionally, dropout layers were added to combat overfitting. Weights below .2 were dropped by these layers.</p>
</div>
<div id="choosing-loss-function-optimizer" class="section level4">
<h4>Choosing Loss Function &amp; Optimizer</h4>
<p>We use the binary crossentropy loss function since we are solving a binary classification problem. The optimizer is a stochastic gradient descent (SGD) variant that specifies how the network weights will be updated. This can be modified via hyperparameter tuning, but I found that rmsprop performed better than others (e.g. adam) here. We also track accuracy metrics during training.</p>
<pre class="r"><code>model %&gt;% compile(
  loss = &quot;binary_crossentropy&quot;,
  optimizer = &quot;rmsprop&quot;,
  metrics = &quot;accuracy&quot;
)</code></pre>
<p>We train the model in mini-batches of size 32 for 10 iterations (epochs) over the training set but set aside 20% of our reviews for validation.</p>
<pre class="r"><code>hist &lt;- model %&gt;%
  fit(
    x_train,
    y_train,
    batch_size = 32,
    epochs = 10,
    validation_split = 0.2
  )

print(hist)</code></pre>
<pre><code>## Trained on 1,880 samples, validated on 470 samples (batch_size=32, epochs=10)
## Final epoch (plot to see history):
## val_loss: 0.446
##  val_acc: 0.8085
##     loss: 0.1866
##      acc: 0.9378</code></pre>
<pre class="r"><code>plot(hist)</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-22-1.png" width="672" />
The training loss decreases with each epoch, while the accuracy increases. However, the validation data does not show this same trend, which indicates that we are overfitting. The right number of epochs appears to be 6. Considering the fact that we only use the 1000 most common words and cut reviews off at 100 words, our model is fairly accurate (peaking at around 80% on the validation set). However, we can likely increase this by increasing the feature space and length of review considered.</p>
<pre class="r"><code>#increase num words for embedding
max_features&lt;-5000
maxlen&lt;-400
#increase length of review considered
x_train_long &lt;- pad_sequences(text_seqs, maxlen = maxlen)

model &lt;- keras_model_sequential() %&gt;% 
  #embedding layer
  layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %&gt;%
  layer_dropout(0.2) %&gt;%
  layer_conv_1d(
    filters, kernel_size, 
    padding = &quot;valid&quot;, activation = &quot;relu&quot;, strides = 1
  ) %&gt;%
  layer_global_max_pooling_1d() %&gt;%
  layer_dense(units=hidden_dims, activation = &quot;relu&quot;) %&gt;%
  layer_dropout(0.2) %&gt;% 
  layer_dense(units=1,activation = &quot;sigmoid&quot;)

model %&gt;% compile(
  loss = &quot;binary_crossentropy&quot;,
  optimizer = &quot;rmsprop&quot;,
  metrics = &quot;accuracy&quot;
)</code></pre>
<pre class="r"><code>hist &lt;- model %&gt;%
  fit(
    x_train_long,
    y_train,
    batch_size = 32,
    epochs = 10,
    validation_split = 0.2
  )

print(hist)</code></pre>
<pre><code>## Trained on 1,880 samples, validated on 470 samples (batch_size=32, epochs=10)
## Final epoch (plot to see history):
## val_loss: 0.4388
##  val_acc: 0.8234
##     loss: 0.154
##      acc: 0.9548</code></pre>
<pre class="r"><code>plot(hist)</code></pre>
<p><img src="/post/2019-04-26-grand-floridian-hotel-reviews_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Again, 6 appears to be the right number of epochs–beyond this we are overfitting. Although validation accuracy improves to about 84% (a 4% increase from the previous model), there is a tradeoff in that this model takes more than 2 times the amount of time to train. If rapid classification is necessary, the smaller feature space performance may be sufficient.</p>
</div>
</div>
