---
title: "Wait Times at Walt Disney World"
author: 'Mikayla'
date: '2019-04-22'
slug: wait-times-at-walt-disney-world
tags: []
categories: []
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>


<p>The data used for this analysis was originally published by <a href="https://touringplans.com/blog/2018/06/25/disney-world-wait-times-available-for-data-science-and-machine-learning/">Touring Plans</a>.</p>
<p>I used the data for six rides: two from the Magic Kingdom (Seven Dwarfs Mine Train &amp; Splash Mountain), two from Hollywood Studios (Toy Story Mania &amp; The Rockin Rollercoaster), and two from Epcot (Soarin &amp; Spaceship Earth).</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(skimr)
library(ModelMetrics)
library(janitor)
library(broom)
library(forcats)
library(kableExtra)
library(stringr)</code></pre>
<p>There was a separate csv file for each ride. I originally merged &amp; cleaned this data using Python, which can be found <a href="https://github.com/mikaylaedwards/Disney-wait-times/blob/master/wait%20comparison.ipynb">here</a>.
I then created a new file (merged_rides), which I load below. I also load the metadata file, which contains additional information about each observed date (e.g. holidays, WDW events, etc.). This will later be merged with the ride wait time data for modeling.</p>
<pre class="r"><code>meta&lt;-read_csv(&quot;metadata.csv&quot;)
merged_rides&lt;- read_csv(&quot;merged_rides_.csv&quot;)
merged_rides$datetime &lt;- as_datetime(merged_rides$datetime)</code></pre>
<pre class="r"><code>kable(head(merged_rides)) %&gt;%   kable_styling(font_size = 9,bootstrap_options = &quot;striped&quot;, full_width = F)</code></pre>
<table class="table table-striped" style="font-size: 9px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
datetime
</th>
<th style="text-align:right;">
toy_story_mania
</th>
<th style="text-align:right;">
rockin_rollercoaster
</th>
<th style="text-align:right;">
seven_dwarfs
</th>
<th style="text-align:right;">
splash_mountain
</th>
<th style="text-align:right;">
space_earth
</th>
<th style="text-align:right;">
soarin
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
2014-05-23 09:14:00
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
45
</td>
</tr>
<tr>
<td style="text-align:left;">
2014-05-23 09:15:00
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
45
</td>
</tr>
<tr>
<td style="text-align:left;">
2014-05-23 09:27:00
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
45
</td>
</tr>
<tr>
<td style="text-align:left;">
2014-05-23 09:29:00
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
45
</td>
</tr>
<tr>
<td style="text-align:left;">
2014-05-23 09:37:00
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
45
</td>
</tr>
<tr>
<td style="text-align:left;">
2014-05-23 09:40:00
</td>
<td style="text-align:right;">
85
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
45
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>merged_rides %&gt;% gather(key=&quot;ride&quot;,value=&quot;wait_time&quot;,-datetime) %&gt;% 
ggplot(aes(ride, wait_time, group = ride)) +
  geom_boxplot(colour = &quot;black&quot;, fill = &quot;#56B4E9&quot;,notch = T) +
  ylab(&quot;Wait time&quot;)</code></pre>
<p><img src="/post/2019-04-22-wait-times-at-walt-disney-world_files/figure-html/unnamed-chunk-4-1.png" width="672" />
This shows the distribution of wait times by ride. We see, for example, that Seven Dwarfs has the highest median wait time, while Spaceship Earth has the lowest.</p>
<pre class="r"><code>rides1&lt;-merged_rides%&gt;%
  mutate(hour=hour(datetime),
         date=date(datetime)) %&gt;%
  group_by(hour,date)%&gt;% filter(hour&lt;21 &amp; hour&gt;8) %&gt;%
  summarize_at(
    vars(toy_story_mania:soarin),
    ~mean(.,na.rm =TRUE)
  )
avg_by_month &lt;- rides1 %&gt;%
  mutate(month = month(date, label = TRUE)) %&gt;%
  group_by(month) %&gt;%
  summarize_at(vars(toy_story_mania:soarin),
  ~ mean(., na.rm = TRUE))

avg_by_month %&gt;% gather(key=&quot;ride&quot;,value=&quot;avg_wait&quot;,-month,) %&gt;% 
ggplot(aes(x=month,y=avg_wait,color=ride)) + geom_line(aes(group = ride)) +
geom_point() + xlab(&quot;Month&quot;)</code></pre>
<p><img src="/post/2019-04-22-wait-times-at-walt-disney-world_files/figure-html/unnamed-chunk-5-1.png" width="672" />
All rides have the lowest average wait in September. Splash Mountain sees the largest decline.</p>
<pre class="r"><code>rides1 %&gt;%
  keep(is.numeric) %&gt;% 
  gather(key=&quot;ride&quot;,value=&quot;wait&quot;,-hour) %&gt;% 
ggplot(aes(x = wait, fill = as.factor(ride))) + 
  geom_density(alpha = .3)+xlim(5,220)+ labs(title=&quot;Wait Time Dist.&quot;)+scale_fill_discrete(name=&quot;Ride&quot;)</code></pre>
<p><img src="/post/2019-04-22-wait-times-at-walt-disney-world_files/figure-html/unnamed-chunk-6-1.png" width="672" />
Spaceship Earth’s distribution appeairs skewed leftward because wait times are consistenly low.</p>
<p>Now, we bring in the metadata file to examine additional features that can be used as we build various models to predict the ride wait times.</p>
<pre class="r"><code>##selecting features to explore
meta1&lt;-meta %&gt;% select(DATE, contains(&#39;event&#39;),-contains(&#39;eventN&#39;),contains(&#39;HOLIDAY&#39;),-contains(&#39;DAYS&#39;),-contains(&#39;STREAK&#39;),-HOLIDAYJ)

meta1$date&lt;-as.character(meta1$DATE)
meta1$date &lt;-ymd(mdy(meta1$date))
meta1$DATE&lt;-NULL
meta1&lt;-meta1 %&gt;% clean_names()  
meta1$holidayn&lt;-fct_explicit_na(meta1$holidayn, &quot;no_holiday&quot;)</code></pre>
<pre class="r"><code>mrides&lt;-rides1 %&gt;% 
merge(meta1,by=&quot;date&quot;)%&gt;% 
arrange(date,hour) %&gt;% mutate(
  hour=as.factor(hour),  
  month=as.factor(month(date)),
  year=as.factor(year(date))) 

kable(head(mrides)) %&gt;% 
  kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F,font_size = 8)</code></pre>
<table class="table table-striped" style="font-size: 8px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
date
</th>
<th style="text-align:left;">
hour
</th>
<th style="text-align:right;">
toy_story_mania
</th>
<th style="text-align:right;">
rockin_rollercoaster
</th>
<th style="text-align:right;">
seven_dwarfs
</th>
<th style="text-align:right;">
splash_mountain
</th>
<th style="text-align:right;">
space_earth
</th>
<th style="text-align:right;">
soarin
</th>
<th style="text-align:right;">
wd_wevent
</th>
<th style="text-align:right;">
m_kevent
</th>
<th style="text-align:right;">
e_pevent
</th>
<th style="text-align:right;">
h_sevent
</th>
<th style="text-align:right;">
a_kevent
</th>
<th style="text-align:right;">
holidaypx
</th>
<th style="text-align:right;">
holidaym
</th>
<th style="text-align:left;">
holidayn
</th>
<th style="text-align:right;">
holiday
</th>
<th style="text-align:left;">
month
</th>
<th style="text-align:left;">
year
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
2014-05-23
</td>
<td style="text-align:left;">
9
</td>
<td style="text-align:right;">
67.72727
</td>
<td style="text-align:right;">
30.00000
</td>
<td style="text-align:right;">
58.18182
</td>
<td style="text-align:right;">
10.90909
</td>
<td style="text-align:right;">
5.909091
</td>
<td style="text-align:right;">
45.90909
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
no_holiday
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
2014
</td>
</tr>
<tr>
<td style="text-align:left;">
2014-05-23
</td>
<td style="text-align:left;">
10
</td>
<td style="text-align:right;">
80.00000
</td>
<td style="text-align:right;">
31.66667
</td>
<td style="text-align:right;">
60.00000
</td>
<td style="text-align:right;">
34.58333
</td>
<td style="text-align:right;">
15.000000
</td>
<td style="text-align:right;">
89.16667
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
no_holiday
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
2014
</td>
</tr>
<tr>
<td style="text-align:left;">
2014-05-23
</td>
<td style="text-align:left;">
11
</td>
<td style="text-align:right;">
80.00000
</td>
<td style="text-align:right;">
46.00000
</td>
<td style="text-align:right;">
60.00000
</td>
<td style="text-align:right;">
35.00000
</td>
<td style="text-align:right;">
40.000000
</td>
<td style="text-align:right;">
87.50000
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
no_holiday
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
2014
</td>
</tr>
<tr>
<td style="text-align:left;">
2014-05-23
</td>
<td style="text-align:left;">
12
</td>
<td style="text-align:right;">
70.00000
</td>
<td style="text-align:right;">
34.00000
</td>
<td style="text-align:right;">
60.00000
</td>
<td style="text-align:right;">
72.50000
</td>
<td style="text-align:right;">
40.000000
</td>
<td style="text-align:right;">
60.00000
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
no_holiday
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
2014
</td>
</tr>
<tr>
<td style="text-align:left;">
2014-05-23
</td>
<td style="text-align:left;">
13
</td>
<td style="text-align:right;">
55.00000
</td>
<td style="text-align:right;">
48.57143
</td>
<td style="text-align:right;">
60.00000
</td>
<td style="text-align:right;">
67.14286
</td>
<td style="text-align:right;">
40.000000
</td>
<td style="text-align:right;">
61.42857
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
no_holiday
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
2014
</td>
</tr>
<tr>
<td style="text-align:left;">
2014-05-23
</td>
<td style="text-align:left;">
14
</td>
<td style="text-align:right;">
82.50000
</td>
<td style="text-align:right;">
40.00000
</td>
<td style="text-align:right;">
41.25000
</td>
<td style="text-align:right;">
68.12500
</td>
<td style="text-align:right;">
33.750000
</td>
<td style="text-align:right;">
60.00000
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
no_holiday
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
2014
</td>
</tr>
</tbody>
</table>
<p>With this data, we can look at how ride wait times are affected by holidays.</p>
<pre class="r"><code>mrides %&gt;% group_by(holiday,hour) %&gt;% summarize_at(
  vars(toy_story_mania:soarin),
  ~mean(.)
) %&gt;% 
  gather(key=&quot;ride&quot;,value=&quot;wait_time&quot;,toy_story_mania:soarin) %&gt;% 
  ggplot(aes(x=hour,y=wait_time,color=as.logical(holiday)))+geom_line(aes(group=holiday))+facet_wrap(~ride,scales = &quot;free_x&quot;)</code></pre>
<p><img src="/post/2019-04-22-wait-times-at-walt-disney-world_files/figure-html/unnamed-chunk-9-1.png" width="672" />
This shows that the hourly average wait is higher on holidays but has a similar slope.</p>
<div id="part-i.-linear-model-for-each-ride" class="section level4">
<h4>Part I. Linear Model for Each Ride</h4>
<p>Using the tidyverse list-column workflow, we can build a separate model for each ride within the same dataframe.</p>
<pre class="r"><code>rides_nest&lt;-mrides %&gt;% gather(key=&quot;ride&quot;,value=&quot;wait_time&quot;,toy_story_mania:soarin)%&gt;%
  group_by(ride) %&gt;% nest()
head(rides_nest)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   ride                 data                  
##   &lt;chr&gt;                &lt;list&gt;                
## 1 toy_story_mania      &lt;tibble [20,793 × 14]&gt;
## 2 rockin_rollercoaster &lt;tibble [20,793 × 14]&gt;
## 3 seven_dwarfs         &lt;tibble [20,793 × 14]&gt;
## 4 splash_mountain      &lt;tibble [20,793 × 14]&gt;
## 5 space_earth          &lt;tibble [20,793 × 14]&gt;
## 6 soarin               &lt;tibble [20,793 × 14]&gt;</code></pre>
<p>The data column shown above is a list where each element is a tibble that stores the data corresponding to the ride variable.
Now, we append a “model” variable to the data tibble for each ride. Using the broom package, we can easily extract model results. The “tidy” function, for example, retrieves the coefficient values of the linear model.</p>
<pre class="r"><code>##building model for each ride
nest_model&lt;-rides_nest %&gt;% 
  mutate(model=map(data,~lm(formula =wait_time~.,data=.x)))

##retrieve coef         
coef&lt;-nest_model %&gt;% 
  mutate(coef=map(model,~tidy(.x))) %&gt;% 
  unnest(coef)

coef %&gt;% select(ride:estimate) %&gt;% filter(term!=&#39;(Intercept)&#39;) %&gt;%  group_by(ride) %&gt;% top_n(5,estimate) %&gt;%  ggplot(aes(x=term,y=estimate))+geom_bar(stat=&quot;identity&quot;) + facet_wrap(~ride,scales=&quot;free_y&quot;) +
  coord_flip()</code></pre>
<p><img src="/post/2019-04-22-wait-times-at-walt-disney-world_files/figure-html/unnamed-chunk-11-1.png" width="672" />
This shows the variables with the highest coefficient estimate for each ride. We can see that “nye” (New Years Eve) is in the top 5 for every ride. For seven dwarfs, the mid-day hours are particularly significant.</p>
<p>We can also compare different holidays, such as New Years Eve, Christmas Eve, and Independence Day.</p>
<pre class="r"><code>mrides %&gt;% 
  filter(stringr::str_detect(holidayn, &quot;nye|cme|ind&quot;) ) %&gt;% gather(key=&quot;ride&quot;,value=&quot;wait_time&quot;,toy_story_mania:soarin) %&gt;% group_by(ride,hour,holidayn) %&gt;% summarize(avg_wait=mean(wait_time)) %&gt;% 
  ggplot(aes(x=hour,y=avg_wait,group=holidayn,color=holidayn))+geom_line(aes(group=holidayn))+facet_wrap(~ride,scales=&quot;free_y&quot;,&quot;free_x&quot;)</code></pre>
<p><img src="/post/2019-04-22-wait-times-at-walt-disney-world_files/figure-html/unnamed-chunk-12-1.png" width="672" />
NYE seems to have the highest wait times for all rides. This is consistent with the coefficient importance results shown above. For some rides,the wait time difference is more significant(e.g.soarin).This is also reflected in the coefficient estimate above. For splash mountain, however, independence day (July 4th) times are also very high. This makes sense given that it’s a water ride whose popularity we would expect to be highest in summer.</p>
<p>Now, we can compare the model performance for each ride. Broom has a “glance” function that provides a data frame with model performance statistics and an “augment” function through which we can derive model predictions.
Here, I compare the mean absolute error (mae) for each ride model.</p>
<pre class="r"><code>model_perf&lt;-nest_model %&gt;% 
  mutate(
    fit=map(model,~glance(.x)),
    augmented=map(model,~augment(.x))
  )

aug &lt;- model_perf %&gt;%
  unnest(augmented)
  ##calc mae for each ride model
  aug_mae &lt;- aug %&gt;% group_by(ride) %&gt;%
  transmute(mae = mae(wait_time, .fitted)) %&gt;% distinct()
  
  aug_mae %&gt;% ggplot(aes(x = ride, y = mae)) + geom_segment(aes(
  x = ride ,
  xend = ride,
  y = 0,
  yend = mae
  ), color = &quot;black&quot;) +
  geom_point(size = 3, color = &quot;deepskyblue&quot;) + geom_text(aes(label = round(mae,2), y = mae + 0.75),
  position = position_dodge(0.9),
  vjust = 0) + coord_flip()</code></pre>
<p><img src="/post/2019-04-22-wait-times-at-walt-disney-world_files/figure-html/unnamed-chunk-13-1.png" width="672" />
The highest mae (mean absolute error) is observed for seven dwarfs. Spaceship Earth has the lowest, which makes sense given its minimal variation. We can interpret the mae as being the average size of the prediction error (i.e. the absolute difference between the model prediction and the actual wait time). So the seven dwarfs model was off, on average, by about 17 minutes.</p>
<pre class="r"><code>ggplot(aug,aes(wait_time,.fitted))+geom_smooth()+geom_abline(intercept=0,slope=1)+facet_wrap(~ride)</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39;</code></pre>
<p><img src="/post/2019-04-22-wait-times-at-walt-disney-world_files/figure-html/unnamed-chunk-14-1.png" width="672" />
By plotting our predictions (y axis) against the actual time (x axis), we can see that out models were generally much better at predicting low wait times. They did not accurately capture the high wait times. This is particularly true for splash mountain, where our predictions were very low (50-75) despite high observed wait times (200-300).</p>
</div>
<div id="part-ii.-xgboost-model" class="section level4">
<h4>Part II. XGboost Model</h4>
<p>Instead of building a separate model for each ride, we could instead have a single model that uses ride as a feature in the dataset. Here, I will use eXtreme Gradient Boosting (XGboost), an implementation of the gradient boosted decision tree algorithm. To give a short explanation, gradient boosting is an ensemble technique that sequentially creates new models to predict the residuals (errors) of previous models. The gradient descent algorithm is used to minimize the loss between subsequent models. The residuals are added together when the model makes a final prediction.</p>
<pre class="r"><code>library (xgboost)
set.seed(1234)

#select numeric features
mrides_long&lt;-mrides %&gt;% gather(key=&quot;ride&quot;,value=&quot;wait_time&quot;,toy_story_mania:soarin) %&gt;% mutate_if(is.factor,as.character) 
mrides_long_numeric&lt;-mrides_long %&gt;% select(-date) %&gt;% select_if(is.numeric)


#get vector of training labels
waitLabels &lt;- mrides_long_numeric %&gt;% mutate(wait_time=as.numeric(wait_time)) %&gt;% select(wait_time)
waitLabels&lt;-as.vector(waitLabels$wait_time)

#one-hot encoding for categorical variables (holidayn,hour,month, etc. )
holiday_name &lt;- model.matrix(~holidayn-1, mrides_long)
hour_mat &lt;- model.matrix(~hour-1, mrides_long)
mon_mat &lt;- model.matrix(~month-1, mrides_long)
year_mat &lt;- model.matrix(~year-1, mrides_long)
ride_mat&lt;- model.matrix(~ride-1, mrides_long)

#column bind existing numeric data with one-hot encoded categorical variables
mrides_long_numeric&lt;- cbind(mrides_long_numeric, holiday_name,hour_mat,mon_mat,year_mat,ride_mat) 

#remove outcome variable
mrides_long_num_no_resp&lt;-mrides_long_numeric %&gt;% select(-wait_time)

#convert to numeric &amp; create matrix
mrides_long_num_no_resp&lt;-mrides_long_num_no_resp %&gt;% mutate_if(is.integer,as.numeric)
mrides_matrix &lt;- data.matrix(mrides_long_num_no_resp)</code></pre>
<p>To accurately measure the performance of our model, we can split the data to create training and testing sets. Below, I use a 70/30 train/test split, which means that 70% of the data is used to train the model.</p>
<pre class="r"><code># get the 70/30 training test split
numberOfTrainingSamples &lt;- round(length(waitLabels) * .7)

# training data
train_data &lt;- mrides_matrix[1:numberOfTrainingSamples,]
train_labels &lt;- waitLabels[1:numberOfTrainingSamples]

# testing data
test_data &lt;- mrides_matrix[-(1:numberOfTrainingSamples),]
test_labels &lt;- waitLabels[-(1:numberOfTrainingSamples)]

# put testing &amp; training data into seperate Dmatrix objects
dtrain &lt;- xgb.DMatrix(data = train_data, label= train_labels)
dtest &lt;- xgb.DMatrix(data = test_data, label= test_labels)</code></pre>
<p>Now we train an xgboost model on the training set.
For this model, I use 200 rounds of boosting. The “early_stop_round” hyper-parameter (set to 20) specifies the number of rounds after which the algorithm will stop if performance has not improved. This is just a baseline to see the performance of the xgboost package. I will conduct hyper-parameter tuning later in this post with the mlr package.</p>
<pre class="r"><code>xgb &lt;- xgboost(data = dtrain,   
                nrounds = 200,print_every_n = 50, early_stop_round = 20, eval_metric=&#39;mae&#39;,verbose = 1)</code></pre>
<pre><code>## [1]  train-mae:42.753963 
## [51] train-mae:13.566384 
## [101]    train-mae:12.531466 
## [151]    train-mae:11.913087 
## [200]    train-mae:11.448717</code></pre>
<pre class="r"><code>#model prediction
xgbpred &lt;- predict (xgb,dtest)
xgb_mae&lt;- mae(test_labels, xgbpred)
print(xgb_mae)</code></pre>
<pre><code>## [1] 14.50678</code></pre>
<p>The model performance on the training data steadily improved as the number of boosting rounds increased, although gains were diminishing. Overall, it had a mean absolute error of ~14.5 minutes on the test data. This is pretty good considering it was predicting across the entire span of rides (as opposed to only one ride like the separate linear models above).</p>
<pre class="r"><code>## Plot the feature importance
importance_matrix &lt;- xgb.importance(colnames(dtrain), model = xgb)
xgb.plot.importance(importance_matrix = importance_matrix[1:20],xlab=&quot;Gain&quot;)</code></pre>
<p><img src="/post/2019-04-22-wait-times-at-walt-disney-world_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>The plot above shows the most important features for our xgboost model. Specifying the ride as a feature was obviously important for performance (which makes sense: we expect that we would need to know which ride we were making a prediction on in order to be accurate).</p>
<p>We can conduct hyper-parameter tuning to improve performance. I will use the mlr package for tuning. Unlike xgboost, which requires a matrix, mlr functions take in a dataframe. So I will use the mrides_long df again.</p>
<pre class="r"><code>library(mlr)
library(caret)

set.seed(1)

# Split df into train/test

names(mrides_long_numeric)&lt;-gsub(x=names(mrides_long_numeric),&quot;\\|&quot;,&quot;_&quot;) 

idTrain=createDataPartition(y=mrides_long_numeric$wait_time,p=0.7,list=FALSE)
train_df=mrides_long_numeric[idTrain,]
test_df=mrides_long_numeric[-idTrain,]</code></pre>
</div>
<div id="baseline-performance-with-mlr-no-hyper-parameter-tuning" class="section level4">
<h4>Baseline performance with mlr: no hyper-parameter tuning</h4>
<pre class="r"><code>#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = &quot;wait_time&quot;)
test_task = makeRegrTask(data = test_df, target = &quot;wait_time&quot;)

#learner
xgb_learner &lt;- makeLearner(
  &quot;regr.xgboost&quot;, predict.type = &quot;response&quot;, eval_metric=&quot;mae&quot;)

xgb_model &lt;- mlr::train(xgb_learner, task = train_task)

result &lt;- predict(xgb_model, test_task)
performance(result,measures = mae)</code></pre>
<pre><code>##      mae 
## 38.41872</code></pre>
<p>The above model uses the default hyper-parameter for nrounds (1). It has a pretty high MAE and is off by almost 40 minutes on average. However, we did not specify any hyper-parameters and defaults were used.</p>
<p>We can improve model performance by tuning our parameters. Below we can see all available parameters.</p>
<pre class="r"><code># To see all the parameters of the xgboost regr
getParamSet(&quot;regr.xgboost&quot;)</code></pre>
<pre><code>## Warning in makeParam(id = id, type = &quot;numeric&quot;, learner.param = TRUE, lower = lower, : NA used as a default value for learner parameter missing.
## ParamHelpers uses NA as a special value for dependent parameters.</code></pre>
<pre><code>##                                 Type  len        Def               Constr
## booster                     discrete    -     gbtree gbtree,gblinear,dart
## watchlist                    untyped    -     &lt;NULL&gt;                    -
## eta                          numeric    -        0.3               0 to 1
## gamma                        numeric    -          0             0 to Inf
## max_depth                    integer    -          6             1 to Inf
## min_child_weight             numeric    -          1             0 to Inf
## subsample                    numeric    -          1               0 to 1
## colsample_bytree             numeric    -          1               0 to 1
## colsample_bylevel            numeric    -          1               0 to 1
## num_parallel_tree            integer    -          1             1 to Inf
## lambda                       numeric    -          1             0 to Inf
## lambda_bias                  numeric    -          0             0 to Inf
## alpha                        numeric    -          0             0 to Inf
## objective                    untyped    - reg:linear                    -
## eval_metric                  untyped    -       rmse                    -
## base_score                   numeric    -        0.5          -Inf to Inf
## max_delta_step               numeric    -          0             0 to Inf
## missing                      numeric    -                     -Inf to Inf
## monotone_constraints   integervector &lt;NA&gt;          0              -1 to 1
## tweedie_variance_power       numeric    -        1.5               1 to 2
## nthread                      integer    -          -             1 to Inf
## nrounds                      integer    -          1             1 to Inf
## feval                        untyped    -     &lt;NULL&gt;                    -
## verbose                      integer    -          1               0 to 2
## print_every_n                integer    -          1             1 to Inf
## early_stopping_rounds        integer    -     &lt;NULL&gt;             1 to Inf
## maximize                     logical    -     &lt;NULL&gt;                    -
## sample_type                 discrete    -    uniform     uniform,weighted
## normalize_type              discrete    -       tree          tree,forest
## rate_drop                    numeric    -          0               0 to 1
## skip_drop                    numeric    -          0               0 to 1
## callbacks                    untyped    -                               -
##                        Req Tunable Trafo
## booster                  -    TRUE     -
## watchlist                -   FALSE     -
## eta                      -    TRUE     -
## gamma                    -    TRUE     -
## max_depth                -    TRUE     -
## min_child_weight         -    TRUE     -
## subsample                -    TRUE     -
## colsample_bytree         -    TRUE     -
## colsample_bylevel        -    TRUE     -
## num_parallel_tree        -    TRUE     -
## lambda                   -    TRUE     -
## lambda_bias              -    TRUE     -
## alpha                    -    TRUE     -
## objective                -   FALSE     -
## eval_metric              -   FALSE     -
## base_score               -   FALSE     -
## max_delta_step           -    TRUE     -
## missing                  -   FALSE     -
## monotone_constraints     -    TRUE     -
## tweedie_variance_power   Y    TRUE     -
## nthread                  -   FALSE     -
## nrounds                  -    TRUE     -
## feval                    -   FALSE     -
## verbose                  -   FALSE     -
## print_every_n            Y   FALSE     -
## early_stopping_rounds    -   FALSE     -
## maximize                 -   FALSE     -
## sample_type              Y    TRUE     -
## normalize_type           Y    TRUE     -
## rate_drop                Y    TRUE     -
## skip_drop                Y    TRUE     -
## callbacks                -   FALSE     -</code></pre>
<p>Below we will tune the nrounds, max_depth, eta, and lambda hyper-parameters.</p>
<pre class="r"><code>xgb_params &lt;- makeParamSet(makeIntegerParam(&quot;nrounds&quot;, lower = 100, upper = 300),
  # number of splits in each tree
  makeIntegerParam(&quot;max_depth&quot;, lower = 1, upper = 10),
  # &quot;shrinkage&quot; - prevents overfitting
  makeNumericParam(&quot;eta&quot;, lower = .1, upper = .5),
  # L2 regularization - prevents overfitting
  makeNumericParam(&quot;lambda&quot;, lower = -1, upper = 0, trafo = function(x) 10^x)
)
control &lt;- makeTuneControlRandom(maxit = 1)

# Choose a resampling strategy
resample_desc = makeResampleDesc(&quot;CV&quot;, iters = 2)

# tuning 
tuned_params &lt;- tuneParams(
  learner = xgb_learner,
  task = train_task,
  resampling = resample_desc,
  par.set = xgb_params,
  control = control
)
xgb_tuned_learner&lt;- setHyperPars(
  learner = xgb_learner,
  par.vals = tuned_params$x
)
print(xgb_tuned_learner)</code></pre>
<pre><code>## Learner regr.xgboost from package xgboost
## Type: regr
## Name: eXtreme Gradient Boosting; Short name: xgboost
## Class: regr.xgboost
## Properties: numerics,weights,featimp,missings
## Predict-Type: response
## Hyperparameters: nrounds=242,verbose=0,eval_metric=mae,max_depth=4,eta=0.107,lambda=0.598</code></pre>
<p>The optimal hyperparameter results from tuning are shown above. Now, we can re-train our model with these results.</p>
<pre class="r"><code># Re-train parameters using tuned hyperparameters 
xgb_tuned_model &lt;- mlr::train(xgb_tuned_learner, train_task)

# Make a new prediction
result_tuned &lt;- predict(xgb_tuned_model, test_task)
performance(result_tuned,measures = mae)</code></pre>
<pre><code>##      mae 
## 13.92094</code></pre>
<p>The performance improved significantly with hyper-parameter tuning. This model performs better than the one created originally with the xgboost package.</p>
<pre class="r"><code>feat_imp&lt;-getFeatureImportance(xgb_tuned_model)

#plot top 20 features
feat_imp$res %&gt;% gather(key=&quot;feature&quot;,value=&quot;importance&quot;) %&gt;% top_n(20,importance) %&gt;% ggplot(aes(x=fct_reorder(feature,importance))) + geom_col(aes(y=importance))+coord_flip()</code></pre>
<p><img src="/post/2019-04-22-wait-times-at-walt-disney-world_files/figure-html/unnamed-chunk-24-1.png" width="672" />
This feature importance plot is very similar to that found above using xgboost untuned. The ride type is the most important feature in this model as well. Additionally, we see that the hour, holiday metric (holidaym), and month are significant.</p>
</div>
