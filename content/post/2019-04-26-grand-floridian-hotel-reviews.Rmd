---
title: Grand Floridian Hotel Reviews Sentiment Analysis & NLP
author: ''
date: '2019-08-11'
slug: grand-floridian-hotel-reviews
categories: []
tags:
  - sentiment analysis
  - NLP
  - keras
---
## Grand Floridian Hotel Reviews: Sentiment Analysis and NLP

```{r,message=FALSE,warning=FALSE}
library(tidyverse)
library(zoo)
library(knitr)
library(tidytext)
library(SnowballC)
library(keras)
library(kableExtra)

```

This is the raw version of the reviews data: it has the text content of the review, the star rating (1-5), and the date of stay (month/year).
```{r include=FALSE}
#load data
rdf<-readRDS("/Users/mikaylaedwards/Downloads/first_3000.Rda")
str(rdf)
```

It requires a bit of cleaning to get in the form we want. 
```{r}
#rename & change variable type
rdf <- rdf %>% rename(stay_date = id_date,
review = reviews) %>%
mutate_at(vars(stay_date, review),
~ as.character(.)) %>%
mutate(rating = as.factor(rating))

#change stay_date variable from character to date type

rdf$stay_date <- gsub("Date of stay: ", "", rdf$stay_date)
rdf$stay_date <- as.yearmon(rdf$stay_date, "%b %Y")
kable(head(rdf,3)) %>% kable_styling(bootstrap_options = "striped", full_width = F,font_size = 8) 

```

Above we see three example reviews. 
Now we can plot the share each rating category (1-5) over time.
```{r}
rdf %>% 
  ggplot(aes(x=stay_date))+geom_bar(aes(fill=rating))+scale_x_yearmon(format = "%b-%y")
```
It seems like most reviews have been consistenly positive. We can also analyze the distribution of reviews by each rating category.
```{r}
#set color
colors<-c("cadetblue1", "deepskyblue", "deepskyblue3","deepskyblue4","dodgerblue3")

rdf %>% 
  ggplot(aes(x=rating,fill=rating))+geom_bar()+scale_fill_manual(values=colors)
```
##Sentiment Analysis
Using the tidytext package, we first transform the text of the reviews into tidy format. We remove stop words and filter unwanted characters. Then, we can visualize the most common words in the text of all reviews.  
```{r,message=FALSE}

rdf_tidy <- rdf %>% mutate(review_num = row_number()) %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words) %>% filter(str_detect(word, "^[a-z']+$")) %>%
  distinct()

most_common_words <- rdf_tidy %>%
  group_by(word) %>%
  summarize(freq = n()) %>%
  arrange(desc(freq))

#rearrange by frequency, eliminate duplicates

most_common_words %>%
  mutate(word = reorder(word, freq)) %>%
  head(25) %>%  ggplot(aes(x = word, y = freq)) + geom_segment(aes(
    x = word ,
    xend = word,
    y = 0,
    yend = freq
  ), color = "black") +
  geom_point(size = 3, color = "deepskyblue") +
  labs(title = "Top 25 Most Common Words in Review Text", y = "# times used", x =
         "") + coord_flip()

```


If we want to compare word usage between good and bad reviews, we can use the inverse document frequency. In other words, which words appear frequently in high reviews but not low (and vice versa)?
```{r,message=FALSE,warning=FALSE}
#group rating 1&2 into "low" category; rating 3,4,& 5 & into "high" category
by_rate <- rdf %>%
mutate(rating_group = case_when(rating == "1" |
rating == "2" ~ "low", TRUE ~ "high"))


by_rate_inverse <- by_rate %>% unnest_tokens(word, review) %>%
count(rating_group, word, sort = TRUE) %>% filter(!nchar(word) <= 3) %>%
bind_tf_idf(word, rating_group, n) %>% group_by(rating_group) %>%
mutate(rank = rank(desc(tf_idf), ties.method = "first")) %>%
arrange(rank)

by_rate_inverse %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(rating_group) %>% filter(rank <= 10) %>%
ungroup() %>%
ggplot(aes(word, tf_idf, fill = rating_group)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap( ~ rating_group, ncol = 2, scales = "free") +
coord_flip()

```

We can also visualize the most common "bigrams" (i.e. group of two words). To do this, we first separate the unnested bigram and filter for stop words. Then we put them back together and count the frequency of appearance.
```{r}
##bigram: separate-->filter out stop words; put back together-->count
bigram_filt<-rdf %>% 
  unnest_tokens(bigram, review, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 


bigram_count <- bigram_filt %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)

bigram_count %>% filter(!str_detect(bigram,"floridian")) %>% head(25) %>%  mutate(bigram = reorder(bigram, n)) %>% 
  ggplot(aes(x=bigram,y=n))+ geom_segment(aes(x=bigram ,xend=bigram, y=0, yend=n),color="black") +
  geom_point(size=3, color="deepskyblue")+
  labs(title = "Top 25 Most Common Bigrams in Review Text", y = "# times used", x="") + coord_flip()


```
Some of the bigram results are particularly interesting, such as "1900 park" (the on-site restaraunt) and "sugar load" (the name of the club level). It also makes sense that "Magic Kingdom" (located right next to the GF) is the most commonly used. 


The tidytext package has several lexicons (stored as "sentiments") that can be used for sentiment analysis. Below, we explore the number of distint words in each lexicon. We exclude "loughran", the finance lexicon.
```{r}
bing<-get_sentiments(lexicon ="bing") %>% mutate(lexicon="bing")
afinn<-get_sentiments(lexicon ="afinn") %>% rename(sentiment=value) %>% mutate(lexicon="afinn")
sentiments_df<-rbind(bing,afinn)

sentiments_df <- sentiments_df %>%
  group_by(lexicon) %>%
  mutate(words_in_lexicon = n_distinct(word)) %>%
  ungroup()

sentiments_df %>%
  group_by(lexicon) %>%
  summarise(distinct_words = n_distinct(word)) %>%
  ungroup() 
```
We can see how many words in the review text are found in each lexicon. 
```{r message=FALSE, warning=FALSE}
rdf_tidy %>%
  mutate(words_in_reviews = n_distinct(word)) %>%
  inner_join(sentiments_df) %>%
  group_by(lexicon, words_in_reviews, words_in_lexicon) %>%
  summarise(num_match = n_distinct(word)) %>%
  ungroup() %>%
  mutate(match_ratio = num_match / words_in_reviews) %>%
  select(lexicon, words_in_reviews, num_match, match_ratio)
```
The bing lexicon classifies words as either positive or negative. Using this, we can see when the most negative words were used in reviews. 
```{r,message=FALSE}
#get negative words
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

#normalize w/count of total words in reviews for given stay date
wordcounts <- rdf_tidy %>%
  group_by(stay_date) %>%
  summarize(words = n())

most_neg<-rdf_tidy %>%
  semi_join(bingnegative) %>%
  group_by(stay_date) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("stay_date")) %>%
  mutate(ratio_neg = negativewords/words,ratio_pos=1-ratio_neg) %>%
  ungroup()


most_neg %>% ggplot(aes(x=stay_date,y=ratio_neg,color=ratio_neg))+scale_x_yearmon(format = "%b-%y")+geom_line()
mean(most_neg$ratio_neg)
```
From this, we can see that 8% is the max percentage of negative words used in reviews over a given month. On average, it's about 4%. 
The AFINN lexicon provides a positivity score for each word, from -5 (most negative) to 5 (most positive). We can visualize how these positivity scores compare to the actual numeric rating (1-5) given. 
```{r}
##gives word and score
afinn <- get_sentiments("afinn") %>% 
select(word, afinn_score = value)

reviews_sentiment <- rdf_tidy %>%
inner_join(afinn, by = "word") %>%
group_by(review_num, rating) %>%
summarize(sentiment = mean(afinn_score))


ggplot(reviews_sentiment, aes(rating, sentiment, group = rating)) +
geom_boxplot(colour = "black",
fill = "#56B4E9",
notch = T) +
ylab("Average sentiment score")
```
Although there are outliers with low scores among 5 star reviews, we see that sentiment scores roughly correlate with the rating given. There is the largest gap between the 25th and 50th percentile in rating 1 group, indicating that some reviews here were quite negative.

Finally, we can visualize difference in word usage between positive and negative reviews by plotting the most frequently used words against average rating. To do this we obtain a per-word summary and find the average rating for its usage. 
```{r,warning=FALSE,message=FALSE}
#count number of times word used in each review
count_words <- rdf_tidy %>%
  count(review_num, rating, word) %>%
  ungroup() 

#returns most negative words
summary_neg<- count_words %>%  group_by(word) %>%
  summarize(reviews = n(),
            average_rat = mean(as.integer(rating))) %>% filter(reviews>10) %>% arrange(average_rat)



head(summary_neg,15) %>% ggplot(aes(x=reviews,y=average_rat))+ geom_point() +
  geom_label(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 0) +
  xlab("# of reviews") +
  ylab("Average Rating") + expand_limits(x = c(10,40))
```
Many of these are words we would expect in negative reviews. However, "waldorf" is interesting because it may reflect guest making a comparison or speaking highly of another resort.
```{r,warning=FALSE,message=FALSE}
summary_pos<- count_words %>%  group_by(word) %>%
  summarize(reviews = n(),
            average_rat = mean(as.integer(rating))) %>% filter(reviews>50) %>% arrange(desc(average_rat))

head(summary_pos,15) %>% ggplot(aes(x=reviews,y=average_rat))+ geom_point() +
  geom_label(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 0) +
  xlab("# of reviews") +
  ylab("Average Rating")
```
Similarly, many of these words are what we would expect in positive reviews--though "kitchen","pianist",and "orchestra" are notable. 

##Text Classification

Now, we will use keras to build a deep-learning model that classifies whether or not reviews are 5 star based on their text (i.e. the words used). 


```{r,message=FALSE,warning=FALSE}
#set seed
set.seed(42)
#shuffle dataframe
rows <- sample(nrow(rdf))
rdf <- rdf[rows, ]


rdf_bin <- rdf %>% mutate(review_num = row_number()) %>%
  mutate(five_star = case_when(rating != 5 ~ 0, TRUE ~ 1)) %>% select(-rating, -stay_date)

rdf_bin %>% ggplot(aes(x = five_star)) +
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat = "count") +
  scale_y_continuous(labels = scales::percent) + ylab("percent") +
  scale_x_discrete()


```
Almost 70% of the reviews are five star. This will be helpful as we evaluate model performance. 

We tokenize the review text into a sequence of integers where each word will correspond to an integer. To get the 1000 most frequent words in our text, we set num_words to 1000. 
```{r}

# get text of review
text<-rdf_bin$review
max_features <- 1000
tokenizer <- text_tokenizer(num_words = max_features)

#fit tokenizer to actual reviews
tokenizer %>% 
  fit_text_tokenizer(text)

#num reviews
print(tokenizer$document_count)

#list of integers for reviews 
text_seqs<- texts_to_sequences(tokenizer, text)

```

To begin constructing a neural network, we first define training data to map input tensors to target tensors. However, we can't feed the lists of integers (text_seqs) into the network--they must first be converted to tensors. This is accomplished by padding the lists to have the same length (with pad_sequences). Then the lists are converted into an integer tensor which has dimension num_reviews x max_length (2350x100). This is used to construct the first ("embedding") layer.
```{r}
#cut reviews off after 100 words
maxlen <- 100

#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)

y_train <- rdf_bin$five_star
length(y_train)
```
The network will learn 50-dimensional embeddings for each of the 1000 words. 
```{r}
# Network parameters
embedding_dims <- 50
filters <- 64
kernel_size <- 5
hidden_dims <- 50
epochs <- 10


```
#### Build Network: Layers, Hidden Units, Activation Function
Below, we build the model. 
```{r}
#input max_feat (total vocabulary count used for reviews:1000)

model <- keras_model_sequential() %>% 
  #embedding layer
  layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
  layer_dropout(0.2) %>%
  layer_conv_1d(
    filters, kernel_size, 
    padding = "valid", activation = "relu", strides = 1
  ) %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(units=hidden_dims, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units=1,activation = "sigmoid")

model %>% summary()
```
The model is built by sequentially stacking layers: 

1. The embedding layer input dimension is the vocab size. It looks up the embedding vector for each word-index and adds a dimension to the output.

2. 1D convolution layer using kernel size 5: recognize local patterns in sentence to learn words length <5, can recognize in later context

3. Additional dense layer with relu activation

4. The final layer uses the sigmoid activation function to output a single node: a probability score between 0 and 1 indicating how likely a review is to be positive. 

Additionally, dropout layers were added to combat overfitting. Weights below .2 were dropped by these layers.

#### Choosing Loss Function & Optimizer
We use the binary crossentropy loss function since we are solving a binary classification problem. The optimizer is a stochastic gradient descent (SGD) variant that specifies how the network weights will be updated. This can be modified via hyperparameter tuning, but I found that rmsprop performed better than others (e.g. adam) here. We also track accuracy metrics during training.
```{r}
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "rmsprop",
  metrics = "accuracy"
)
```

We train the model in mini-batches of size 32 for 10 iterations (epochs) over the training set but set aside 20% of our reviews for validation. 
```{r,message=FALSE,warning=FALSE}
hist <- model %>%
  fit(
    x_train,
    y_train,
    batch_size = 32,
    epochs = 10,
    validation_split = 0.2
  )

print(hist)
```

```{r}
plot(hist)
```
The training loss decreases with each epoch, while the accuracy increases. However, the validation data does not show this same trend, which indicates that we are overfitting. The right number of epochs appears to be 6.  Considering the fact that we only use the 1000 most common words and cut reviews off at 100 words, our model is fairly accurate (peaking at around 80% on the validation set). However, we can likely increase this by increasing the feature space and length of review considered.

```{r}
#increase num words for embedding
max_features<-5000
maxlen<-400
#increase length of review considered
x_train_long <- pad_sequences(text_seqs, maxlen = maxlen)

model <- keras_model_sequential() %>% 
  #embedding layer
  layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
  layer_dropout(0.2) %>%
  layer_conv_1d(
    filters, kernel_size, 
    padding = "valid", activation = "relu", strides = 1
  ) %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(units=hidden_dims, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units=1,activation = "sigmoid")

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "rmsprop",
  metrics = "accuracy"
)
```


```{r}
hist <- model %>%
  fit(
    x_train_long,
    y_train,
    batch_size = 32,
    epochs = 10,
    validation_split = 0.2
  )

print(hist)
```


```{r}
plot(hist)
```

Again, 6 appears to be the right number of epochs--beyond this we are overfitting. Although validation accuracy improves to about 84% (a 4% increase from the previous model), there is a tradeoff in that this model takes more than 2 times the amount of time to train. If rapid classification is necessary, the smaller feature space performance may be sufficient. 





