---
title: 'Attending self-attention: A case study of visually grounded supervision in
  vision-and-language transformers'
authors:
- Jules Samaran
- Noa Garcia
- Mayu Otani
- Chenhui Chu
- Yuta Nakashima
date: '2021-08-01'
publishDate: '2024-01-12T12:26:47.919164Z'
publication_types:
- paper-conference
publication: '*Proc.~Annual Meeting of the Association for Computational Linguistics
  and the 11th International Joint Conference on Natural Language Processing: Student
  Research Workshop*'
abstract: The impressive performances of pre-trained visually grounded language models
  have motivated a growing body of research investigating what has been learned during
  the pre-training. As a lot of these models are based on Transformers, several studies
  on the attention mechanisms used by the models to learn to associate phrases with
  their visual grounding in the image have been conducted. In this work, we investigate
  how supervising attention directly to learn visual grounding can affect the behavior
  of such models. We compare three different methods on attention supervision and
  their impact on the performances of a state-of-the-art visually grounded language
  model on two popular vision-and-language tasks.
links:
- name: URL
  url: https://aclanthology.org/2021.acl-srw.8/
---
