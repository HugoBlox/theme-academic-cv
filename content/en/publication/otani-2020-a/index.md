---
title: Visually grounded paraphrase identification via gating and phrase localization
_build:
  render: never
  list: never
  publishResources: false
date: '2020-09-01'
publishDate: '2023-08-20T08:43:22.349463Z'
authors:
- Mayu Otani
- Chenhui Chu
- Yuta Nakashima
publication_types:
- '2'
abstract: Visually grounded paraphrases (VGPs) describe the same visual concept but
  in different wording. Previous studies have developed models to identify VGPs from
  language and visual features. In these existing methods, language and visual features
  are simply fused. However, our detailed analysis indicates that VGPs with different
  lexical similarities require different weights on language and visual features to
  maximize identification performance. This motivates us to propose a gated neural
  network model to adaptively control the weights. In addition, because VGP identification
  is closely related to phrase localization, we also propose a way to explicitly incorporate
  phrase-object correspondences. From our evaluation in detail, we confirmed our model
  outperforms the state-of-the-art model.
featured: false
publication: '*Neurocomputing*'
doi: https://doi.org/10.1016/j.neucom.2020.04.066
links:
- name: URL
  url: https://www.sciencedirect.com/science/article/pii/S0925231220306512
---

