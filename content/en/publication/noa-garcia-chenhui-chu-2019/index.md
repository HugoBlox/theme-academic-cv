---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Video meets knowledge in visual question answering
subtitle: ''
summary: ''
authors:
- Noa Garcia
- Chenhui Chu
- Mayu Otani
- Yuta Nakashima
tags: []
categories: []
date: '2019-01-01'
lastmod: 2023-02-15T15:27:43+09:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-08-20T08:43:26.796621Z'
publication_types:
- '1'
abstract: In this work, we address knowledge-based visual question answering in videos.
  First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer
  pairs that combines visual, textual and temporal coherence reasoning together with
  knowledge-based questions. Second, we propose a video understanding model by combining
  the visual and textual video information with specific knowledge about the dataset.
  We find that the incorporation of knowledge produces outstanding improvements for
  VQA in video. However, the performance on KnowIT VQA still lags well behind human
  accuracy, indicating its usefulness for studying current video modelling limitations.
publication: '*画像の認識・理解シンポジウム(MIRU2019)論文集*'
---
