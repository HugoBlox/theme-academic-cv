---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Cross-lingual visual grounding
subtitle: ''
summary: ''
authors:
- Wenjian Dong
- Mayu Otani
- Noa Garcia
- Yuta Nakashima
- Chenhui Chu
tags: []
categories: []
date: '2020-12-01'
lastmod: 2023-02-15T15:27:35+09:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-08-20T08:43:21.914469Z'
publication_types:
- '2'
abstract: Visual grounding is a vision and language understanding task aiming at locating
  a region in an image according to a specific query phrase. However, most previous
  studies only address this task for the English language. Although there are previous
  cross-lingual vision and language studies, they work on image and video captioning,
  and visual question answering. In this paper, we present the first work on cross-lingual
  visual grounding to expand the task to different languages to study an effective
  yet efficient way for visual grounding on other languages. We construct a visual
  grounding dataset for French via crowdsourcing. Our dataset consists of 14k, 3k,
  and 3k query phrases with their corresponding image regions for 5k, 1k, and 1k training,
  validation and test images, respectively. In addition, we propose a cross-lingual
  visual grounding approach that transfers the knowledge from a learnt English model
  to a French model. Despite that the size of our French dataset is 1/6 of the English
  dataset, experiments indicate that our model achieves an accuracy of 65.17%, which
  is comparable to the accuracy 69.04% of the English model. Our dataset and codes
  are available at https://github.com/ids-cv/Multi-Lingual-Visual-Grounding.
publication: '*IEEE Access*'
doi: https://doi.org/10.1109/ACCESS.2020.3046719
links:
- name: URL
  url: https://ieeexplore.ieee.org/document/9305199
---
