---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Deep Gesture Generation for Social Robots Using Type-Specific Libraries
subtitle: ''
summary: ''
authors:
- Hitoshi Teshima
- Naoki Wake
- Diego Thomas
- Yuta Nakashima
- Hiroshi Kawasaki
- Katsushi Ikeuchi
tags: []
categories: []
date: '2022-01-01'
lastmod: 2023-02-15T15:27:32+09:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-02-15T06:27:32.514891Z'
publication_types:
- '1'
abstract: 'Body language such as conversational gesture is a powerful way to ease
  communication. Conversational gestures do not only make a speech more lively but
  also contain semantic meaning that helps to stress important information in the
  discussion. In the field of robotics, giving conversational agents (humanoid robots
  or virtual avatars) the ability to properly use gestures is critical, yet remain
  a task of extraordinary difficulty. This is because given only a text as input,
  there are many possibilities and ambiguities to generate an appropriate gesture.
  Different to previous works we propose a new method that explicitly takes into account
  the gesture types to reduce these ambiguities and generate human-like conversational
  gestures. Key to our proposed system is a new gesture database built on the TED
  dataset that allows us to map a word to one of three types of gestures: “Imagistic”
  gestures, which express the content of the speech, “Beat” gestures, which emphasize
  words, and “No gestures.” We propose a system that first maps the words in the input
  text to their corresponding gesture type, generate type-specific gestures and combine
  the generated gestures into one final smooth gesture. In our comparative experiments,
  the effectiveness of the proposed method was confirmed in user studies for both
  avatar and humanoid robot.'
publication: '*2022 IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS)*'
---
