@inproceedings{wang2023learning,
 abstract = {Interpreting and explaining the behavior of deep neural networks is critical for many tasks. Explainable AI provides a way to address this challenge, mostly by providing per-pixel relevance to the decision. Yet, interpreting such explanations may require expert knowledge. Some recent attempts toward interpretability adopt a concept-based framework, giving a higher-level relationship between some concepts and model decisions. This paper proposes Bottleneck Concept Learner (BotCL), which represents an image solely by the presence/absence of concepts learned through training over the target task without explicit supervision over the concepts. It uses self-supervision and tailored regularizers so that learned concepts can be human-understandable. Using some image classification tasks as our testbed, we demonstrate BotCL's potential to rebuild neural networks for better interpretability.},
 author = {Bowen Wang and Liangzhi Li and Yuta Nakashima and Hajime Nagahara},
 booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 month = {6},
 pages = {10962--10971},
 title = {Learning bottleneck concepts in image classification},
 url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Learning_Bottleneck_Concepts_in_Image_Classification_CVPR_2023_paper.html},
 year = {2023}
}

