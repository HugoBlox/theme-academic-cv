---
title: Multi-modal humor segment prediction in video
authors:
- Zekun Yang
- nÌ†derlineYuta Nakashima
- Haruo Takemura
date: '2023-06-01'
publishDate: '2023-11-27T07:32:34.706212Z'
publication_types:
- article-journal
publication: '*Multimedia Systems*'
doi: https://doi.org/10.1007/s00530-023-01105-x
abstract: Humor can be induced by various signals in the visual, linguistic, and vocal
  modalities emitted by humans. Finding humor in videos is an interesting but challenging
  task for an intelligent system. Previous methods predict humor in the sentence level
  given some text (e.g., speech transcript), sometimes together with other modalities,
  such as videos and speech. Such methods ignore humor caused by the visual modality
  in their design, since their prediction is made for a sentence. In this work, we
  first give new annotations to humor based on a sitcom by setting up temporal segments
  of ground truth humor derived from the laughter track. Then, we propose a method
  to find these temporal segments of humor. We adopt an approach based on sliding
  window, where the visual modality is described by pose and facial features along
  with the linguistic modality given as subtitles in each sliding window. We use long
  short-term memory networks to encode the temporal dependency in poses and facial
  features and pre-trained BERT to handle subtitles. Experimental results show that
  our method improves the performance of humor prediction.
links:
- name: URL
  url: https://link.springer.com/article/10.1007/s00530-023-01105-x
---
