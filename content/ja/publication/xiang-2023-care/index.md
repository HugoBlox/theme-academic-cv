---
title: 'CARE-MI: Chinese benchmark for misinformation evaluation in maternity and
  infant care'
authors:
- Tong Xiang
- Liangzhi Li
- Wangyue Li
- Mingbai Bai
- Lu Wei
- Bowen Wang
- Noa Garcia
date: '2023-01-01'
publishDate: '2024-01-15T05:01:00.754930Z'
publication_types:
- paper-conference
publication: '*Proceedings of Neural Information Processing Systems, Datasets and
  Benchmarks Track*'
abstract: 'The recent advances in NLP, have led to a new trend of applying LLMs to
  real-world scenarios. While the latest LLMs are astonishingly fluent when interacting
  with humans, they suffer from the misinformation problem by unintentionally generating
  factually false statements. This can lead to harmful consequences, especially when
  produced within sensitive contexts, such as healthcare. Yet few previous works have
  focused on evaluating misinformation in the long-form generation of LLMs, especially
  for knowledge-intensive topics. Moreover, although LLMs have been shown to perform
  well in different languages, misinformation evaluation has been mostly conducted
  in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation
  in: 1) a sensitive topic, specifically the maternity and infant care domain; and
  2) a language other than English, namely Chinese. Most importantly, we provide an
  innovative paradigm for building long-form generation evaluation benchmarks that
  can be transferred to other knowledge-intensive domains and low-resourced languages.
  Our proposed benchmark fills the gap between the extensive usage of LLMs and the
  lack of datasets for assessing the misinformation generated by these models. It
  contains 1,612 expert-checked questions, accompanied with human-selected references.
  Using our benchmark, we conduct extensive experiments and found that current Chinese
  LLMs are far from perfect in the topic of maternity and infant care. In an effort
  to minimize the reliance on human resources for performance evaluation, we offer
  a judgment model for automatically assessing the long-form output of LLMs using
  the benchmark questions. Moreover, we compare potential solutions for long-form
  generation evaluation and provide insights for building more robust and efficient
  automated metric.'
links:
- name: URL
  url: https://arxiv.org/abs/2307.01458
---
