@article{yang2021bert,
 abstract = {With the goal of correctly answering questions about images or videos, visual question answering (VQA) has quickly developed in recent years. However, current VQA systems mainly focus on answering questions about a single image and face many challenges in answering video-based questions. VQA in video not only has to understand the evolution between video frames but also requires a certain understanding of corresponding subtitles. In this paper, we propose a language Transformer-based video question answering model to encode the complex semantics from video clips. Different from previous models which represent visual features by recurrent neural networks, our model encodes visual concept sequences with a pre-trained language Transformer. We investigate the performance of our model using four language Transformers over two different datasets. The results demonstrate outstanding improvements compared to previous work.},
 author = {Zekun Yang and Noa Garcia and Chenhui Chu and Mayu Otani and Yuta Nakashima and Haruo Takemura},
 doi = {https://doi.org/10.1016/j.neucom.2021.02.092},
 journal = {Neurocomputing},
 month = {7},
 pages = {pp.~121--133},
 title = {A comparative study of language Transformers for video question answering},
 url = {https://doi.org/10.1016/j.neucom.2021.02.092},
 volume = {445},
 year = {2021}
}

