---
title: A comparative study of language Transformers for video question answering
authors:
- Zekun Yang
- Noa Garcia
- Chenhui Chu
- Mayu Otani
- Yuta Nakashima
- Haruo Takemura
date: '2021-07-01'
publishDate: '2023-11-28T13:40:05.301093Z'
publication_types:
- article-journal
publication: '*Neurocomputing*'
doi: https://doi.org/10.1016/j.neucom.2021.02.092
abstract: With the goal of correctly answering questions about images or videos, visual
  question answering (VQA) has quickly developed in recent years. However, current
  VQA systems mainly focus on answering questions about a single image and face many
  challenges in answering video-based questions. VQA in video not only has to understand
  the evolution between video frames but also requires a certain understanding of
  corresponding subtitles. In this paper, we propose a language Transformer-based
  video question answering model to encode the complex semantics from video clips.
  Different from previous models which represent visual features by recurrent neural
  networks, our model encodes visual concept sequences with a pre-trained language
  Transformer. We investigate the performance of our model using four language Transformers
  over two different datasets. The results demonstrate outstanding improvements compared
  to previous work.
links:
- name: URL
  url: https://doi.org/10.1016/j.neucom.2021.02.092
---
