@article{chu2021semantic,
 abstract = {Visually grounded paraphrases (VGPs) are different phrasal expressions describing the same visual concept in an image. Previous studies treat VGP identification as a binary classification task, which ignores various phenomena behind VGPs (i.e., different linguistic interpretation of the same visual concept) such as linguistic paraphrases and VGPs from different aspects. In this paper, we propose semantic typology for VGPs, aiming to elucidate the VGP phenomena and deepen the understanding about how human beings interpret vision with language. We construct a large VGP dataset that annotates the class to which each VGP pair belongs according to our typology. In addition, we present a classification model that fuses language and visual features for VGP classification on our dataset. Experiments indicate that joint language and vision representation learning is important for VGP classification. We further demonstrate that our VGP typology can boost the performance of visually grounded textual entailment.},
 author = {Chu, Chenhui and Oliveira, Vinicius and Virgo, Felix Giovanni and Otani, Mayu and Garcia, Noa and Yuta Nakashima},
 doi = {https://doi.org/10.1016/j.cviu.2021.103333},
 journal = {Computer Vision and Image Understanding},
 month = {12},
 pages = {10 pages},
 title = {The semantic typology of visually grounded paraphrases},
 url = {https://www.sciencedirect.com/science/article/pii/S1077314221001697},
 volume = {215},
 year = {2021}
}

