@inproceedings{teshima2023act2g,
author = {Hitoshi Teshima and Naoki Wake and Diego Thomas and Yuta Nakashima and Hiroshi Kawasaki and Katsushi Ikeuchi},
title = {{ACT2G}: Attention-based Contrastive Learning for Text-to-Gesture Generation},
booktitle = {Proceedings of the ACM on Computer Graphics and Interactive Techniques},
pages = {35:1--35:17},
year = {2023},
url = {https://dl.acm.org/doi/abs/10.1145/3606940},
doi = {https://doi.org/10.1145/3606940},
abstract = {Recent increase of remote-work, online meeting and tele-operation task makes people find that gesture for avatars and communication robots is more important than we have thought. It is one of the key factors to achieve smooth and natural communication between humans and AI systems and has been intensively researched. Current gesture generation methods are mostly based on deep neural network using text, audio and other information as the input, however, they generate gestures mainly based on audio, which is called a beat gesture. Although the ratio of the beat gesture is more than 70% of actual human gestures, content based gestures sometimes play an important role to make avatars more realistic and human-like. In this paper, we propose a attention-based contrastive learning for text-to-gesture (ACT2G), where generated gestures represent content of the text by estimating attention weight for each word from the input text. In the method, since text and gesture features calculated by the attention weight are mapped to the same latent space by contrastive learning, once text is given as input, the network outputs a feature vector which can be used to generate gestures related to the content. User study confirmed that the gestures generated by ACT2G were better than existing methods. In addition, it was demonstrated that wide variation of gestures were generated from the same text by changing attention weights by creators.},
}

@inproceedings{zhao2023fakenews,
abstract = {Fake news detection in social media has become increasingly important due to the rapid proliferation of personal media channels and the consequential dissemination of misleading information. Existing methods, which primarily rely on multimodal features and graph-based techniques, have shown promising performance in detecting fake news. However, they still face a limitation, i.e., sparsity in graph connections, which hinders capturing possible interactions among tweets. This challenge has motivated us to explore a novel method that densifies the graph's connectivity to capture denser interaction better. Our method constructs a cross-modal tweet graph using CLIP, which encodes images and text into a unified space, allowing us to extract potential connections based on similarities in text and images. We then design a Feature Contextualization Network with Label Propagation (FCN-LP) to model the interaction among tweets as well as positive or negative correlations between predicted labels of connected tweets. The propagated labels from the graph are weighted and aggregated for the final detection. To enhance the model's generalization ability to unseen events, we introduce a domain generalization loss that ensures consistent features between tweets on seen and unseen events. We use three publicly available fake news datasets, Twitter, PHEME, and Weibo, for evaluation. Our method consistently improves the performance over the state-of-the-art methods on all benchmark datasets and effectively demonstrates its aptitude for generalizing fake news detection in social media.},
author = {Wanqing Zhao and Yuta Nakashima and Haiyuan Chen and Noboru Babaguchi},
title = {Enhancing Fake News Detection in Social Media via Label Propagation on Cross-modal Tweet Graph},
booktitle = {Proc. ACM International Conference on Multimedia},
pages = {2400--2408},
year = {2023},
url = {https://dl.acm.org/doi/abs/10.1145/3581783.3612086},
doi = {https://doi.org/10.1145/3581783.3612086},
}


@article{okita2023atlantoaxial,
abstract = {This work aims to develop a deep learning model, assessing atlantoaxial subluxation (AAS) in rheumatoid arthritis (RA), which can often be ambiguous in clinical practice.},
author = {Yasutaka Okita and Toru Hirano and Bowen Wang and Yuta Nakashima and Saki Minoda and Hajime Nagahara and Atsushi Kumanogoh},
title = {Automatic evaluation of atlantoaxial subluxation in rheumatoid arthritis by a deep learning model},
volume = {25},
number = {1},
pages = {181:1--181:7},
journal = {Arthritis Research \& Therapy},
year = {2023},
url = {https://link.springer.com/article/10.1186/s13075-023-03172-x},
doi = {https://doi.org/10.1186/s13075-023-03172-x},
}

@article{Pham2023,
abstract={In this paper, we present a prototype pseudo-direct time-of-flight (ToF) CMOS image sensor, achieving high distance accuracy, precision, and robustness to multipath interference. An indirect ToF (iToF)-based image sensor, which enables high spatial resolution, is used to acquire temporal compressed signals in the charge domain. Whole received light waveforms, like those acquired with conventional direct ToF (dToF) image sensors, can be obtained after image reconstruction based on compressive sensing. Therefore, this method has the advantages of both dToF and iToF depth image sensors, such as high resolution, high accuracy, immunity to multipath interference, and the absence of motion artifacts. Additionally, two approaches to refine the depth resolution are explained: (1) the introduction of a sub-time window; and (2) oversampling in image reconstruction and quadratic fitting in the depth calculation. Experimental results show the separation of two reflections 40 cm apart under multipath interference conditions and a significant improvement in distance precision down to around 1 cm. Point cloud map videos demonstrate the improvements in depth resolution and accuracy. These results suggest that the proposed method could be a promising approach for virtually implementing dToF imaging suitable for challenging environments with multipath interference.},
author={Pham, Anh Ngoc and Ibrahim Thoriq and Yasutomi, Keita and Kawahito, Shoji and Nagahara, Hajime and Kagawa, Keiichiro Yagi},
title = {Depth Quality Improvement with a 607 MHz Time-Compressive Computational Pseudo-dToF CMOS Image Sensor},
journal={MDPI Sensors},
volume={23(23)},
number={9332},
pages={1-13},
year={2023},
url={https://www.mdpi.com/1424-8220/23/23/9332}
}

@article{Thuong2023,
abstract={Lensless imaging protects visual privacy by capturing heavily blurred images that are imperceptible for humans to recognize the subject but contain enough information for machines to infer information. Unfortunately, protecting visual privacy comes with a reduction in recognition accuracy and vice versa. We propose a learnable lensless imaging framework that protects visual privacy while maintaining recognition accuracy. To make captured images imperceptible to humans, we designed several loss functions based on total variation, invertibility, and the restricted isometry property. We studied the effect of privacy protection with blurriness on the identification of personal identity via a quantitative method based on a subjective evaluation. Moreover, we validate our simulation by implementing a hardware realization of lensless imaging with photo-lithographically printed masks.},
title ={Human-Imperceptible Identification With Learnable Lensless Imaging},
author={Canh, Thuong Nguyen and Ngo, Trung Thanh and Nagahara, Hajime},
journal={IEEE Access},
volume={11},
pages={95724--95733},
year={2023},
url={https://ieeexplore.ieee.org/document/10227292}
}

@article{yoshida2023,
abstract={A camera captures multidimensional information of the real world by convolving it into two dimensions using a sensing matrix. The original multidimensional information is then reconstructed from captured images. Traditionally, multidimensional information has been captured by uniform sampling, but by optimizing the sensing matrix, we can capture images more efficiently and reconstruct multidimensional information with high quality. Although compressive video sensing requires random sampling as a theoretical optimum, when designing the sensing matrix in practice, there are many hardware limitations (such as exposure and color filter patterns). Existing studies have found random sampling is not always the best solution for compressive sensing because the optimal sampling pattern is related to the scene context, and it is hard to manually design a sampling pattern and reconstruction algorithm. In this paper, we propose an end-to-end learning approach that jointly optimizes the sampling pattern as well as the reconstruction decoder. We applied this deep sensing approach to the video compressive sensing problem. We modeled the spatio–temporal sampling and color filter pattern using a convolutional neural network constrained by hardware limitations during network training. We demonstrated that the proposed method performs better than the manually designed method in gray-scale video and color video acquisitions.},
title ={Deep Sensing for Compressive Video Acquisition},
author={Yoshida, Michitaka and Torii, Akihiko and Okutomi, Masatoshi and Taniguchi, Rin-ichiro and Nagahara, Hajime and Yagi, Yasushi },
journal={MDPI Sensors},
volume={23(17)},
number={7535},
pages={1--19},
year={2023},
url={https://www.mdpi.com/1424-8220/23/17/7535}
}

@article{wang2023realtime,
    author = {Bowen Wang and Liangzhi Li and \underline{Yuta Nakashima} and Ryo Kawasaki and Hajime Nagahara},
    title = {Real-time estimation of the remaining surgery duration for cataract surgery using deep convolutional neural networks and long short-term memory},
    journal = {BMC Medical Informatics and Decision Making},
    year = {2023},
    month = 5,
    volume = {23},
    number = {1},
    pages = {80},
    doi = {https://doi.org/10.1186/s12911-023-02160-0},
    url = {https://link.springer.com/article/10.1186/s12911-023-02160-0},
    abstract = {Estimating the surgery length has the potential to be utilized as skill assessment, surgical training, or efficient surgical facility utilization especially if it is done in real-time as a remaining surgery duration (RSD). Surgical length reflects a certain level of efficiency and mastery of the surgeon in a well-standardized surgery such as cataract surgery. In this paper, we design and develop a real-time RSD estimation method for cataract surgery that does not require manual labeling and is transferable with minimum fine-tuning.}
}

@article{yang2023multi,
author = {Zekun Yang and \underline{Yuta Nakashima} and Haruo Takemura},
title = {Multi-modal humor segment prediction in video},
journal = {Multimedia Systems},
year = {2023},
month = 6,
volume={29},
pages = {2389--2398},
doi = {https://doi.org/10.1007/s00530-023-01105-x},
url = {https://link.springer.com/article/10.1007/s00530-023-01105-x},
abstract = {Humor can be induced by various signals in the visual, linguistic, and vocal modalities emitted by humans. Finding humor in videos is an interesting but challenging task for an intelligent system. Previous methods predict humor in the sentence level given some text (e.g., speech transcript), sometimes together with other modalities, such as videos and speech. Such methods ignore humor caused by the visual modality in their design, since their prediction is made for a sentence. In this work, we first give new annotations to humor based on a sitcom by setting up temporal segments of ground truth humor derived from the laughter track. Then, we propose a method to find these temporal segments of humor. We adopt an approach based on sliding window, where the visual modality is described by pose and facial features along with the linguistic modality given as subtitles in each sliding window. We use long short-term memory networks to encode the temporal dependency in poses and facial features and pre-trained BERT to handle subtitles. Experimental results show that our method improves the performance of humor prediction.}
}

@inproceedings{habault2023icdar,
    author = {Guillaume Habault and Minh-Son Dao and Michael Alexander Riegler and Duc Tien Dang Nguyen and Yuta Nakashima and Cathal Gurrin},
    title = {ICDAR’23: Intelligent Cross-Data Analysis and Retrieval},
    booktitle = {Proc.~ACM International Conference on Multimedia Retrieval},
    year = {2023},
    month = 6,
    abstract = {Recently, there has been an increased interest in cross-data research problems, such as predicting air quality using life logging images, predicting congestion using weather and tweets data, and predicting sleep quality using daily exercises and meals. Although several research focusing on multimodal data analytics have been performed, few studies have been conducted on cross-data research (e.g., cross-modal data, cross-domain, cross-platform). The article collection “Intelligent Cross-Data Analysis and Retrieval” aims to encourage research in intelligent cross-data analytics and retrieval and contribute to the creation of a sustainable society. Researchers from diverse domains such as well-being, disaster prevention and mitigation, mobility, climate, tourism and healthcare are welcome to contribute to this Research Topic.}
}

@inproceedings{wu2023notonly,
    author = {Yankun Wu and Yuta Nakashima and Noa Garcia},
    title = {Not only generative art: Stable diffusion for content-style disentanglement in art analysis},
    booktitle = {Proc.~ 2023 ACM International Conference on Multimedia Retrieval (ICMR)},
    year = {2023},
    month = 6,
    pages = {199--208},
    url = {https://dl.acm.org/doi/abs/10.1145/3591106.3592262},
    doi = {https://doi.org/10.1145/3591106.3592262},
    abstract = {The duality of content and style is inherent to the nature of art. For humans, these two elements are clearly different: content refers to the objects and concepts in the piece of art, and style to the way it is expressed. This duality poses an important challenge for computer vision. The visual appearance of objects and concepts is modulated by the style that may reflect the author’s emotions, social trends, artistic movement, etc., and their deep comprehension undoubtfully requires to handle both. A promising step towards a general paradigm for art analysis is to disentangle content and style, whereas relying on human annotations to cull a single aspect of artworks has limitations in learning semantic concepts and the visual appearance of paintings. We thus present GOYA, a method that distills the artistic knowledge captured in a recent generative model to disentangle content and style. Experiments show that synthetically generated images sufficiently serve as a proxy of the real distribution of artworks, allowing GOYA to separately represent the two elements of art while keeping more information than existing methods.}
}

@inproceedings{wang2023learning,
    author = {Bowen Wang and Liangzhi Li and Yuta Nakashima and Hajime Nagahara},
    title = {Learning bottleneck concepts in image classification},
    booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2023},
    month = 6,
    pages = {10962--10971},
    url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Learning_Bottleneck_Concepts_in_Image_Classification_CVPR_2023_paper.html},
    abstract = {Interpreting and explaining the behavior of deep neural networks is critical for many tasks. Explainable AI provides a way to address this challenge, mostly by providing per-pixel relevance to the decision. Yet, interpreting such explanations may require expert knowledge. Some recent attempts toward interpretability adopt a concept-based framework, giving a higher-level relationship between some concepts and model decisions. This paper proposes Bottleneck Concept Learner (BotCL), which represents an image solely by the presence/absence of concepts learned through training over the target task without explicit supervision over the concepts. It uses self-supervision and tailored regularizers so that learned concepts can be human-understandable. Using some image classification tasks as our testbed, we demonstrate BotCL's potential to rebuild neural networks for better interpretability.}
}

@inproceedings{hirota2023model,
    author = {Yusuke Hirota and Yuta Nakashima and Noa Garcia},
    title = {Model-agnostic gender debiased image captioning},
    booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages = {15191--15200},
    month = 6,
    year = {2023},
    url = {https://openaccess.thecvf.com/content/CVPR2023/html/Hirota_Model-Agnostic_Gender_Debiased_Image_Captioning_CVPR_2023_paper.html},
    abstract = {Image captioning models are known to perpetuate and amplify harmful societal bias in the training set. In this work, we aim to mitigate such gender bias in image captioning models. While prior work has addressed this problem by forcing models to focus on people to reduce gender misclassification, it conversely generates gender-stereotypical words at the expense of predicting the correct gender. From this observation, we hypothesize that there are two types of gender bias affecting image captioning models: 1) bias that exploits context to predict gender, and 2) bias in the probability of generating certain (often stereotypical) words because of gender. To mitigate both types of gender biases, we propose a framework, called LIBRA, that learns from synthetically biased samples to decrease both types of biases, correcting gender misclassification and changing gender-stereotypical words to more neutral ones.}
}

@inproceedings{garcia2023uncurated,
    author = {Noa Garcia and Yusuke Hirota and Yankun Wu and Yuta Nakashima},
    title = {Uncurated image-text datasets: Shedding light on demographic bias},
    booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2023},
    month = 6,
    pages = {6957--6966},
    url = {https://openaccess.thecvf.com/content/CVPR2023/html/Garcia_Uncurated_Image-Text_Datasets_Shedding_Light_on_Demographic_Bias_CVPR_2023_paper.html},
    abstract = {The increasing tendency to collect large and uncurated datasets to train vision-and-language models has raised concerns about fair representations. It is known that even small but manually annotated datasets, such as MSCOCO, are affected by societal bias. This problem, far from being solved, may be getting worse with data crawled from the Internet without much control. In addition, the lack of tools to analyze societal bias in big collections of images makes addressing the problem extremely challenging. Our first contribution is to annotate part of the Google Conceptual Captions dataset, widely used for training vision-and-language models, with four demographic and two contextual attributes. Our second contribution is to conduct a comprehensive analysis of the annotations, focusing on how different demographic groups are represented. Our last contribution lies in evaluating three prevailing vision-and-language tasks: image captioning, text-image CLIP embeddings, and text-to-image generation, showing that societal bias is a persistent problem in all of them.}
}

@inproceedings{otani2023toward,
    author = {Mayu Otani and Riku Togashi and Yu Sawai and Ryosuke Ishigami and Yuta Nakashima and Esa Rahtu and Janne Heikkil\"{a} and Shin’ichi Satoh},
    title = {Toward verifiable and reproducible human evaluation for text-to-image generation},
    booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2023},
    month = 6,
    pages = {14277--14286},
    url = {https://openaccess.thecvf.com/content/CVPR2023/html/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.html},
    abstract = {Human evaluation is critical for validating the performance of text-to-image generative models, as this highly cognitive process requires deep comprehension of text and images. However, our survey of 37 recent papers reveals that many works rely solely on automatic measures (eg, FID) or perform poorly described human evaluations that are not reliable or repeatable. This paper proposes a standardized and well-defined human evaluation protocol to facilitate verifiable and reproducible human evaluation in future works. In our pilot data collection, we experimentally show that the current automatic measures are incompatible with human perception in evaluating the performance of the text-to-image generation results. Furthermore, we provide insights for designing human evaluation experiments reliably and conclusively. Finally, we make several resources publicly available to the community to facilitate easy and fast implementations.}
}

@inproceedings{li2023inverse,
  abstract={In this work, we propose an inverse rendering model that estimates 3D shape, spatially-varying reflectance, homogeneous subsurface scattering parameters, and an environment illumination jointly from only a pair of captured images of a translucent object. In order to solve the ambiguity problem of inverse rendering, we use a physically-based renderer and a neural renderer for scene reconstruction and material editing. Because two renderers are differentiable, we can compute a reconstruction loss to assist parameter estimation. To enhance the supervision of the proposed neural renderer, we also propose an augmented loss. In addition, we use a flash and no-flash image pair as the input. To supervise the training, we constructed a large-scale synthetic dataset of translucent objects, which consists of 117K scenes. Qualitative and quantitative results on both synthetic and real-world datasets demonstrated the effectiveness of the proposed model.},
  title={Inverse Rendering of Translucent Objects using Physical and Neural Renderers},
  author={Li, Chenhao and Ngo, Trung Thanh and Nagahara, Hajime},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12510--12520},
  year={2023}
} 


@inproceedings{mizuno2022dynamic,
author = {Mizuno, Ryoya and Takahashi, Keita and Yoshida, Michitaka and Tsutake, Chihiro and Fujii, Toshiaki and Nagahara, Hajime},
title = {Acquiring a Dynamic Light Field Through a Single-Shot Coded Image},
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {19830--19840},
year = {2022},
month = 6,
url = {https://openaccess.thecvf.com/content/CVPR2022/papers/Mizuno_Acquiring_a_Dynamic_Light_Field_Through_a_Single-Shot_Coded_Image_CVPR_2022_paper.pdf},
abstract = {We propose a method for compressively acquiring a dynamic light field (a 5-D volume) through a single-shot coded image (a 2-D measurement). We designed an imaging model that synchronously applies aperture coding and pixel-wise exposure coding within a single exposure time. This coding scheme enables us to effectively embed the original information into a single observed image. The observed image is then fed to a convolutional neural network (CNN) for light-field reconstruction, which is jointly trained with the camera-side coding patterns. We also developed a hardware prototype to capture a real 3-D scene moving over time. We succeeded in acquiring a dynamic light field with 5x5 viewpoints over 4 temporal sub-frames (100 views in total) from a single observed image. Repeating capture and reconstruction processes over time, we can acquire a dynamic light field at 4x the frame rate of the camera. To our knowledge, our method is the first to achieve a finer temporal resolution than the camera itself in compressive light-field acquisition. Our software is available from our project webpage.},
}

@article{yoshida2019highspeed,
author = {Yoshida, Michitaka and Sonoda, Toshiki and Nagahara, Hajime and Endo, Kenta and Sugiyama, Yukinobu and Taniguchi, Rin-ichiro},
title = {High-Speed Imaging Using CMOS Image Sensor With Quasi Pixel-Wise Exposure},
journal = {IEEE Transactions on Computational Imaging},
year = {2019},
pages = {463--476},
volume = {6},
url = {https://ieeexplore.ieee.org/abstract/document/8918110},
doi = {10.1109/TCI.2019.2956885},
abstract = {Several recent studies on compressive video sensing realized scene capture beyond the fundamental trade-off limit between spatial resolution and temporal resolution using random space-time sampling. However, most of these studies obtained results for higher-frame-rate video that was produced in simulation experiments or using an optically simulated random sampling camera, because there are currently no commercially available image sensors with random exposure or sampling capabilities. We fabricated a prototype complementary metal-oxide-semiconductor (CMOS) image sensor with quasi pixel-wise exposure timing that can realize nonuniform space-time sampling. The prototype sensor resets exposures independently by columns and fixes these exposures by rows for each 8 × 8-pixel block. This CMOS sensor is not fully controllable the pixels and has line-dependent control, but offers greater flexibility when compared with regular CMOS or charge-coupled device sensors with global or rolling shutters. We propose a method of realizing pseudo-random sampling for high-speed video acquisition that uses the flexibility of the CMOS sensor. We reconstruct the high-speed video sequence from images produced in pseudo-random sampling using a pre-learned decoder.},
}

@article{li2023cross,
  title={Cross-language font style transfer},
  author={Li, Chenhao and Taniguchi, Yuta and Lu, Min and Konomi, Shin'ichi and Nagahara, Hajime},
  journal={Applied Intelligence},
  pages={1--15},
  year={2023},
  abstract={In this paper, we propose a cross-language font style transfer system that can synthesize a new font by observing only a few samples from another language. Automatic font synthesis is a challenging task and has attracted much research interest. Most previous works addressed this problem by transferring the style of the given subset to the content of unseen ones. Nevertheless, they only focused on the font style transfer in the same language. In many cases, we need to learn font style from one language and then apply it to other languages. Existing methods make this difficult to accomplish because of the abstraction of style and language differences. To address this problem, we specifically designed the network into a multi-level attention form to capture both local and global features of the font style. To validate the generative ability of our model, we constructed an experimental font dataset of 847 fonts, each containing English and Chinese characters with the same style. Results show that our model generates 80.3% of users’ preferred images compared with state-of-the-art models.},
}

@article{wang2022match,
  title={Match them up: visually explainable few-shot image classification},
  author={Wang, Bowen and Li, Liangzhi and Verma, Manisha and Nakashima, Yuta and Kawasaki, Ryo and Nagahara, Hajime},
  journal={Applied Intelligence},
  pages={1--22},
  year={2022},
  url = {https://github.com/wbw520/MTUNet},
  doi = {https://doi.org/10.1007/s10489-022-04072-4},
  abstract={Few-shot learning (FSL) approaches, mostly neural network-based, assume that pre-trained knowledge can be obtained from base (seen) classes and transferred to novel (unseen) classes. However, the black-box nature of neural networks makes it difficult to understand what is actually transferred, which may hamper FSL application in some risk-sensitive areas. In this paper, we reveal a new way to perform FSL for image classification, using a visual representation from the backbone model and patterns generated by a self-attention based explainable module. The representation weighted by patterns only includes a minimum number of distinguishable features and the visualized patterns can serve as an informative hint on the transferred knowledge. On three mainstream datasets, experimental results prove that the proposed method can enable satisfying explainability and achieve high classification results. Code is available at https://github.com/wbw520/MTUNet.}
}

@article{goto2023development,
  title={Development of a vertex finding algorithm using Recurrent Neural Network},
  author={Goto, Kiichi and Suehara, Taikan and Yoshioka, Tamaki and Kurata, Masakazu and Nagahara, Hajime and Nakashima, Yuta and Takemura, Noriko and Iwasaki, Masako},
  journal={Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  volume={1047},
  pages={167836},
  year={2023},
  abstract={Deep learning is a rapidly-evolving technology with the possibility to significantly improve the physics reach of collider experiments. In this study we developed a novel vertex finding algorithm for future lepton colliders such as the International Linear Collider. We deploy two networks: one consists of simple fully-connected layers to look for vertex seeds from track pairs, and the other is a customized Recurrent Neural Network with an attention mechanism and an encoder–decoder structure to associate tracks to the vertex seeds. The performance of the vertex finder is compared with the standard ILC vertex reconstruction algorithm.}
}

@inproceedings{vo2022tone,
  title={Tone Classification for Political Advertising Video using Multimodal Cues},
  author={Vo, Anh-Khoa and Nakashima, Yuta},
  booktitle={Proceedings of the 3rd ACM Workshop on Intelligent Cross-Data Analysis and Retrieval},
  pages={17--21},
  year={2022},
  abstract={Politics has always gotten much attention throughout history, and video advertisement has become one of the most essential tools for political communication. Analysis of such political advertising videos can provide more insight into the political campaign by evaluating the message in it, such as a candidate's attitude toward a certain political issue. In this paper, we propose to classify the tone in political advertising videos into promotive, contrastive, and their mixture using a deep neural network to benefit automatic analysis of such videos. We especially explore how different modalities of videos, i.e., visuals, audio, and text, contribute to improving the classification accuracy.}
}

@inproceedings{dao2022icdar,
  title={ICDAR'22: Intelligent Cross-Data Analysis and Retrieval},
  author={Dao, Minh-Son and Riegler, Michael Alexander and Dang-Nguyen, Duc-Tien and Gurrin, Cathal and Nakashima, Yuta and Dong, Mianxiong},
  booktitle={Proceedings of the 2022 International Conference on Multimedia Retrieval},
  pages={690--691},
  year={2022},
  abstract={We have witnessed the rise of cross-data against multimodal data problems recently. The cross-modal retrieval system uses a textual query to look for images; the air quality index can be predicted using lifelogging images; the congestion can be predicted using weather and tweets data; daily exercises and meals can help to predict the sleeping quality are some examples of this research direction. Although vast investigations focusing on multimodal data analytics have been developed, few cross-data (e.g., cross-modal data, cross-domain, cross-platform) research has been carried on. In order to promote intelligent cross-data analytics and retrieval research and to bring a smart, sustainable society to human beings, the specific article collection on "Intelligent Cross-Data Analysis and Retrieval" is introduced. This Research Topic welcomes those who come from diverse research domains and disciplines such as well-being, disaster prevention and mitigation, mobility, climate change, tourism, healthcare, and food computing.}
}

@inproceedings{verma2022multi,
  title={Multi-label disengagement and behavior prediction in online learning},
  author={Verma, Manisha and Nakashima, Yuta and Takemura, Noriko and Nagahara, Hajime},
  booktitle={Artificial Intelligence in Education: 23rd International Conference, AIED 2022, Durham, UK, July 27--31, 2022, Proceedings, Part I},
  pages={633--639},
  year={2022},
  abstract={Student disengagement prediction in online learning environments is beneficial in various ways, especially to help provide timely cues to make some feedback or stimuli to the students. In this work, we propose a neural network-based model to predict students’ disengagement, as well as other behavioral cues, which might be relevant to students’ performance, using facial image sequences. For training and evaluating our model, we collected samples from multiple participants and annotated them with temporal segments of disengagement and other relevant behavioral cues with our multiple in-house annotators. We present prediction results of all behavior cues along with baseline comparison.}
}

@article{tanaka2022corpus,
  title={Corpus Construction for Historical Newspapers: A Case Study on Public Meeting Corpus Construction Using OCR Error Correction},
  author={Tanaka, Koji and Chu, Chenhui and Kajiwara, Tomoyuki and Nakashima, Yuta and Takemura, Noriko and Nagahara, Hajime and Fujikawa, Takao},
  journal={SN Computer Science},
  volume={3},
  number={6},
  pages={489},
  year={2022},
  publisher={Large text corpora are indispensable for natural language processing. However, in various fields such as literature and humanities, many documents to be studied are only scanned to images, but not converted to text data. Optical character recognition (OCR) is a technology to convert scanned document images into text data. However, OCR often misrecognizes characters due to the low quality of the scanned document images, which is a crucial factor that degrades the quality of constructed text corpora. This paper works on corpus construction for historical newspapers. We present a corpus construction method based on a pipeline of image processing, OCR, and filtering. To improve the quality, we further propose to integrate OCR error correction. To this end, we manually construct an OCR error correction dataset in the historical newspaper domain, propose methods to improve a neural OCR correction model and compare various OCR error correction models. We evaluate our corpus construction method on the accuracy of extracting articles of a specific topic to construct a historical newspaper corpus. As a result, our method improves the article extraction F score by 1.7% via OCR error correction comparing to previous work. This verifies the effectiveness of OCR error correction for corpus construction.}
}

@inproceedings{teshima2022deep,
  title={Deep Gesture Generation for Social Robots Using Type-Specific Libraries},
  author={Teshima, Hitoshi and Wake, Naoki and Thomas, Diego and Nakashima, Yuta and Kawasaki, Hiroshi and Ikeuchi, Katsushi},
  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={8286--8291},
  year={2022},
  abstract={Body language such as conversational gesture is a powerful way to ease communication. Conversational gestures do not only make a speech more lively but also contain semantic meaning that helps to stress important information in the discussion. In the field of robotics, giving conversational agents (humanoid robots or virtual avatars) the ability to properly use gestures is critical, yet remain a task of extraordinary difficulty. This is because given only a text as input, there are many possibilities and ambiguities to generate an appropriate gesture. Different to previous works we propose a new method that explicitly takes into account the gesture types to reduce these ambiguities and generate human-like conversational gestures. Key to our proposed system is a new gesture database built on the TED dataset that allows us to map a word to one of three types of gestures: “Imagistic” gestures, which express the content of the speech, “Beat” gestures, which emphasize words, and “No gestures.” We propose a system that first maps the words in the input text to their corresponding gesture type, generate type-specific gestures and combine the generated gestures into one final smooth gesture. In our comparative experiments, the effectiveness of the proposed method was confirmed in user studies for both avatar and humanoid robot.}
}

@inproceedings{pang2023contrastive,
  title={Contrastive Losses Are Natural Criteria for Unsupervised Video Summarization},
  author={Pang, Zongshang and Nakashima, Yuta and Otani, Mayu and Nagahara, Hajime},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2010--2019},
  year={2023},
  abstract={Video summarization aims to select a most informative subset of frames in a video to facilitate efficient video browsing. Unsupervised methods usually rely on heuristic training objectives such as diversity and representativeness. However, such methods need to bootstrap the online-generated summaries to compute the objectives for importance score regression. We consider such a pipeline inefficient and seek to directly quantify the frame-level importance with the help of contrastive losses in the representation learning literature. Leveraging the contrastive losses, we propose three metrics featuring a desirable key frame: local dissimilarity, global consistency, and uniqueness. With features pre-trained on an image classification task, the metrics can already yield high-quality importance scores, demonstrating better or competitive performance compared with past heavily-trained methods. We show that by refining the pre-trained features with contrastive learning, the frame-level importance scores can be further improved, and the model can learn from random videos and generalize to test videos with decent performance.}
}

@inproceedings{suzuki2022emotional,
  title={Emotional Intensity Estimation based on Writer’s Personality},
  author={Suzuki, Haruya and Tarumoto, Sora and Kajiwara, Tomoyuki and Ninomiya, Takashi and Nakashima, Yuta and Nagahara, Hajime},
  booktitle={Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: Student Research Workshop},
  pages={1--7},
  year={2022},
  abstract={We propose a method for personalized emotional intensity estimation based on a writer’s personality test for Japanese SNS posts. Existing emotion analysis models are difficult to accurately estimate the writer’s subjective emotions behind the text. We personalize the emotion analysis using not only the text but also the writer’s personality information. Experimental results show that personality information improves the performance of emotional intensity estimation. Furthermore, a hybrid model combining the existing personalized method with ours achieved state-of-the-art performance.}
}

@article{li2023automated,
  title={Automated grading system of retinal arterio-venous crossing patterns: A deep learning approach replicating ophthalmologist’s diagnostic process of arteriolosclerosis},
  author={Li, Liangzhi and Verma, Manisha and Wang, Bowen and Nakashima, Yuta and Nagahara, Hajime and Kawasaki, Ryo},
  journal={PLOS Digital Health},
  volume={2},
  number={1},
  pages={e0000174},
  year={2023},
  url={https://github.com/conscienceli/MDTNet},
  doi={https://doi.org/10.1371/journal.pdig.0000174},
  abstract={The morphological feature of retinal arterio-venous crossing patterns is a valuable source of cardiovascular risk stratification as it directly captures vascular health. Although Scheie’s classification, which was proposed in 1953, has been used to grade the severity of arteriolosclerosis as diagnostic criteria, it is not widely used in clinical settings as mastering this grading is challenging as it requires vast experience. In this paper, we propose a deep learning approach to replicate a diagnostic process of ophthalmologists while providing a checkpoint to secure explainability to understand the grading process. The proposed pipeline is three-fold to replicate a diagnostic process of ophthalmologists. First, we adopt segmentation and classification models to automatically obtain vessels in a retinal image with the corresponding artery/vein labels and find candidate arterio-venous crossing points. Second, we use a classification model to validate the true crossing point. At last, the grade of severity for the vessel crossings is classified. To better address the problem of label ambiguity and imbalanced label distribution, we propose a new model, named multi-diagnosis team network (MDTNet), in which the sub-models with different structures or different loss functions provide different decisions. MDTNet unifies these diverse theories to give the final decision with high accuracy. Our automated grading pipeline was able to validate crossing points with precision and recall of 96.3% and 96.3%, respectively. Among correctly detected crossing points, the kappa value for the agreement between the grading by a retina specialist and the estimated score was 0.85, with an accuracy of 0.92. The numerical results demonstrate that our method can achieve a good performance in both arterio-venous crossing validation and severity grading tasks following the diagnostic process of ophthalmologists. By the proposed models, we could build a pipeline reproducing ophthalmologists’ diagnostic process without requiring subjective feature extractions. The code is available (https://github.com/conscienceli/MDTNet).}
}

@article{virgo2022sncs,
author = {Felix Giovanni Virgo and Chenhui Chu and Takaya Ogawa and Koji Tanaka and Kazuki Ashihara and Yuta Nakashima and Noriko Takemura and Hajime Nagahara and Takao Fujikawa},
title = {Information Extraction from Public Meeting Articles},
journal = {SN Computer Science},
volume = {3},
number = {285},
year = {2022},
month = 5,
abstract = {Public meeting articles are the key to understanding the history of public opinion and public sphere in Australia. Information extraction from public meeting articles can obtain new insights into Australian history. In this paper, we create an information extraction dataset in the public meeting domain. We manually annotate the date and time, place, purpose, people who requested the meeting, people who convened the meeting, and people who were convened of 1258 public meeting articles. We further present an information extraction system, which formulates information extraction from public meeting articles as a machine reading comprehension task. Experiments indicate that our system can achieve an F1 score of 74.98% for information extraction from public meeting articles.},
}

@article{kuang2022privacy,
author = {Zhenzhong Kuang and Longbin Teng and Xingchi He and Jiajun Ding and Yuta Nakashima and Noboru Babaguchi}, 
title = {Anonymous identity sampling and reusable synthesis for sensitive face camouflage},
journal = {Journal of Electronic Imaging},
year = {2022},
volume = {31},
number = {2}, 
month = 3,
pages = {023011-1--023011-18},
doi = {https://doi.org/10.1117/1.JEI.31.2.023011},
url = {https://doi.org/10.1117/1.JEI.31.2.023011},
abstract = {An increasing amount of face images are being captured, shared, or applied in various applications. These images usually contain lots of sensitive information that may lead to privacy disclosure and misuse problems. Some pioneering works show that face image anonymization is one of the promising solutions. We present an innovative identity (ID) camouflage approach by synthesizing anonymous faces so that both artificial intelligence algorithms and humans are unable to recognize them and misuse them freely. Given a face image, our approach consists of two steps. First, we sample an anonymous ID feature point in the feature space. Then, we synthesize a camouflage face by training an anonymous deep generative adversarial network model. To reduce the risk of re-identification, we optimize our anonymous face generator based on the k-nearest neighbors to make a good balance between anonymity and utility of the original face image. The experimental results over the public dataset have verified the feasibility and state-of-the-art efficacy of our approach.},
}

@article{kumawat2021stft,
author = {Sudhakar Kumawat and Manisha Verma and Yuta Nakashima and Shanmuganathan Raman}, 
title = {Depthwise spatio-temporal {STFT} convolutional neural networks for human action recognition}, 
journal = {IEEE Trans.~Pattern Analysis and Machine Intelligence},
year = {2022},
pages = {4839--4851},
volume = {44},
url = {https://doi.ieeecomputersociety.org/10.1109/TPAMI.2021.3076522},
doi = {https://doi.org/10.1109/TPAMI.2021.3076522},
abstract = {Conventional 3D convolutional neural networks (CNNs) are computationally expensive, memory intensive, prone to overfitting, and most importantly, there is a need to improve their feature learning capabilities. To address these issues, we propose spatio-temporal short-term Fourier transform (STFT) blocks, a new class of convolutional blocks that can serve as an alternative to the 3D convolutional layer and its variants in 3D CNNs. An STFT block consists of non-trainable convolution layers that capture spatially and/or temporally local Fourier information using an STFT kernel at multiple low frequency points, followed by a set of trainable linear weights for learning channel correlations. The STFT blocks significantly reduce the space-time complexity in 3D CNNs. In general, they use 3.5 to 4.5 times less parameters and 1.5 to 1.8 times less computational costs when compared to the state-of-the-art methods. Furthermore, their feature learning capabilities are significantly better than the conventional 3D convolutional layer and its variants. Our extensive evaluation on seven action recognition datasets, including Something-Something v1 and v2, Jester, Diving-48, Kinetics-400, UCF 101, and HMDB 51, demonstrate that STFT blocks based 3D CNNs achieve on par or even better performance compared to the state-of-the-art methods.},
}

@article{chu2021semantic,
  title = {The semantic typology of visually grounded paraphrases},
  author = {Chu, Chenhui and Oliveira, Vinicius and Virgo, Felix Giovanni and Otani, Mayu and Garcia, Noa and Yuta Nakashima},
  journal = {Computer Vision and Image Understanding},
  year = {2021},
  volume = {215},
  month = 12,
  pages = {10 pages},
  doi = {https://doi.org/10.1016/j.cviu.2021.103333},
  url = {https://www.sciencedirect.com/science/article/pii/S1077314221001697},
  abstract = {Visually grounded paraphrases (VGPs) are different phrasal expressions describing the same visual concept in an image. Previous studies treat VGP identification as a binary classification task, which ignores various phenomena behind VGPs (i.e., different linguistic interpretation of the same visual concept) such as linguistic paraphrases and VGPs from different aspects. In this paper, we propose semantic typology for VGPs, aiming to elucidate the VGP phenomena and deepen the understanding about how human beings interpret vision with language. We construct a large VGP dataset that annotates the class to which each VGP pair belongs according to our typology. In addition, we present a classification model that fuses language and visual features for VGP classification on our dataset. Experiments indicate that joint language and vision representation learning is important for VGP classification. We further demonstrate that our VGP typology can boost the performance of visually grounded textual entailment.},
}

@article{wang2021noisy,
author = {Bowen Wang and Liangzhi Li and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara and Yasushi Yagi},
title = {{Noisy-LSTM}: Improving temporal awareness for video semantic segmentation},
journal = {IEEE Access}, 
volume = {9}, 
pages = {pp.~46810--46820},
year = {2021},
month = 3,
url = {https://doi.org/10.1109/ACCESS.2021.3067928},
doi = {https://doi.org/10.1109/ACCESS.2021.3067928},
abstract = {Semantic video segmentation is a key challenge for various applications. This paper presents a new model named Noisy-LSTM, which is trainable in an end-to-end manner, with convolutional LSTMs (ConvLSTMs) to leverage the temporal coherence in video frames, together with a simple yet effective training strategy that replaces a frame in a given video sequence with noises. Our training strategy spoils the temporal coherence in video frames and thus makes the temporal links in ConvLSTMs unreliable; this may consequently improve the ability of the model to extract features from video frames and serve as a regularizer to avoid overfitting, without requiring extra data annotations or computational costs. Experimental results demonstrate that the proposed model can achieve state-of-the-art performances on both the CityScapes and EndoVis2018 datasets. The code for the proposed method is available at https://github.com/wbw520/NoisyLSTM.},
}

@article{yang2021bert,
author = {Zekun Yang and Noa Garcia and Chenhui Chu and Mayu Otani and Yuta Nakashima and Haruo Takemura}, 
title = {A comparative study of language Transformers for video question answering}, 
journal = {Neurocomputing}, 
volume = {445}, 
pages = {pp.~121--133},
year = {2021},
month = 7,
url = {https://doi.org/10.1016/j.neucom.2021.02.092},
doi = {https://doi.org/10.1016/j.neucom.2021.02.092},
abstract = {With the goal of correctly answering questions about images or videos, visual question answering (VQA) has quickly developed in recent years. However, current VQA systems mainly focus on answering questions about a single image and face many challenges in answering video-based questions. VQA in video not only has to understand the evolution between video frames but also requires a certain understanding of corresponding subtitles. In this paper, we propose a language Transformer-based video question answering model to encode the complex semantics from video clips. Different from previous models which represent visual features by recurrent neural networks, our model encodes visual concept sequences with a pre-trained language Transformer. We investigate the performance of our model using four language Transformers over two different datasets. The results demonstrate outstanding improvements compared to previous work.},
}

@inproceedings{hirota2022facct,
author = {Yusuke Hirota and Yuta Nakashima and Noa Garcia},
title = {Gender and Racial Bias in Visual Question Answering Datasets},
booktitle = {Proc.~ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
year = {2022},
month = 6,
url = {https://facctconference.org/static/pdfs_2022/facct22-102.pdf},
addendum = {(accepted, \textbf{採択率: 25.1\%})}
}

@inproceedings{hirota2022cvpr,
author = {Yusuke Hirota and Yuta Nakashima and Noa Garcia}, 
title = {Quantifying Societal Bias Amplification in Image Captioning}, 
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
note = {10 pages},
year = {2022},
month = 6,
url = {https://openaccess.thecvf.com/content/CVPR2022/papers/Hirota_Quantifying_Societal_Bias_Amplification_in_Image_Captioning_CVPR_2022_paper.pdf},
abstract = {Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.},
}


@inproceedings{otani2022cvpr,
author = {Mayu Otani and Riku Togashi and Yuta Nakashima and Esa Rahtu and Janne Heikkil\"a and Shin'ichi Satoh}, 
title = {Optimal Correction Cost for Object Detection Evaluation}, 
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
note = {9 pages},
year = {2022},
month = 6,
url = {https://openaccess.thecvf.com/content/CVPR2022/papers/Otani_Optimal_Correction_Cost_for_Object_Detection_Evaluation_CVPR_2022_paper.pdf}
abstract = {Mean Average Precision (mAP) is the primary evaluation measure for object detection. Although object detection has a broad range of applications, mAP evaluates detectors in terms of the performance of ranked instance retrieval. Such the assumption for the evaluation task does not suit some downstream tasks. To alleviate the gap between downstream tasks and the evaluation scenario, we propose Optimal Correction Cost (OC-cost), which assesses detection accuracy at image level. OC-cost computes the cost of correcting detections to ground truths as a measure of accuracy. The cost is obtained by solving an optimal transportation problem between the detections and the ground truths. Unlike mAP, OC-cost is designed to penalize false positive and false negative detections properly, and every image in a dataset is treated equally. Our experimental result validates that OC-cost has better agreement with human preference than a ranking-based measure, i.e., mAP for a single image. We also show that detectors' rankings by OC-cost are more consistent on different data splits than mAP. Our goal is not to replace mAP with OC-cost but provide an additional tool to evaluate detectors from another aspect. To help future researchers and developers choose a target measure, we provide a series of experiments to clarify how mAP and OC-cost differ.},
}


@inproceedings{togashi2022cvpr,
author = {Riku Togashi and Mayu Otani and Yuta Nakashima and Esa Rahtu, Janne Heikkil\"a and Tetsuya Sakai}, 
title = {{AxIoU}: An Axiomatically Justified Measure for Video Moment Retrieval}, 
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
note = {10 pages},
year = {2022},
month = 6,
url = {https://openaccess.thecvf.com/content/CVPR2022/papers/Togashi_AxIoU_An_Axiomatically_Justified_Measure_for_Video_Moment_Retrieval_CVPR_2022_paper.pdf},
abstract = {Evaluation measures have a crucial impact on the direction of research. Therefore, it is of utmost importance to develop appropriate and reliable evaluation measures for new applications where conventional measures are not well suited. Video Moment Retrieval (VMR) is one such application, and the current practice is to use R@K,θ for evaluating VMR systems. However, this measure has two disadvantages. First, it is rank-insensitive: It ignores the rank positions of successfully localised moments in the top-K ranked list by treating the list as a set. Second, it binarizes the Intersection over Union (IoU) of each retrieved video moment using the threshold θ and thereby ignoring fine-grained localisation quality of ranked moments.
We propose an alternative measure for evaluating VMR, called Average Max IoU (AxIoU), which is free from the above two problems. We show that AxIoU satisfies two important axioms for VMR evaluation, namely, Invariance against Redundant Moments and Monotonicity with respect to the Best Moment, and also that R@K,θ satisfies the first axiom only. We also empirically examine how AxIoU agrees with R@K,θ, as well as its stability with respect to change in the test data and human-annotated temporal boundaries.},
}

@inproceedings{teshima2022,
title = {Integration of gesture generation system using gesture library with {DIY} robot design kit},
author = {Hitoshi Teshima and Naoki Wake and Diego Thomas and Yuta Nakashima and David Baumert and Hiroshi Kawasaki and Katsushi Ikeuchi},
year = {2022},
month = 1,
booktitle = {Proc.~IEEE/SICE International Symposium on System Integration (SII)},
pages = {361--366},
url = {https://ieeexplore.ieee.org/document/9708837},
abstract = {Conversational agents are expected to improve the quality of communication by adding gestures to the speech, and are considered to be a promising tool. Recent data-driven methods are capable of attaching gestures to arbitrary speech, but the output is still not in line with human intuition. Therefore, we propose a gesture transformation system that utilizes gesture types as intermediate information, based on the theory of psycholinguistics. We employ the gesture-first principle to create gesture clusters based on gesture similarities among imagistic gestures, one type of gesture to represent image-like motions, which are considered to represent important concepts in conversations. Since this system explicitly takes into account the gesture types recognized by a deep neural network (DNN) and the semantics of the sentence to select gestures, it is expected to output gestures that are more in line with human intuition than existing end-to-end systems that do not place these intermediate states. We prepared a DIY robot kit consisting of cheap parts so that conversational agents at home become available to ordinary users, and implemented the proposed gesture generation system on this robot. In order to evaluate the effectiveness of the conversational agent, we evaluated user impression when using various media for conversation and confirmed the advantage of using our agent.},
}

@inproceedings{shoji2021, 
title = {Museum Experience into a Souvenir: Generating Memorable Postcards from Guide Device Behavior Log},
Author = {Yoshiyuki Shoji and Kenro Aihara and Noriko Kando and Yuta Nakashima and Hiroaki Ohshima and Shio Takidaira and Masaki Ueta and Takehiro Yamamoto and Yusuke Yamamoto},
year = {2021},
month = 9, 
booktitle = {Proc.~ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
pages = {120-129},
url = {https://ieeexplore.ieee.org/document/9651764},
doi = {https://doi.org/10.1109/JCDL52503.2021.00024},
abstract = {This paper proposes a method for automatically generating postcards that reflect each visitor's museum experience by analyzing the log of our original iPad app that supports and guides personalized navigation in the National Museum of Ethnology. Museum experiences have become more personalized with the evolution of guiding devices. Each visitor views the different exhibits in a different order. Souvenirs serve to remind visitors of their museum experience and cement it in their memories; thus, souvenir postcards should be tailored to each visitor's museum experience. Such tailored postcards can effectively remind visitors of their experiences, deepen their impressions when they look back at them, and promote post-learning. In this paper, we proposed a system that automatically generates a postcard for each visitor for each visit by selecting the five most relevant and impressive exhibits based on the search and navigation logs on our museum guide app. We analyzed the search logs of the guide devices based on the psychological effects on impressions and memory to estimate which exhibits had the strongest impact on the visitors. We then conducted an in laboratory controlled user experiment with 16 participants to check what exhibits had made an impression on visitors by using the implemented system. The results showed that the exhibits that were seen frequently and the exhibits that participants added to their favorites were the most memorable.}
}

@inproceedings{wu2021transferring,
author = {Tianran Wu and Noa Garcia and Mayu Otani and Chenhui Chu and Yuta Nakashima and Haruo Takemura},
title = {Transferring domain-agnostic knowledge in video question answering},
booktitle = {Proc.~British Machine Vision Conference (BMVC)},
month = 11,
year = {2021},
note = {13 pages},
}

@inproceedings{vaigh2021gcnboost,
author = {Cheikh Brahim El Vaigh and Noa Garcia and Benjamin Renoust and Chenhui Chu and Yuta Nakashima and Hajime Nagahara},
title = {{GCNBoost}: Artwork Classificationby Label Propagation Through a Knowledge Graph},
booktitle = {Proc.~ACM International Conference on Multimedia Retrieval (ICMR)},
month = 11,
year = {2021},
note = {92--100},
url = {https://www.bmvc2021-virtualconference.com/assets/papers/1187.pdf},
abstract = {Video question answering (VideoQA) is designed to answer a given question based on a relevant video clip. The current available large-scale datasets have made it possible to formulate VideoQA as the joint understanding of visual and language information. However, this training procedure is costly and still less competent with human performance. In this paper, we investigate a transfer learning method by the introduction of domain-agnostic knowledge and domain-specific knowledge. First, we develop a novel transfer learning framework, which finetunes the pre-trained model by applying domain-agnostic knowledge as the medium. Second, we construct a new VideoQA dataset with 21,412 human-generated question-answer samples for comparable transfer of knowledge. Our experiments show that: (i) domain-agnostic knowledge is transferable and (ii) our proposed transfer learning framework can boost VideoQA performance effectively.},
}

@inproceedings{wang2021image,
author = {Bowen Wang and Liangzhi Li and Yuta Nakashima and Takehiro Yamamoto and Hiroaki Ohshima and Yoshiyuki Shoji and Kenro Aihara and Noriko Kando},
title = {Image Retrieval by Hierarchy-aware Deep Hashing Based on Multi-task Learning},
booktitle = {Proc.~ACM International Conference on Multimedia Retrieval (ICMR)},
month = 11,
year = {2021},
pages = {486--490},
url = {https://dl.acm.org/doi/10.1145/3460426.3463586},
abstract = {Deep hashing has been widely used to approximate nearest-neighbor search for image retrieval tasks. Most of them are trained with image-label pairs without any inter-label relationship, which may not make full use of the real-world data. This paper presents deep hashing, named HA2SH, that leverages multiple types of labels with hierarchical structures that an ethnological museum assigns to their artifacts. We experimentally prove that HA2SH can learn to generate hashes that give a better retrieval performance. Our code is available at https://github.com/wbw520/minpaku.},
}

@inproceedings{qian2021built,
author = {Yiming Qian and Cheikh Brahim El Vaigh and Yuta Nakashima and Benjamin Renoust and Hajime Nagahara and Yutaka Fujioka},
title = {Built year prediction from Buddha face with heterogeneous labels},
booktitle = {Proc.~Workshop on Structuring and Understanding of Multimedia Heritage Contents (SUMAC)},
month = 10,
year = {2021},
pages = {5--12},
url = {https://dl.acm.org/doi/abs/10.1145/3475720.3484441},
abstract = {Buddha statues are a part of human culture, especially of the Asia area, and they have been alongside human civilisation for more than 2,000 years. As history goes by, due to wars, natural disasters, and other reasons, the records that show the built years of Buddha statues went missing, which makes it an immense work for historians to estimate the built years. In this paper, we pursue the idea of building a neural network model that automatically estimates the built years of Buddha statues based only on their face images. Our model uses a loss function that consists of three terms: an MSE loss that provides the basis for built year estimation; a KL divergence-based loss that handles the samples with both an exact built year and a possible range of built years (e.g., dynasty or centuries) estimated by historians; finally a regularisation that utilises both labelled and unlabelled samples based on manifold assumption. By combining those three terms in the training process, we show that our method is able to estimate built years for given images with 37.5 years of a mean absolute error on the test set.},
}


@inproceedings{bai2021explain,
author = {Zechen Bai and Yuta Nakashima and Noa Garcia},
title = {Explain me the painting: Multi-topic knowledgeable art description generation},
booktitle = {Proc.~IEEE/CVF International Conference on Computer Vision (ICCV)},
month = 10,
year = {2021},
pages = {5422--5432},
url = {https://openaccess.thecvf.com/content/ICCV2021/papers/Bai_Explain_Me_the_Painting_Multi-Topic_Knowledgeable_Art_Description_Generation_ICCV_2021_paper.pdf},
abstract = {Have you ever looked at a painting and wondered what is the story behind it? This work presents a framework to bring art closer to people by generating comprehensive descriptions of fine-art paintings. Generating informative descriptions for artworks, however, is extremely challenging, as it requires to 1) describe multiple aspects of the image such as its style, content, or composition, and 2) provide background and contextual knowledge about the artist, their influences, or the historical period. To address these challenges, we introduce a multi-topic and knowledgeable art description framework, which modules the generated sentences according to three artistic topics and, additionally, enhances each description with external knowledge. The framework is validated through an exhaustive analysis, both quantitative and qualitative, as well as a comparative human evaluation, demonstrating outstanding results in terms of both topic diversity and information veracity.},
}

@inproceedings{li2021scouter,
author = {Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara},
title = {{SCOUTER}: Slot attention-based classifier for explainable image recognition},
booktitle = {Proc.~IEEE/CVF International Conference on Computer Vision (ICCV)},
note = {pp.~1046--1055},
month = 11,
year = {2021},
pages = {5422--5432},
url = {https://openaccess.thecvf.com/content/ICCV2021/papers/Li_SCOUTER_Slot_Attention-Based_Classifier_for_Explainable_Image_Recognition_ICCV_2021_paper.pdf},
abstract = {Explainable artificial intelligence has been gaining attention in the past few years. However, most existing methods are based on gradients or intermediate features, which are not directly involved in the decision-making process of the classifier. In this paper, we propose a slot attention-based classifier called SCOUTER for transparent yet accurate classification. Two major differences from other attention-based methods include: (a) SCOUTER's explanation is involved in the final confidence for each category, offering more intuitive interpretation, and (b) all the categories have their corresponding positive or negative explanation, which tells "why the image is of a certain category" or "why the image is not of a certain category." We design a new loss tailored for SCOUTER that controls the model's behavior to switch between positive and negative explanations, as well as the size of explanatory regions. Experimental results show that SCOUTER can give better visual explanations in terms of various metrics while keeping good accuracy on small and medium-sized datasets.},
}

@inproceedings{hirota2021visual,
author = {Yusuke Hirota and Noa Garcia and Mayu Otani and Chenhui Chu and Yuta Nakashima and Ittetsu Taniguchi and Takao Onoye},
title = {Visual question answering with textual representations for images},
booktitle = {Proc.~IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
month = 10,
year = {2021},
pages = {3154--3157},
url = {https://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Hirota_Visual_Question_Answering_With_Textual_Representations_for_Images_ICCVW_2021_paper.pdf}
abstract = {How far can we go with textual representations for understanding pictures? Deep visual features extracted by object recognition models are prevailing used in multiple tasks, and especially in visual question answering (VQA). However, conventional deep visual features may struggle to convey all the details in an image as we humans do. Meanwhile, with recent language models' progress, descriptive text may be an alternative to this problem. This paper delves into the effectiveness of textual representations for image understanding in the specific context of VQA.},
}

@inproceedings{sayo2021posern,
author = {Akihiko Sayo and Diego Thomas and Hiroshi Kawasaki and Yuta Nakashima and Katsushi Ikeuchi},
title = {{PoseRN}: A {2D} pose refinement network for bias-free multi-view {3D} human pose estimation},
booktitle = {Proc.~International Conference on Image Processing (ICIP)},
month = 9,
year = {2021},
pages = {3233--3237},
url = {https://arxiv.org/pdf/2107.03000.pdf},
abstract = {We propose a new 2D pose refinement network that learns to predict the human bias in the estimated 2D pose. There are biases in 2D pose estimations that are due to differences between annotations of 2D joint locations based on annotators' perception and those defined by motion capture (MoCap) systems. These biases are crafted into publicly available 2D pose datasets and cannot be removed with existing error reduction approaches. Our proposed pose refinement network allows us to efficiently remove the human bias in the estimated 2D poses and achieve highly accurate multi-view 3D human pose estimation.},
}

@inproceedings{verma2021learners,
author = {Manisha Verma and Yuta Nakashima and Hirokazu Kobori and Ryota Takaoka and Noriko Takemura and Tsukasa Kimura and Hajime Nagahara and Masayuki Numao and Kazumitsu Shinohara},
title = {Learners' efficiency prediction using facial behavior analysis},
booktitle = {Proc.~International Conference on Image Processing (ICIP)},
month = 9,
year = {2021},
pages = {1084--1088},
url = {https://ieeexplore.ieee.org/document/9506203},
abstract = {In the e-learning context, how much the learner is concentrated and engaged, or the learners' efficiency, is essential for providing adaptive and flexible materials, timely suggestions, etc., which can lead to efficient learning. In this work, we explore to predict learners' efficiency with a realistic configuration, in which we use a webcam or a laptop PC's built-in camera. Specifically, we first provide a feasible definition of the learners' efficiency, and based on this definition, we predict one's efficiency from facial behavior. We predict the learners' efficiency using various convolutional neural networks. Results are discussed using different evaluation metrics.}
} 

@inproceedings{samaran2021attending,
author = {Jules Samaran and Noa Garcia and Mayu Otani and Chenhui Chu and Yuta Nakashima},
title = {Attending self-attention: A case study of visually grounded supervision in vision-and-language transformers},
booktitle = {Proc.~Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop},
month = 8,
year = {2021},
pages = {81--86},
url = {https://aclanthology.org/2021.acl-srw.8/},
abstract = {The impressive performances of pre-trained visually grounded language models have motivated a growing body of research investigating what has been learned during the pre-training. As a lot of these models are based on Transformers, several studies on the attention mechanisms used by the models to learn to associate phrases with their visual grounding in the image have been conducted. In this work, we investigate how supervising attention directly to learn visual grounding can affect the behavior of such models. We compare three different methods on attention supervision and their impact on the performances of a state-of-the-art visually grounded language model on two popular vision-and-language tasks.},
}

@inproceedings{kajiwara2021wrime,
author = {Tomoyuki Kajiwara and Chenhui Chu and Noriko Takemura and Yuta Nakashima and Hajime Nagahara},
title = {{WRIME}: A new dataset for emotional intensity estimation with subjective and objective annotations},
booktitle = {Proc.~Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
month = 6,
year = {2021},
pages = {2095--2104},
url = {https://aclanthology.org/2021.naacl-main.169/},
abstract = {We annotate 17,000 SNS posts with both the writer’s subjective emotional intensity and the reader’s objective one to construct a Japanese emotion analysis dataset. In this study, we explore the difference between the emotional intensity of the writer and that of the readers with this dataset. We found that the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer’s subjective labels than the readers’. The large gap between the subjective and objective emotions imply the complexity of the mapping from a post to the subjective emotion intensities, which also leads to a lower performance with machine learning models.},
}

@inproceedings{wang2021mtunet,
author = {Bowen Wang and Liangzhi Li and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara},
title = {{MTUNet}: Few-shot image classification with visual explanations},
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
month = 6,
year = {2021},
pages = {2294--2298},
url = {https://openaccess.thecvf.com/content/CVPR2021W/RCV/papers/Wang_MTUNet_Few-Shot_Image_Classification_With_Visual_Explanations_CVPRW_2021_paper.pdf},
abstract = {Few-shot learning (FSL) approaches, mostly neural network-based, are assuming that the pre-trained knowledge can be obtained from base (seen) categories and transferred to novel (unseen) categories. However, the black-box nature of neural networks makes it difficult to understand what is actually transferred, which may hamper its application in some risk-sensitive areas. In this paper, we reveal a new way to perform explainable FSL for image classification, using discriminative patterns and pairwise matching. Experimental results prove that the proposed method can achieve satisfactory explainability on two mainstream datasets. Code is available*.},
}



@inproceedings{bae2020,
  author={Kyuho Bae and Andre Ivan and Hajime Nagahara and In Kyu Park},
  booktitle={International Conference on Pattern Recognition}, 
  title={5D Light Field Synthesis from a Monocular Video}, 
abstract = {Commercially available light field cameras have difficulty in capturing 5D (4D + time) light field videos. They can only capture still light field images or are excessively expensive for normal users to capture the light field video. To tackle this problem, we propose a deep learning-based method for synthesizing a light field video from a monocular video. We propose a new synthetic light field video dataset that renders photorealistic scenes using Unreal Engine because no light field video dataset is available. The proposed deep learning framework synthesizes the light field video with a full set (9x9) of sub-aperture images from a normal monocular video. The proposed network consists of three sub-networks, namely, feature extraction, 5D light field video synthesis, and temporal
consistency refinement. Experimental results show that our model can successfully synthesize the light field video for synthetic and
real scenes and outperforms the previous frame-by-frame method quantitatively and qualitatively.},
  year={2020},
  volume={},
  number={},
  pages={},
url={http://image.inha.ac.kr/wp-content/uploads/2020/10/ICPR1980.L1u63.pdf},
}

@article{Sakai2021,
author = {Kohei Sakai and Yasutaka Inagaki and Keita Takahashi and Toshiaki Fujii and Hajime Nagahara},
title = {CFA Handling and Quality Analysis for Compressive Light Field Camera},
abstract = {A light field can carry rich visual information of a real 3-D scene, leading to many attractive applications.
However, the acquisition of a light field is challenging due to the large amount of data. In our previous work, we proposed an
efficient method for this task using a coded-aperture camera with a convolutional neural network (CNN) which can computationally
reconstruct a light field from several images acquired with different aperture patterns. In this work, we report two
follow-up contributions to the previous work. First, we integrated a color filter array, which is common in RGB cameras, and
the related color processing into the algorithm pipeline. This integration led to better reconstruction quality for color light
fields. We then analyzed how the reconstruction quality obtained with our method was affected by the complexity of light
fields. We also showed the possibility of using this analysis to predict the reconstruction quality from the acquired images.},
doi = {https://doi.org/10.3169/mta.9.25},
booktitle = {ITE Transactions on Media Technology and Applications},
volume={9},
number={1},
pages={25--32},
month = {January},
year = {2021},
}

@article{Arno2020,
author = {Arno Germond and Yulia Panina and Mikio Shiga and Hirohiko Niioka and Tomonobu M. Watanabe},
title = {Following Embryonic Stem Cells, Their Differentiated Progeny, and Cell-State Changes During iPS Reprogramming by Raman Spectroscopy},
abstract = {Monitoring cell-state transition in pluripotent cells is invaluable for application and basic research. In this study, we demonstrate the pertinence of noninvasive, label-free Raman spectroscopy to monitor and characterize the cell-state transition of mouse stem cells undergoing reprogramming. Using an isogenic cell line of mouse stem cells, reprogramming from neuronal cells was performed, and we showcase a comparative analysis of living single-cell spectral data of the original stem cells, their neuronal progenitors, and reprogrammed cells. Neural network, regression models, and ratiometric analyses were used to discriminate the cell states and extract several important biomarkers specific to differentiation or reprogramming. Our results indicated that the Raman spectrum allowed us to build a low-dimensional space allowing us to monitor and characterize the dynamics of cell-state transition at a single-cell level, scattered in heterogeneous populations. The ability of monitoring pluripotency by Raman spectroscopy and distinguishing differences between ES and reprogrammed cells is also discussed.},
doi = {https://doi.org/10.1021/acs.analchem.0c01800},
booktitle = {Analytical Chemistry},
month = {October},
year = {2020},
url = {https://pubs.acs.org/doi/pdf/10.1021/acs.analchem.0c01800},
}


@article{Dong2021,
author = {Wenjian Dong and Mayu Otani and Noa Garcia and Yuta Nakashima and Chenhui Chu},
title = {Cross-lingual visual grounding},
abstract = {Visual grounding is a vision and language understanding task aiming at locating a region in an image according to a specific query phrase. However, most previous studies only address this task for the English language. Although there are previous cross-lingual vision and language studies, they work on image and video captioning, and visual question answering. In this paper, we present the first work on cross-lingual visual grounding to expand the task to different languages to study an effective yet efficient way for visual grounding on other languages. We construct a visual grounding dataset for French via crowdsourcing. Our dataset consists of 14k, 3k, and 3k query phrases with their corresponding image regions for 5k, 1k, and 1k training, validation and test images, respectively. In addition, we propose a cross-lingual visual grounding approach that transfers the knowledge from a learnt English model to a French model. Despite that the size of our French dataset is 1/6 of the English dataset, experiments indicate that our model achieves an accuracy of 65.17%, which is comparable to the accuracy 69.04% of the English model. Our dataset and codes are available at https://github.com/ids-cv/Multi-Lingual-Visual-Grounding.},
year = {2020},
month = {December},
journal = {IEEE Access},
volume = {9},
url = {https://ieeexplore.ieee.org/document/9305199},
doi = {https://doi.org/10.1109/ACCESS.2020.3046719},
}

@article{Babaguchi2021,
author = {Noboru Babaguchi and Isao Echizen and Junichi Yamagishi and Naoko Nitta and Yuta Nakashima and Kazuaki Nakamura and Kazuhiro Kono and Fuming Fand, Seiko Myojin and Zhenzhong Kuang and Huy H Nguyen and Ngoc-Dung T Tieu},
title = {Preventing fake information generation against media clone attacks},
abstract = {Fake media has been spreading due to remarkable advances in media processing and machine leaning technologies, causing serious problems in society. We are conducting a research project called Media Clone aimed at developing methods for protecting people from fake but skillfully fabricated replicas of real media called media clones. Such media can be created from fake information about a specific person. Our goal is to develop a trusted communication system that can defend against attacks of media clones. This paper describes some research results of the Media Clone project, in particular, various methods for protecting personal information against generating fake information. We focus on 1) fake information generation in the physical world, 2) anonymization and abstraction in the cyber world, and 3) modeling of media clone attacks.},
year = {2021},
month = {January},
journal = {IEICE Transactions on Information and Systems},
volume = {E104-D},
number = {1},
pages = {2-11},
url = {https://search.ieice.org/bin/summary.php?id=e104-d_1_2},
doi = {https://doi.org/10.1587/transinf.2020MUI0001},
}

@article{Echizen2021,
author = {Isao Echizen and Noboru Babaguchi and Junichi Yamagishi and Naoko Nitta and Yuta Nakashima and Kazuaki Nakamura and Kazuhiro Kono and Fuming Fand and Seiko Myojin and Zhenzhong Kuang and Huy H Nguyen and Ngoc-Dung T Tieu},
title = {Generation and detection of media clones},
abstract = {With the spread of high-performance sensors and social network services (SNS) and the remarkable advances in machine learning technologies, fake media such as fake videos, spoofed voices, and fake reviews that are generated using high-quality learning data and are very close to the real thing are causing serious social problems. We launched a research project, the Media Clone (MC) project, to protect receivers of replicas of real media called media clones (MCs) skillfully fabricated by means of media processing technologies. Our aim is to achieve a communication system that can defend against MC attacks and help ensure safe and reliable communication. This paper describes the results of research in two of the five themes in the MC project: 1) verification of the capability of generating various types of media clones such as audio, visual, and text derived from fake information and 2) realization of a protection shield for media clones' attacks by recognizing them.},
year = {2021},
month = {January},
journal = {IEICE Transactions on Information and Systems },
volume = {E104-D},
number = {1},
pages = {12-23},
url = {https://search.ieice.org/bin/summary.php?id=e104-d_1_12},
doi = {https://doi.org/10.1587/transinf.2020MUI0002},
}

@inproceedings{Kayatani2021,
author = {Yuta Kayatani and Zekun Yang and Mayu Otani and Noa Garcia and Chenhui Chu and Yuta Nakashima and Haruo Takemura},
title = {The laughing machine: Predicting humor in video},
abstract  = {Humor is a very important communication tool; yet, it is an open problem for machines to understand humor. In this paper, we build a new multimodal dataset for humor prediction that includes subtitles and video frames, as well as humor labels associated with video's timestamps. On top of it, we present a model to predict whether a subtitle causes laughter. Our model uses the visual modality through facial expression and character name recognition, together with the verbal modality, to explore how the visual modality helps. In addition, we use an attention mechanism to adjust the weight for each modality to facilitate humor prediction. Interestingly, our experimental results show that the performance boost by combinations of different modalities, and the attention mechanism and the model mostly relies on the verbal modality.},
year = {2021},
month = {January},
booktitle = {Proceedings - IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
pages = {2073-2083},
url = {https://openaccess.thecvf.com/content/WACV2021/html/Kayatani_The_Laughing_Machine_Predicting_Humor_in_Video_WACV_2021_paper.html}
}

@article{Ashihara2020,
author = {Kazuki Ashihara and Cheikh Brahim El Vaigh and Chenhui Chu and Benjamin Renoust and Noriko Okubo and Noriko Takemura and Yuta Nakashima and Hajime Nagahara},
title = {Improving topic modeling through homophily for legal documents},
abstract = {Topic modeling that can automatically assign topics to legal documents is very important in the domain of computational law. The relevance of the modeled topics strongly depends on the legal context they are used in. On the other hand, references to laws and prior cases are key elements for judges to rule on a case. Taken together, these references form a network, whose structure can be analysed with network analysis. However, the content of the referenced documents may not be always accessed. Even in that case, the reference structure itself shows that documents share latent similar characteristics. We propose to use this latent structure to improve topic modeling of law cases using document homophily. In this paper, we explore the use of homophily networks extracted from two types of references: prior cases and statute laws, to enhance topic modeling on legal case documents. We conduct in detail, an analysis on a dataset consisting of rich legal cases, i.e., the COLIEE dataset, to create these networks. The homophily networks consist of nodes for legal cases, and edges with weights for the two families of references between the case nodes. We further propose models to use the edge weights for topic modeling. In particular, we propose a cutting model and a weighting model to improve the relational topic model (RTM). The cutting model uses edges with weights higher than a threshold as document links in RTM; the weighting model uses the edge weights to weight the link probability function in RTM. The weights can be obtained either from the co-citations or from the cosine similarity based on an embedding of the homophily networks. Experiments show that the use of the homophily networks for topic modeling significantly outperforms previous studies, and the weighting model is more effective than the cutting model.},
year = {2020},
month = {October},
journal = {Applied Network Science},
volume = {5},
url = {https://appliednetsci.springeropen.com/articles/10.1007/s41109-020-00321-y},
doi = {https://doi.org/10.1007/s41109-020-00321-y}
}

@inproceedings{Ohashi2020,
author = {Sora Ohashi and Tomoyuki Kajiwara and Chenhui Chu and Noriko Takemura and Yuta Nakashima and Hajime Nagahara},
title = {IDSOU at WNUT-2020 Task 2: Identification of informative COVID-19 English tweets},
abstract = {We introduce the IDSOU submission for the WNUT-2020 task 2: identification of informative COVID-19 English Tweets. Our system is an ensemble of pre-trained language models such as BERT. We ranked 16th in the F1 score.},
year = {2020},
month = {November},
pages = {428--433},
booktitle = {Proceedings - Workshop on Noisy User-generated Text (W-NUT 2020)},
url = {https://www.aclweb.org/anthology/2020.wnut-1.62.pdf}
}

@inproceedings{Otani2020b,
author = {Mayu Otani and Yuta Nakashima and Esa Rahtu and Janne Heikkilä},
title = {Uncovering hidden challenges in query-based video moment retrieval},
abstract = { The query-based moment retrieval is a problem of localising a specific clip from an untrimmed video according a query sentence. This is a challenging task that requires interpretation of both the natural language query and the video content. Like in many other areas in computer vision and machine learning, the progress in query-based mo- ment retrieval is heavily driven by the benchmark datasets and, therefore, their quality has significant impact on the field. In this paper, we present a series of experiments as- sessing how well the benchmark results reflect the true progress in solving the moment retrieval task. Our results indicate substantial biases in the popular datasets and unex- pected behaviour of the state-of-the-art models. Moreover, we present new sanity check experiments and approaches for visualising the results. Finally, we suggest possible di- rections to improve the temporal sentence grounding in the future.},
booktitle = {Proceedings - British Machine Vision Conference},
year = {2020},
month = {September}
url = {https://www.bmvc2020-conference.com/assets/papers/0306.pdf}
}

@article{Otani2020a,
author = {Mayu Otani and Chenhui Chu and Yuta Nakashima},
title = {Visually grounded paraphrase identification via gating and phrase localization},
abstract = {Visually grounded paraphrases (VGPs) describe the same visual concept but in different wording. Previous studies have developed models to identify VGPs from language and visual features. In these existing methods, language and visual features are simply fused. However, our detailed analysis indicates that VGPs with different lexical similarities require different weights on language and visual features to maximize identification performance. This motivates us to propose a gated neural network model to adaptively control the weights. In addition, because VGP identification is closely related to phrase localization, we also propose a way to explicitly incorporate phrase-object correspondences. From our evaluation in detail, we confirmed our model outperforms the state-of-the-art model.},
doi = {https://doi.org/10.1016/j.neucom.2020.04.066},
journal = {Neurocomputing},
volume = {404},
month = {September},
year = {2020},
pages = {165--172},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220306512}
}


@article{Yanagawa2020,
author = {Masahiro Yanagawa and Hirohiko Niioka and Masahiko Kusumoto and Kazuo Awai and Mitsuko Tsubamoto and Yukihisa Satoh and Tomo Miyata and Yuriko Yoshida and Noriko Kikuchi and Akinori Hata and Shohei Yamasaki and Shoji Kido and Hajime Nagahara and Jun Miyake and Noriyuki Tomiyama},
title = {Diagnostic performance for pulmonary adenocarcinoma on CT: comparison of radiologists with and without three-dimensional convolutional neural network},
abstract = {Objectives
To compare diagnostic performance for pulmonary invasive adenocarcinoma among radiologists with and without three-dimensional convolutional neural network (3D-CNN).
Methods
Enrolled were 285 patients with adenocarcinoma in situ (AIS, n = 75), minimally invasive adenocarcinoma (MIA, n = 58), and invasive adenocarcinoma (IVA, n = 152). A 3D-CNN model was constructed with seven convolution-pooling and two max-pooling layers and fully connected layers, in which batch normalization, residual connection, and global average pooling were used. Only the flipping process was performed for augmentation. The output layer comprised two nodes for two conditions (AIS/MIA and IVA) according to prognosis. Diagnostic performance of the 3D-CNN model in 285 patients was calculated using nested 10-fold cross-validation. In 90 of 285 patients, results from each radiologist (R1, R2, and R3; with 9, 14, and 26 years of experience, respectively) with and without the 3D-CNN model were statistically compared.
Results
Without the 3D-CNN model, accuracy, sensitivity, and specificity of the radiologists were as follows: R1, 70.0%, 52.1%, and 90.5%; R2, 72.2%, 75%, and 69%; and R3, 74.4%, 89.6%, and 57.1%, respectively. With the 3D-CNN model, accuracy, sensitivity, and specificity of the radiologists were as follows: R1, 72.2%, 77.1%, and 66.7%; R2, 74.4%, 85.4%, and 61.9%; and R3, 74.4%, 93.8%, and 52.4%, respectively. Diagnostic performance of each radiologist with and without the 3D-CNN model had no significant difference (p > 0.88), but the accuracy of R1 and R2 was significantly higher with than without the 3D-CNN model (p < 0.01).
Conclusions
The 3D-CNN model can support a less-experienced radiologist to improve diagnostic accuracy for pulmonary invasive adenocarcinoma without deteriorating any diagnostic performances.
Key Points
• The 3D-CNN model is a non-invasive method for predicting pulmonary invasive adenocarcinoma in CT images with high sensitivity.
• Diagnostic accuracy by a less-experienced radiologist was better with the 3D-CNN model than without the model.},
doi = {https://doi.org/10.1007/s00330-020-07339-x},
booktitle = {European Radiology},
month = {October},
year = {2020},
}

@article{Tanaka2020,
author = {Shin-ichi Tanaka and Hiroki Wadati and Kazuhisa Sato and Hidehiro Yasuda and Hirohiko Niioka},
title = {Red-Fluorescent Pt Nanoclusters for Detecting and Imaging HER2 in Breast Cancer Cells},
abstract = {Overexpression of human epidermal growth factor receptor 2 (HER2) is associated with more frequent cancer recurrence and metastasis. Sensitive sensing of HER2 in living breast cancer cells is crucial in the early stages of cancer and to further understand its role in cells. Biomedical imaging has become an indispensable tool in the fields of early cancer diagnosis and therapy. In this study, we designed and synthesized platinum (Pt) nanocluster bionanoprobes with red emission (Ex/Em = 535/630 nm) for fluorescence imaging of HER2. Our Pt nanoclusters, which were synthesized using polyamidoamine (PAMAM) dendrimer and preequilibration, exhibited approximately 1% quantum yield and possessed low cytotoxicity, ultrasmall size, and excellent photostability. Furthermore, combined with ProteinA as an adapter protein, we developed Pt bionanoprobes with minimal nonspecific binding and utilized them as fluorescent probes for highly sensitive optical imaging of HER2 at the cellular level. More importantly, molecular probes with long-wavelength emission have allowed visualization of deep anatomical features because of enhanced tissue penetration and a decrease in background noise from tissue scattering. Our Pt nanoclusters are promising fluorescent probes for biomedical applications.},
doi = {https://doi.org/10.1021/acsomega.0c02578},
journal = {ACS Omega},
month = {September},
year = {2020},
url = {https://pubs.acs.org/doi/pdf/10.1021/acsomega.0c02578},
}

@article{Yamato2020a,
author = {Naoki Yamato and Hirohiko Niioka and Jun Miyake and Mamoru Hashimoto},
title = {Improvement of nerve imaging speed with coherent anti-Stokes Raman scattering rigid endoscope using deep-learning noise reduction},
abstract = {A coherent anti-Stokes Raman scattering (CARS) rigid endoscope was developed to visualize peripheral nerves without labeling for nerve-sparing endoscopic surgery. The developed CARS endoscope had a problem with low imaging speed, i.e. low imaging rate. In this study, we demonstrate that noise reduction with deep learning boosts the nerve imaging speed with CARS endoscopy. We employ fine-tuning and ensemble learning and compare deep learning models with three different architectures. In the fine-tuning strategy, deep learning models are pre-trained with CARS microscopy nerve images and retrained with CARS endoscopy nerve images to compensate for the small dataset of CARS endoscopy images. We propose using the equivalent imaging rate (EIR) as a new evaluation metric for quantitatively and directly assessing the imaging rate improvement by deep learning models. The highest EIR of the deep learning model was 7.0 images/min, which was 5 times higher than that of the raw endoscopic image of 1.4 images/min. We believe that the improvement of the nerve imaging speed will open up the possibility of reducing postoperative dysfunction by intraoperative nerve identification.},
doi = {https://doi.org/10.1038/s41598-020-72241-x},
journal = {Scientific Reports},
month = {September},
year = {2020},
url = {https://www.nature.com/articles/s41598-020-72241-x.pdf},
}

@inproceedings{Huckle2020,
author = {Nikolai Huckle and Noa Garcia and Yuta Nakashima},
title = {Demographic influences on contemporary art with unsupervised style embeddings},
abstract = {Computational art analysis has, through its reliance on classification tasks, prioritised historical datasets in which the artworks are already well sorted with the necessary annotations. Art produced today, on the other hand, is numerous and easily accessible, through the internet and social networks that are used by professional and amateur artists alike to display their work. Although this art---yet unsorted in terms of style and genre---is less suited for supervised analysis, the data sources come with novel information that may help frame the visual content in equally novel ways. As a first step in this direction, we present contempArt, a multi-modal dataset of exclusively contemporary artworks. contempArt is a collection of paintings and drawings, a detailed graph network based on social connections on Instagram and additional socio-demographic information; all attached to 442 artists at the beginning of their career. We evaluate three methods suited for generating unsupervised style embeddings of images and correlate them with the remaining data. We find no connections between visual style on the one hand and social proximity, gender, and nationality on the other.},
doi = {},
booktitle = {Proceedings - European Conference on Computer Vision Workshops},
month = {August},
year = {2020},
}

@inproceedings{Yamato2020,
author = {Naoki Yamato and Mana Matsuya and Hirohiko Niioka and Jun Miyake and Mamoru Hashimoto},
title = {Nerve segmentation with deep learning from label-free endoscopic images obtained using coherent anti-stokes {Raman} scattering},
abstract = {Semantic segmentation with deep learning to extract nerves from label-free endoscopic images obtained using coherent anti-Stokes Raman scattering (CARS) for nerve-sparing surgery is described. We developed a CARS rigid endoscope in order to identify the exact location of peripheral nerves in surgery. Myelinated nerves are visualized with a CARS lipid signal in a label-free manner. Because the lipid distribution includes other tissues as well as nerves, nerve segmentation is required to achieve nerve-sparing surgery. We propose using U-Net with a VGG16 encoder as a deep learning model and pre-training with fluorescence images, which visualize the lipid distribution similar to CARS images, before fine-tuning with a small dataset of CARS endoscopy images. For nerve segmentation, we used 24 CARS and 1,818 fluorescence nerve images of three rabbit prostates. We achieved label-free nerve segmentation with a mean accuracy of 0.962 and an F1 value of 0.860. Pre-training on fluorescence images significantly improved the performance of nerve segmentation in terms of the mean accuracy and F1 value (p<0.05). Nerve segmentation of label-free endoscopic images will allow for safer endoscopic surgery, while reducing dysfunction and improving prognosis after surgery.},
doi = {https://doi.org/10.3390/biom10071012},
booktitle = {Biomolecules},
month = {July},
year = {2020},
url = {https://www.mdpi.com/2218-273X/10/7/1012},
}

@inproceedings{Sakai2020a,
author = {Kohei Sakai and Keita Takahashi and Toshiaki Fujii and Hajime Nagahara},
title = {Acquiring dynamic light fields through coded aperture camera},
abstract = {We investigate the problem of compressive acquisition of a dynamic light field. A promising solution for compressive light field acquisition is to use a coded aperture camera, with which an entire light field can be computationally reconstructed from several images captured through differently-coded aperture patterns. With this method, it was assumed that the scene should not move throughout the complete acquisition process, which restricted real applications. In this study, however, we assume that the target scene may change over time, and propose a method for acquiring a dynamic light field (a moving scene) using a coded aperture camera and a convolutional neural network (CNN). To successfully handle scene motions, we develop a new configuration of image observation, called V-shape observation, and train the CNN using a dynamic-light-field dataset with pseudo motions. Our method is validated through experiments using both a computer-generated scene and a real camera.},
doi = {},
booktitle = {Proceedings - European Conference on Computer Vision},
month = {August},
year = {2020},
url = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640358.pdf},
}

@inproceedings{Sasagawa2020a,
author = {Yukihiro Sasagawa and Hajime Nagahara},
title = {{YOLO} in the Dark - {D}omain adaptation method for merging multiple models -},
abstract = {Generating models to handle new visual tasks requires additional datasets, which take considerable effort to create. We propose a method of domain adaptation for merging multiple models with less effort than creating an additional dataset. This method merges pre-trained models in different domains using glue layers and a generative model, which feeds latent features to the glue layers to train them without an additional dataset. We also propose a generative model that is created by distilling knowledge from pre-trained models. This enables the dataset to be reused to create latent features for training the glue layers. We apply this method to object detection in a low-light situation. The YOLO- in-the-Dark model comprises two models, Learning-to-See-in-the-Dark model and YOLO. We present the proposed method and report the result of domain adaptation to detect objects from RAW short-exposure low-light images. The YOLO-in-the-Dark model uses fewer computing resources than the naive approach.},
doi = {},
booktitle = {Proceedings - European Conference on Computer Vision},
month = {August},
year = {2020},
url = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660341.pdf},
}

@inproceedings{Okawara2020a,
  author={Tadashi Okawara and Michitaka Yoshida and Hajime Nagahara and Yasushi Yagi},
  booktitle={Proceedings - 2020 IEEE International Conference on Computational Photography (ICCP)}, 
  title={Action recognition from a single coded image}, 
abstract = {Cameras are prevalent in society at the present time, for example, surveillance cameras, and smartphones equipped with cameras and smart speakers. There is an increasing demand to analyze human actions from these cameras to detect unusual behavior or within a man-machine interface for Internet of Things (IoT) devices. For a camera, there is a trade-off between spatial resolution and frame rate. A feasible approach to overcome this trade-off is compressive video sensing. Compressive video sensing uses random coded exposure and reconstructs higher than read out of sensor frame rate video from a single coded image. It is possible to recognize an action in a scene from a single coded image because the image contains multiple temporal information for reconstructing a video. In this paper, we propose reconstruction-free action recognition from a single coded exposure image. We also proposed deep sensing framework which models camera sensing and classification models into convolutional neural network (CNN) and jointly optimize the coded exposure and classification model simultaneously. We demonstrated that the proposed method can recognize human actions from only a single coded image. We also compared it with competitive inputs, such as low-resolution video with a high frame rate and high-resolution video with a single frame in simulation and real experiments.},
  year={2020},
  pages={1-11},
url={https://ieeexplore.ieee.org/document/9105176},
}

@inproceedings{Tanaka2020a,
title = {Constructing a public meeting corpus},
authors = {Koji Tanaka and Chenhui Chu and Haolin Ren and Benjamin Renoust and Yuta Nakashima and Noriko Takemura and Hajime Nagahara and Takao Fujikawa},
booktitle = {Proceedings - the 12th International Conference on Language Resources and Evaluation (LREC 2020)},
year = {2020},
month = {May},
abstract = {In this paper, we propose a method for constructing a large corpus about a century of public meetings in historical Australian newspapers, and analyze the constructed corpus. The corpus construction method is based on image processing and Optical Character Recognition (OCR). We digitize and transcribe texts of the specific topic of public meeting. Experiments show that our proposed method achieves a F-score of 71.5% with a high recall of 97.5% for corpus construction. This allows us to feed a content search tool for temporal and semantic content analysis.},
url = {https://www.aclweb.org/anthology/2020.lrec-1.238.pdf},
}

@article{Fujikawa2020a,
title = {歴史研究におけるビッグデータの活用－オーストラリアを中心に},
author = {藤川 隆男 and Chenhui Chu and 梶原 智之 and 長原 一},
journal = {西洋史学},
volume = {268},
pages =  {50--61}, 
year = {2019},
month = {December},
}

@inproceedings{Chu2020a,
title = {Public meeting corpus construction and content delivery}, 
author = {Chenhui Chu and Koji Tanaka and Haolin Ren and Benjamin Renoust and Yuta Nakashima and Noriko Takemura and Hajime Nagahara and Takao Fujikawa},
booktitle = {人文科学とコンピュータシンポジウム2019},
year = {2019},
month = {December},
}

@techreport{Tanaka2020b,
title = {公開集会記事からの情報抽出},
author = {田中 昂志 and 芦原 和樹 and Chenhui Chu and 中島 悠太 and 武村 紀子 and 長原 一 and 藤川 隆男},
institution = {2019年度人工知能学会全国大会},
year = {2020},
month = {June},
}

@techreport{Tanaka2020c,
title = {{OCR}誤り訂正を⽤いた歴史新聞データからのコーパス構築}, 
author = {⽥中 昂志 and Chenhui Chu and 梶原 智之 and 中島 悠太 and 武村 紀⼦ and ⻑原 ⼀ and 藤川 隆男},
institution = {⾔語処理学会第26回年次⼤会},
year = {2020},
month = {May},
}

@techreport{Tanaka2019a,
title = {歴史新聞データからのコーパス構築},
author = {田中 昂志 and Chenhui Chu and 中島 悠太 and 武村 紀子 and 長原 一 and 藤川 隆男},
institution = {言語処理学会第25回年次大会},
pages = {1065--1068},
year = {2019},
month = {March},
}

@inproceedings{Garcia2020c,
author = {Noa Garcia and Yuta Nakashima},
title = {Knowledge-based video question answering with unsupervised scene descriptions},
abstract = {To understand movies, humans constantly reason over the dialogues and actions shown in specific scenes and relate them to the overall storyline already seen. Inspired by this behaviour, we design ROLL, a model for knowledge-based video story question answering that leverages three crucial aspects of movie understanding: dialog comprehension, scene reasoning, and storyline recalling. In ROLL, each of these tasks is in charge of extracting rich and diverse information by 1) processing scene dialogues, 2) generating unsupervised video scene descriptions, and 3) obtaining external knowledge in a weakly supervised fashion. To answer a given question correctly, the information generated by each inspired-cognitive task is encoded via Transformers and fused through a modality weighting mechanism, which balances the information from the different sources. Exhaustive evaluation demonstrates the effectiveness of our approach, which yields a new state-of-the-art on two challenging video question answering datasets: KnowIT VQA and TVQA+.},
doi = {},
booktitle = {Proceedings - European Conference on Computer Vision},
month = {August},
year = {2020},
url = {https://arxiv.org/abs/2007.08751},
}

@inproceedings{Garcia2020b,
author = {Noa Garcia and Chentao Ye and Zihua Liu and Qingtao Hu and Mayu Otani and Chenhui Chu and Yuta Nakashima and Teruko Mitamura},
doi = {},
booktitle = {Proceedings - European Conference on Computer Vision Workshops},
month = {August},
year = {2020},
title = {A dataset and baselines for visual question answering on art},
abstract = {nswering questions related to art pieces (paintings) is a difficult task, as it implies the understanding of not only the visual information that is shown in the picture, but also the contextual knowledge that is acquired through the study of the history of art. In this work, we introduce our first attempt towards building a new dataset, coined AQUA (Art QUestion Answering). The question-answer (QA) pairs are automatically generated using state-of-the-art question generation methods based on paintings and comments provided in an existing art understanding dataset. The QA pairs are cleansed by crowdsourcing workers with respect to their grammatical correctness, answerability, and answers' correctness. Our dataset inherently consists of visual (painting-based) and knowledge (comment-based) questions. We also present a two-branch model as baseline, where the visual and knowledge questions are handled independently. We extensively compare our baseline model against the state-of-the-art models for question answering, and we provide a comprehensive study about the challenges and potential future directions for visual question answering on art.}
url = {https://arxiv.org/abs/2008.12520},
}

@article{Hirose2018,
author = {Hirose, Keigo and Fukushima, Shuichiro and Furukawa, Taichi and Niioka, Hirohiko and Hashimoto, Mamoru},
doi = {10.1063/1.5031817},
issn = {2378-0967},
journal = {APL Photonics},
month = {sep},
number = {9},
pages = {092407},
title = {{Invited Article: Label-free nerve imaging with a coherent anti-Stokes Raman scattering rigid endoscope using two optical fibers for laser delivery}},
url = {http://aip.scitation.org/doi/10.1063/1.5031817},
volume = {3},
year = {2018}
}
@article{Yoda2018,
abstract = {The photometric stereo method enables estimation of surface normals from images that have been captured using different but known lighting directions. The classical photometric stereo method requires at least three images to determine the normals in a given scene. However, this method cannot be applied to dynamic scenes because it is assumed that the scene remains static while the required images are captured. In this work, we present a dynamic photometric stereo method for estimation of the surface normals in a dynamic scene. We use a multi-tap complementary metal-oxide-semiconductor (CMOS) image sensor to capture the input images required for the proposed photometric stereo method. This image sensor can divide the electrons from the photodiode from a single pixel into the different taps of the exposures and can thus capture multiple images under different lighting conditions with almost identical timing. We implemented a camera lighting system and created a software application to enable estimation of the normal map in real time. We also evaluated the accuracy of the estimated surface normals and demonstrated that our proposed method can estimate the surface normals of dynamic scenes.},
author = {Yoda, T. and Nagahara, H. and Taniguchi, R.-I. and Kagawa, K. and Yasutomi, K. and Kawahito, S.},
doi = {10.3390/s18030786},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3D surface recovery,Computational photography,Photometric stereo,Vision sensor},
number = {3},
title = {{The dynamic photometric stereo method using a multi-tap CMOS image sensor}},
volume = {18},
year = {2018}
}
@inproceedings{Yagi2018a,
abstract = {A light field, which is often understood as a set of dense multi-view images, has been utilized in various 2D/3D applications. Efficient light field acquisition using a coded aperture camera is the target problem considered in this paper. Specifically, the entire light field, which consists of many images, should be reconstructed from only a few images that are captured through different aperture patterns. In previous work, this problem has often been discussed from the context of compressed sensing (CS). In contrast, we formulated this problem from the perspective of principal component analysis (PCA) to derive optimal non-negative aperture patterns and a straight-forward reconstruction algorithm. Even though it is based on a conventional technique, our method has proven to be more accurate and much faster than a state-of-the-art CS-based method.},
author = {Yagi, Yusuke and Takahashi, Keita and Fujii, Toshiaki and Sonoda, Toshiki and Nagahara, Hajime},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2017.8296839},
isbn = {9781509021758},
issn = {15224880},
keywords = {Coded aperture,Light field,PCA},
month = {feb},
pages = {3031--3035},
publisher = {IEEE Computer Society},
title = {{PCA-coded aperture for light field photography}},
volume = {2017-Septe},
year = {2018}
}
@inproceedings{Chu2018a,
abstract = {A paraphrase is a restatement of the meaning of a text in other words. Paraphrases have been studied to enhance the performance of many natural language processing tasks. In this paper, we propose a novel task iParaphrasing to extract visually grounded paraphrases (VGPs), which are different phrasal expressions describing the same visual concept in an image. These extracted VGPs have the potential to improve language and image multimodal tasks such as visual question answering and image captioning. How to model the similarity between VGPs is the key of iParaphrasing. We apply various existing methods as well as propose a novel neural network-based method with image attention, and report the results of the first attempt toward iParaphrasing.},
archivePrefix = {arXiv},
arxivId = {1806.04284},
author = {Chu, Chenhui and Otani, Mayu and Nakashima, Yuta},
booktitle = {Proceedings - International Conference on Compuational Linguistics (COLING)},
eprint = {1806.04284},
keywords = {vgp},
mendeley-tags = {vgp},
month = {jun},
pages = {3479--3492},
title = {{iParaphrasing: Extracting visually grounded paraphrases via an image}},
url = {http://arxiv.org/abs/1806.04284},
year = {2018}
}
@inproceedings{Otani2017,
author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkil{\"{a}}, Janne},
booktitle = {Proceeedings - Workshop on Closing the Loop Between Vision and Language at ICCV},
pages = {3 pages},
title = {{Fine-grained video retrieval for multi-clip video}},
year = {2017}
}
@inproceedings{Shimanaka2018,
abstract = {We introduce the RUSE metric for the WMT18 metrics shared task. Sentence embeddings can capture global information that cannot be captured by local features based on character or word N-grams. Although training sentence embeddings using small-scale translation datasets with manual evaluation is difficult, sentence embeddings trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. We use a multi-layer perceptron regressor based on three types of sentence embeddings. The experimental results of the WMT16 and WMT17 datasets show that the RUSE metric achieves a state-of-the-art performance in both segment- and system-level metrics tasks with embedding features only.},
address = {Stroudsburg, PA, USA},
author = {Shimanaka, Hiroki and Kajiwara, Tomoyuki and Komachi, Mamoru},
booktitle = {Proceedings of the Third Conference on Machine Translation: Shared Task Papers (WMT 18)},
doi = {10.18653/v1/W18-6456},
pages = {751--758},
publisher = {Association for Computational Linguistics},
title = {{RUSE: Regressor using sentence embeddings for automatic machine translation evaluation}},
url = {http://aclweb.org/anthology/W18-6456},
year = {2018}
}
@inproceedings{Roberto2017,
abstract = {{\textcopyright} 2017 MVA Organization All Rights Reserved. This paper presents an incremental structural modeling approach that improves the precision and stability of existing batch based methods for sparse and noisy point clouds from visual SLAM. The main idea is to use the generating process of point clouds on SLAM effectively. First, a batch based method is applied to point clouds that are incrementally generated from SLAM. Then, the temporal history of reconstructed geometric primitives is statistically merged to suppress incorrect reconstruction. The evaluation shows that both precision and stability are improved compared to a batch based method and the proposed method is suitable for real-time structural modeling.},
author = {Roberto, R. and Uchiyama, H. and Lima, J.P. and Nagahara, H. and Taniguchi, R.-I. and Teichrieb, V.},
booktitle = {Proceedings of the 15th IAPR International Conference on Machine Vision Applications, MVA 2017},
doi = {10.23919/MVA.2017.7986765},
isbn = {9784901122160},
title = {{Incremental structural modeling on sparse visual SLAM}},
year = {2017}
}
@article{Niioka2017,
abstract = {{\textcopyright} 2017 The Author(s) In the field of regenerative medicine, tremendous numbers of cells are necessary for tissue/organ regeneration. Today automatic cell-culturing system has been developed. The next step is constructing a non-invasive method to monitor the conditions of cells automatically. As an image analysis method, convolutional neural network (CNN), one of the deep learning method, is approaching human recognition level. We constructed and applied the CNN algorithm for automatic cellular differentiation recognition of myogenic C2C12 cell line. Phase-contrast images of cultured C2C12 are prepared as input dataset. In differentiation process from myoblasts to myotubes, cellular morphology changes from round shape to elongated tubular shape due to fusion of the cells. CNN abstract the features of the shape of the cells and classify the cells depending on the culturing days from when differentiation is induced. Changes in cellular shape depending on the number of days of culture (Day 0, Day 3, Day 6) are classified with 91.3{\%} accuracy. Image analysis with CNN has a potential to realize regenerative medicine industry.},
author = {Niioka, H. and Asatani, S. and Yoshimura, A. and Ohigashi, H. and Tagawa, S. and Miyake, J.},
doi = {10.1007/s13577-017-0191-9},
issn = {17490774},
journal = {Human Cell},
keywords = {Automatic target recognition,Cell differentiation,Convolutional neural network,Deep learning,Image analysis,Phase contrast microscopy},
title = {{Classification of C2C12 cells at differentiation by convolutional neural network of deep learning using phase contrast images}},
year = {2017}
}
@inproceedings{Kimura2018,
abstract = {Reconstruction of the shape and motion of humans from RGB-D is a challenging problem, receiving much attention in recent years. Recent approaches for full-body reconstruction use a statistic shape model, which is built upon accurate full-body scans of people in skin-tight clothes, to complete invisible parts due to occlusion. Such a statistic model may still be fit to an RGB-D measurement with loose clothes but cannot describe its deformations, such as clothing wrinkles. Observed surfaces may be reconstructed precisely from actual measurements, while we have no cues for unobserved surfaces. For full-body reconstruction with loose clothes, we propose to use lower dimensional embeddings of texture and deformation referred to as eigen-texturing and eigen-deformation, to reproduce views of even unobserved surfaces. Provided a full-body reconstruction from a sequence of partial measurements as 3D meshes, the texture and deformation of each triangle are then embedded using eigen-decomposition. Combined with neural-network-based coefficient regression, our method synthesizes the texture and deformation from arbitrary viewpoints. We evaluate our method using simulated data and visually demonstrate how our method works on real data.},
archivePrefix = {arXiv},
arxivId = {1807.02632},
author = {Kimura, Ryosuke and Sayo, Akihiko and Dayrit, Fabian Lorenzo and Nakashima, Yuta and Kawasaki, Hiroshi and Blanco, Ambrosio and Ikeuchi, Katsushi},
booktitle = {Proceedings - International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2018.8545658},
eprint = {1807.02632},
isbn = {9781538637883},
issn = {10514651},
keywords = {Non-rigid 3D deformation,eigen-deformation,eigen-texture,human motion capture,sensing},
mendeley-tags = {sensing},
month = {nov},
pages = {1043--1048},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Representing a partially observed non-rigid 3D human using eigen-texture and eigen-deformation}},
year = {2018}
}
@article{Tsutsumi2019,
abstract = {{\textcopyright} 2019 The Authors  Background: Biomedical imaging devices that utilize the optical characteristics of hemoglobin (Hb) have become widespread. In the field of gastroenterology, there is a strong demand for devices that can apply this technique to surgical navigation. We aimed to introduce our novel multispectral device capable of intraoperatively performing quantitative imaging of the oxygen (O 2 ) saturation and Hb amount of tissues noninvasively and in real time, and to examine its application for deciding the appropriate anastomosis point after subtotal or total esophagectomy. Materials and methods: A total of 39 patients with esophageal cancer were studied. Tissue O 2 saturation and Hb amount of the gastric tube just before esophagogastric anastomosis were evaluated using a multispectral tissue quantitative imaging device. The anastomosis point was decided depending on the quantitative values and patterns of both the tissue O 2 saturation and Hb amount. Results: The device can instantaneously and noninvasively quantify and visualize the tissue O 2 saturation and Hb amount using reflected light. The tissue Hb status could be classified into the following four types: good circulation type, congestion type, ischemia type, and mixed type of congestion and ischemia. Postoperative anastomotic failure occurred in 2 cases, and both were mixed cases. Conclusions: The method of quantitatively imaging the tissue O 2 saturation and Hb level in real time and noninvasively using a multispectral device allows instantaneous determination of the anastomosis and related organ conditions, thereby contributing to determining the appropriate treatment direction.},
author = {Tsutsumi, R. and Ikeda, T. and Nagahara, H. and Saeki, H. and Nakashima, Y. and Oki, E. and Maehara, Y. and Hashizume, M.},
doi = {10.1016/j.jss.2019.04.033},
issn = {10958673},
journal = {Journal of Surgical Research},
keywords = {Anastomosis,Esophagogastrostomy,Hemoglobin,Oxygen saturation,Quantitative imaging},
title = {{Efficacy of Novel Multispectral Imaging Device to Determine Anastomosis for Esophagogastrostomy}},
volume = {242},
year = {2019}
}
@article{Ma2019,
abstract = {Fall is one of the leading causes of injury for the elderly individuals. Systems that automatically detect falls can significantly reduce the delay of assistance. Most of commercialized fall detection systems are based on wearable devices, which elderly individuals tend to forget wearing. Using surveillance cameras to detect falls based on computer vision is ideal, because anyone in the monitoring scopes can be under protection. However, the privacy protection issue using surveillance cameras has been bothering people. To effectively protect the privacy, we proposed an optical level anonymous image sensing system, which can protect the privacy by hiding the facial regions optically at the video capturing phase. We apply the system to fall detection. In detecting falls, we propose a neural network by combining a 3D convolutional neural network for feature extraction and an autoencoder for modelling the normal behaviors. The learned autoencoder reconstructs the features extracted from videos with normal behaviors with smaller average errors than those extracted from videos with falls. We evaluated our neural network by a hold-out validation experiment, and showed its effectiveness. In field tests, we showed and discussed the applicability of the optical level anonymous image sensing system for privacy protection and fall detection.},
author = {Ma, Chao and Shimada, Atsushi and Uchiyama, Hideaki and Nagahara, Hajime and ichiro Taniguchi, Rin},
doi = {10.1016/j.optlastec.2018.07.013},
issn = {00303992},
journal = {Optics and Laser Technology},
keywords = {3D convolutional neural network,Computational imaging,Fall detection,Optical level anonymous,Privacy protection},
month = {feb},
pages = {44--61},
publisher = {Elsevier Ltd},
title = {{Fall detection using optical level anonymous image sensing system}},
volume = {110},
year = {2019}
}
@inproceedings{Renoust2019,
abstract = {{\textcopyright} 2019 Copyright held by the owner/author(s). While Buddhism has spread along the Silk Roads, many pieces of art have been displaced. Only a few experts may identify these works, subjectively to their experience. The construction of Buddha statues was taught through the definition of canon rules, but the applications of those rules greatly varies across time and space. Automatic art analysis aims at supporting these challenges. We propose to automatically recover the proportions induced by the construction guidelines, in order to use them and compare between different deep learning features for several classification tasks, in a medium size but rich dataset of Buddha statues, collected with experts of Buddhism art history.},
author = {Renoust, B. and Franca, M.O. and Chan, J. and Garcia, N. and Le, V. and Uesaka, A. and Nakashima, Y. and Nagahara, H. and Wang, J. and Fujioka, Y.},
booktitle = {SUMAC 2019 - Proceedings of the 1st Workshop on Structuring and Understanding of Multimedia heritAge Contents, co-located with MM 2019},
doi = {10.1145/3347317.3357239},
isbn = {9781450369107},
keywords = {Art history,Buddha statues,Classification,Face landmarks,buddha},
mendeley-tags = {buddha},
title = {{Historical and modern features for Buddha statue classification}},
year = {2019}
}
@article{Yamanaka2019,
author = {Yamanaka, Masahito and Niioka, Hirohiko and Furukawa, Taichi and Nishizawa, Norihiko},
doi = {10.1117/1.JBO.24.7.070501},
issn = {1083-3668},
journal = {Journal of Biomedical Optics},
month = {jul},
number = {07},
pages = {1},
title = {{Excitation of erbium-doped nanoparticles in 1550-nm wavelength region for deep tissue imaging with reduced degradation of spatial resolution}},
url = {https://www.spiedigitallibrary.org/journals/journal-of-biomedical-optics/volume-24/issue-07/070501/Excitation-of-erbium-doped-nanoparticles-in-1550-nm-wavelength-region/10.1117/1.JBO.24.7.070501.full},
volume = {24},
year = {2019}
}
@article{Shimanaka2019,
abstract = {This study describes a segment-level metric for automatic machine translation evaluation (MTE). Although various MTE metrics have been proposed, most MTE metrics, including the current de facto standard BLEU, can handle only limited information for segment-level MTE. Therefore, we propose an MTE metric using pre-trained sentence embeddings in order to evaluate MT translation considering global information. In our proposed method, we obtain sentence embeddings of MT translation and reference translation using a sentence encoder pre-trained on a large corpus. Then, we estimate the translation quality by a regression model based on sentence embeddings of MT translation and reference translation as input. Our metric achieved state-of-the-art performance in segment-level metrics tasks for all to-English language pairs on the WMT dataset with human evaluation score.},
author = {Shimanaka, Hiroki and Kajiwara, Tomoyuki and Komachi, Mamoru},
doi = {10.5715/jnlp.26.613},
journal = {Journal of Natural Language Processing},
month = {sep},
number = {3},
pages = {613--634},
title = {{Metric for automatic machine translation evaluation based on pre-trained sentence embeddings}},
url = {https://www.jstage.jst.go.jp/article/jnlp/26/3/26{\_}613/{\_}article/-char/ja/},
volume = {26},
year = {2019}
}
@inproceedings{Li2020,
abstract = {Retinal vessel segmentation is of great interest for diagnosis of retinal vascular diseases. To further improve the performance of vessel segmentation, we propose IterNet, a new model based on UNet, with the ability to find obscured details of the vessel from the segmented vessel image itself, rather than the raw input image. IterNet consists of multiple iterations of a mini-UNet, which can be 4{\$}\backslashtimes{\$} deeper than the common UNet. IterNet also adopts the weight-sharing and skip-connection features to facilitate training; therefore, even with such a large architecture, IterNet can still learn from merely 10{\$}\backslashsim{\$}20 labeled images, without pre-training or any prior knowledge. IterNet achieves AUCs of 0.9816, 0.9851, and 0.9881 on three mainstream datasets, namely DRIVE, CHASE-DB1, and STARE, respectively, which currently are the best scores in the literature. The source code is available.},
archivePrefix = {arXiv},
arxivId = {1912.05763},
author = {Li, Liangzhi and Verma, Manisha and Nakashima, Yuta and Nagahara, Hajime and Kawasaki, Ryo},
booktitle = {Proceedings - The IEEE Winter Conference on Applications of Computer Vision (WACV)},
doi = {10.1109/wacv45572.2020.9093621},
eprint = {1912.05763},
isbn = {9781728165530},
pages = {3645--3654},
title = {{IterNet: retinal image segmentation utilizing structural redundancy in vessel networks}},
url = {https://www.liangzhili.com/publication/li-2020-iternet/},
year = {2020}
}
@inproceedings{Nakashima2017,
abstract = {Realtime novel view synthesis, which generates a novel view of a real object or scene in realtime, enjoys a wide range of applications including augmented reality, telepresence, and immersive telecommunication. Image-based rendering (IBR) with rough geometry can be done using only an off-the-shelf camera and thus can be used by many users. However, IBR from images in the wild (e.g., lighting condition changes or the scene contains objects with specular surfaces) has been a tough problem due to color discontinuity; IBR with rough geometry picks up appropriate images for a given viewpoint, but the image used for a rendering unit (a face or pixel) switches when the viewpoint moves, which may cause noticeable changes in color. We use the eigen-texture technique, which represents images for a certain face using a point in the eigenspace. We propose to regress a new point in this space, which moves smoothly, given a viewpoint so that we can generate an image whose color smoothly changes according to the point. Our regressor is based on a neural network with a single hidden layer and hyperbolic tangent nonlinearity. We demonstrate the advantages of our IBR approach using our own datasets as well as publicly available datasets for comparison.},
author = {Nakashima, Yuta and Okura, Fumio and Kawai, Norihiko and Kawasaki, Hiroshi and Blanco, Ambrosio and Ikeuchi, Katsushi},
booktitle = {Proceedings - British Machine Vision Conference (BMVC)},
isbn = {190172560X},
pages = {12 pages},
publisher = {BMVA Press},
title = {{Realtime novel view synthesis with eigen-texture regression}},
url = {http://www.bmva.org/bmvc/2017/papers/paper083/paper083.pdf},
year = {2017}
}
@article{Otani2018,
abstract = {Finding important regions is essential for applications, such as content-aware video compression and video retargeting to automatically crop a region in a video for small screens. Since people are one of main subjects when taking a video, some methods for finding important regions use a visual attention model based on face/pedestrian detection to incorporate the knowledge that people are important. However, such methods usually do not distinguish important people from passers-by and bystanders, which results in false positives. In this paper, we propose a deep neural network (DNN)-based method, which classifies a person into important or unimportant, given a video containing multiple people in a single frame and captured with a hand-held camera. Intuitively, important/ unimportant labels are highly correlated given that corresponding people's spatial motions are similar. Based on this assumption, we propose to boost the performance of our important/unimportant classification by using conditional random fields (CRFs) built upon the DNN, which can be trained in an end-to-end manner. Our experimental results show that our method successfully classifies important people and the use of a DNN with CRFs improves the accuracy.},
author = {Otani, Mayu and Nishida, Atsushi and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
doi = {10.1587/transinf.2018EDP7029},
issn = {17451361},
journal = {IEICE Transactions on Information and Systems},
keywords = {Conditional random field,Important people classification,Neural network},
month = {oct},
number = {10},
pages = {2509--2517},
publisher = {Institute of Electronics, Information and Communication, Engineers, IEICE},
title = {{Finding important people in a video using deep neural networks with conditional random fields}},
volume = {E101D},
year = {2018}
}
@inproceedings{Yoshida2018,
abstract = {Compressive video sensing is the process of encoding multiple sub-frames into a single frame with controlled sensor exposures and reconstructing the sub-frames from the single compressed frame. It is known that spatially and temporally random exposures provide the most balanced compression in terms of signal recovery. However, sensors that achieve a fully random exposure on each pixel cannot be easily realized in practice because the circuit of the sensor becomes complicated and incompatible with the sensitivity and resolution. Therefore, it is necessary to design an exposure pattern by considering the constraints enforced by hardware. In this paper, we propose a method of jointly optimizing the exposure patterns of compressive sensing and the reconstruction framework under hardware constraints. By conducting a simulation and actual experiments, we demonstrated that the proposed framework can reconstruct multiple sub-frame images with higher quality.},
author = {Yoshida, Michitaka and Torii, Akihiko and Okutomi, Masatoshi and Endo, Kenta and Sugiyama, Yukinobu and Taniguchi, Rin Ichiro and Nagahara, Hajime},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01249-6_39},
isbn = {9783030012489},
issn = {16113349},
keywords = {Compressive sensing,Deep neural network,Video reconstruction},
pages = {649--663},
publisher = {Springer Verlag},
title = {{Joint optimization for compressive video sensing and reconstruction under hardware constraints}},
volume = {11214 LNCS},
year = {2018}
}
@inproceedings{Inagaki2018,
abstract = {We propose a learning-based framework for acquiring a light field through a coded aperture camera. Acquiring a light field is a challenging task due to the amount of data. To make the acquisition process efficient, coded aperture cameras were successfully adopted; using these cameras, a light field is computationally reconstructed from several images that are acquirToshiakied with different aperture patterns. However, it is still difficult to reconstruct a high-quality light field from only a few acquired images. To tackle this limitation, we formulated the entire pipeline of light field acquisition from the perspective of an auto-encoder. This auto-encoder was implemented as a stack of fully convolutional layers and was trained end-to-end by using a collection of training samples. We experimentally show that our method can successfully learn good image-acquisition and reconstruction strategies. With our method, light fields consisting of 5 × 5 or 8 × 8 images can be successfully reconstructed only from a few acquired images. Moreover, our method achieved superior performance over several state-of-the-art methods. We also applied our method to a real prototype camera to show that it is capable of capturing a real 3-D scene.},
author = {Inagaki, Yasutaka and Kobayashi, Yuto and Takahashi, Keita and Fujii, Toshiaki and Nagahara, Hajime},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01234-2_26},
isbn = {9783030012335},
issn = {16113349},
keywords = {CNN,Coded aperture,Light field},
pages = {431--448},
publisher = {Springer Verlag},
title = {{Learning to capture light fields through a coded aperture camera}},
volume = {11211 LNCS},
year = {2018}
}
@inproceedings{Otani2017b,
author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkil{\"{a}}, Janne},
booktitle = {Proceedings - Open Knowledge Base and Question Answering Workshop at SIGIR},
pages = {3 pages},
title = {{Video question answering to find a desired video eegment}},
year = {2017}
}
@article{Tejero-De-Pablos2018,
abstract = {Automatically generating a summary of a sports video poses the challenge of detecting interesting moments, or highlights, of a game. Traditional sports video summarization methods leverage editing conventions of broadcast sports video that facilitate the extraction of high-level semantics. However, user-generated videos are not edited and, thus, traditional methods are not suitable to generate a summary. In order to solve this problem, this paper proposes a novel video summarization method that uses players' actions as a cue to determine the highlights of the original video. A deep neural-network-based approach is used to extract two types of action-related features and to classify video segments into interesting or uninteresting parts. The proposed method can be applied to any sports in which games consist of a succession of actions. Especially, this paper considers the case of Kendo (Japanese fencing) as an example of a sport to evaluate the proposed method. The method is trained using Kendo videos with ground truth labels that indicate the video highlights. The labels are provided by annotators possessing a different experience with respect to Kendo to demonstrate how the proposed method adapts to different needs. The performance of the proposed method is compared with several combinations of different features, and the results show that it outperforms previous summarization methods.},
archivePrefix = {arXiv},
arxivId = {1709.08421},
author = {Tejero-De-Pablos, Antonio and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu and Linna, Marko and Rahtu, Esa},
doi = {10.1109/TMM.2018.2794265},
eprint = {1709.08421},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {3D convolutional neural networks,Cameras,Feature extraction,Games,Hidden Markov models,Japanese fencing,Kendo videos,Semantics,Sports video summarization,Three-dimensional displays,action recognition,action-related features,deep action recognition features,deep learning,deep neural-network-based approach,feature extraction,high-level semantics,image segmentation,interesting parts,long short-term memory,neural nets,player action,sport,uninteresting parts,user-generated sport video summarization method,user-generated video,video highlights,video segments,video signal processing},
mendeley-tags = {3D convolutional neural networks,Cameras,Feature extraction,Games,Hidden Markov models,Japanese fencing,Kendo videos,Semantics,Sports video summarization,Three-dimensional displays,action recognition,action-related features,deep action recognition features,deep learning,deep neural-network-based approach,feature extraction,high-level semantics,image segmentation,interesting parts,long short-term memory,neural nets,player action,sport,uninteresting parts,user-generated sport video summarization method,user-generated video,video highlights,video segments,video signal processing},
month = {aug},
number = {8},
pages = {2000--2011},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Summarization of user-generated sports video by using deep action recognition features}},
url = {https://ieeexplore.ieee.org/document/8259321},
volume = {20},
year = {2018}
}
@inproceedings{Yamaguchi2020,
abstract = {This paper presents a method for reconstructing 3D image from multi-focus microscopic images captured with different focuses. We model the multi-focus imaging by a microscopy and produce the 3D image of a target object based on the model. The 3D image reconstruction is done by minimizing the difference between the observed images and the simulated images generated by the imaging model. Simulation and experimental result shows that the proposed method can generate the 3D image of a transparent object efficiently and reliably.},
author = {Yamaguchi, Takahiro and Nagahara, Hajime and Morooka, Ken'ichi and Nakashima, Yuta and Uranishi, Yuki and Miyauchi, Shoko and Kurazume, Ryo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-39770-8_6},
isbn = {9783030397692},
issn = {16113349},
keywords = {3D imaging,Microscopy,Multi-focus images,Transparent object},
pages = {73--85},
publisher = {Springer},
title = {{3D Image Reconstruction from Multi-focus Microscopic Images}},
volume = {11994 LNCS},
year = {2020}
}
@article{Matsumoto2019,
author = {Matsumoto, Tatsuya and Niioka, Hirohiko and Kumamoto, Yasuaki and Sato, Junya and Inamori, Osamu and Nakao, Ryuta and Harada, Yoshinori and Konishi, Eiichi and Otsuji, Eigo and Tanaka, Hideo and Miyake, Jun and Takamatsu, Tetsuro},
doi = {10.1038/s41598-019-53405-w},
issn = {2045-2322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {16912},
title = {{Deep-UV excitation fluorescence microscopy for detection of lymph node metastasis using deep neural network}},
url = {http://www.nature.com/articles/s41598-019-53405-w},
volume = {9},
year = {2019}
}
@inproceedings{Nishihara2019,
abstract = {We propose a method to control the level of a sentence in a text simplification task. Text simplification is a monolingual translation task translating a complex sentence into a simpler and easier to understand the alternative. In this study, we use the grade level of the US education system as the level of the sentence. Our text simplification method succeeds in translating an input into a specific grade level by considering levels of both sentences and words. Sentence level is considered by adding the target grade level as input. By contrast, the word level is considered by adding weights to the training loss based on words that frequently appear in sentences of the desired grade level. Although existing models that consider only the sentence level may control the syntactic complexity, they tend to generate words beyond the target level. Our approach can control both the lexical and syntactic complexity and achieve an aggressive rewriting. Experiment results indicate that the proposed method improves the metrics of both BLEU and SARI.},
address = {Florence, Italy},
author = {Nishihara, Daiki. and Kajiwara, Tomoyuki. and Arase, Yuki.},
booktitle = {Proceedings of the ACL 2019 Student Research Workshop (ACL 2019 SRW)},
doi = {10.18653/v1/P19-2036},
isbn = {9781950737475},
pages = {260--266},
publisher = {Association for Computational Linguistics},
title = {{Controllable text simplification with lexical constraint loss}},
url = {https://www.aclweb.org/anthology/P19-2036/},
year = {2019}
}
@article{Otani2017c,
abstract = {Authoring video blogs requires a video editing process, which is cumbersome for ordinary users. Video summarization can automate this process by extracting important segments from original videos. Because bloggers typically have certain stories for their blog posts, video summaries of a blog post should take the author's intentions into account. However, most prior works address video summarization by mining patterns from the original videos without considering the blog author's intentions. To generate a video summary that reflects the blog author's intention, we focus on supporting texts in video blog posts and present a text-based method, in which the supporting text serves as a prior to the video summary. Given video and text that describe scenes of interest, our method segments videos and assigns to each video segment its priority in the summary based on its relevance to the input text. Our method then selects a subset of segments with content that is similar to the input text. Accordingly, our method produces different video summaries from the same set of videos, depending on the input text. We evaluated summaries generated from both blog viewers' and authors' perspectives in a user study. Experimental results demonstrate the advantages to the proposed text-based method for video blog authoring.},
author = {Otani, Mayu and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
doi = {10.1007/s11042-016-4061-3},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Text-based video summarization,User study,Video skimming},
month = {may},
number = {9},
pages = {12097--12115},
publisher = {Springer New York LLC},
title = {{Video summarization using textual descriptions for authoring video blogs}},
volume = {76},
year = {2017}
}
@inproceedings{Renoust2019a,
abstract = {{\textcopyright} 2019 Copyright held by the owner/author(s). We introduce BUDA.ART, a system designed to assist researchers in Art History, to explore and analyze an archive of pictures of Buddha statues. The system combines different CBIR and classical retrieval techniques to assemble 2D pictures, 3D statue scans and meta-data, that is focused on the Buddha facial characteristics. We build the system from an archive of 50,000 Buddhism pictures, identify unique Buddha statues, extract contextual information, and provide specific facial embedding to first index the archive. The system allows for mobile, on-site search, and to explore similarities of statues in the archive. In addition, we provide search visualization and 3D analysis of the statues.},
author = {Renoust, Benjamin and Franca, Matheus Oliveira M.O. and Chan, Jacob and Le, Van and Uesaka, Ayaka and Nakashima, Yuta and Nagahara, Hajime and Wang, Jueren and Fujioka, Yutaka},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia (MM)},
doi = {10.1145/3343031.3350591},
isbn = {9781450368896},
keywords = {2D,3D,Art History,Multimedia Database,Search system,buddha},
mendeley-tags = {buddha},
title = {{Buda.art: A multimodal content-based analysis and retrieval system for Buddha statues}},
year = {2019}
}
@article{Yanagisawa2020,
abstract = {{\textless}p{\textgreater}It is known that single or isolated tumor cells enter cancer patients' circulatory systems. These circulating tumor cells (CTCs) are thought to be an effective tool for diagnosing cancer malignancy. However, handling CTC samples and evaluating CTC sequence analysis results are challenging. Recently, the convolutional neural network (CNN) model, a type of deep learning model, has been increasingly adopted for medical image analyses. However, it is controversial whether cell characteristics can be identified at the single-cell level by using machine learning methods. This study intends to verify whether an AI system could classify the sensitivity of anticancer drugs, based on cell morphology during culture. We constructed a CNN based on the VGG16 model that could predict the efficiency of antitumor drugs at the single-cell level. The machine learning revealed that our model could identify the effects of antitumor drugs with {\~{}}0.80 accuracies. Our results show that, in the future, realizing precision medicine to identify effective antitumor drugs for individual patients may be possible by extracting CTCs from blood and performing classification by using an AI system.{\textless}/p{\textgreater}},
author = {Yanagisawa, Kiminori and Toratani, Masayasu and Asai, Ayumu and Konno, Masamitsu and Niioka, Hirohiko and Mizushima, Tsunekazu and Satoh, Taroh and Miyake, Jun and Ogawa, Kazuhiko and Vecchione, Andrea and Doki, Yuichiro and Eguchi, Hidetoshi and Ishii, Hideshi},
doi = {10.3390/ijms21093166},
issn = {1422-0067},
journal = {International Journal of Molecular Sciences},
month = {apr},
number = {9},
pages = {3166},
title = {{Convolutional Neural Network Can Recognize Drug Resistance of Single Cancer Cells}},
url = {https://www.mdpi.com/1422-0067/21/9/3166},
volume = {21},
year = {2020}
}
@inproceedings{Nagahara2016,
abstract = {Several recent studies in compressive video sensing have realized scene capture beyond the fundamental trade-off limit between spatial resolution and temporal resolution using random space-time sampling. However, most of these studies showed results for higher frame rate video that were produced by simulation experiments or using an optically simulated random sampling camera, because there are currently no commercially available image sensors with random exposure or sampling capabilities. We fabricated a prototype complementary metal oxide semiconductor (CMOS) image sensor with quasi pixel-wise exposure timing that can realize nonuniform space-time sampling. The prototype sensor can reset exposures independently by columns and fix these amount of exposure by rows for each 8×8 pixel block. This CMOS sensor is not fully controllable via the pixels, and has line-dependent controls, but it offers flexibility when compared with regular CMOS or charge-coupled device sensors with global or rolling shutters. We propose a method to realize pseudo-random sampling for high-speed video acquisition that uses the flexibility of the CMOS sensor. We reconstruct the high-speed video sequence from the images produced by pseudo-random sampling using an over-complete dictionary. The proposed method also removes the rolling shutter effect from the reconstructed video.},
author = {Nagahara, Hajime and Sonoda, Toshiki and Endo, Kenta and Sugiyama, Yukinobu and Taniguchi, Rin Ichiro},
booktitle = {2016 IEEE International Conference on Computational Photography, ICCP 2016 - Proceedings},
doi = {10.1109/ICCPHOT.2016.7492875},
isbn = {9781467386234},
month = {jun},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{High-speed imaging using CMOS image sensor with quasi pixel-wise exposure}},
year = {2016}
}
@article{Minematsu2017,
abstract = {We propose a framework for adaptively registering background models with an image for background subtraction with moving cameras. Existing methods search for a background model using a fixed window size, to suppress the number of false positives when detecting the foreground. However, these approaches result in many false negatives because they may use inappropriate window sizes. The appropriate size depends on various factors of the target scenes. To suppress false detections, we propose adaptively controlling the method parameters, which are typically determined heuristically. More specifically, the search window size for background registration and the foreground detection threshold are automatically determined using the re-projection error computed by the homography based camera motion estimate. Our method is based on the fact that the error at a pixel is low if it belongs to background and high if it does not. We quantitatively confirmed that the proposed framework improved the background subtraction accuracy when applied to images from moving cameras in various public datasets.},
author = {Minematsu, Tsubasa and Uchiyama, Hideaki and Shimada, Atsushi and Nagahara, Hajime and ichiro Taniguchi, Rin},
doi = {10.1016/j.patrec.2017.03.010},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Background subtraction,Moving camera,Moving object detection,Re-projection error},
month = {sep},
pages = {86--95},
publisher = {Elsevier B.V.},
title = {{Adaptive background model registration for moving cameras}},
volume = {96},
year = {2017}
}
@article{Nakashima2020,
abstract = {We present a system for reenacting a person's face driven by speech. Given a video sequence with the corresponding audio track of a person giving a speech and another audio track containing different speech from the same person, we reconstruct a 3D mesh of the face in each frame of the video sequence to match the speech in the second audio track. Audio features are extracted from such two audio tracks. Assuming that the appearance of the mouth is highly correlated to these speech features, we extract the mouth region of the face's 3D mesh from the video sequence when speech features from the second audio track are close to those of the video's audio track. While retaining temporal consistency, these extracted mouth regions then replace the original mouth regions in the video sequence, synthesizing a reenactment video where the person seemingly gives the speech from the second audio track. Our system, coined S2TH (speech to talking head), does not require any special hardware to capture the 3D geometry of faces but uses the state-of-the-art method for facial geometry regression. We visually and subjectively demonstrate reenactment quality.},
author = {Nakashima, Yuta and Yasui, Takaaki and Nguyen, Leon and Babaguchi, Noboru},
doi = {10.3169/mta.8.60},
issn = {21867364},
journal = {ITE Transactions on Media Technology and Applications},
keywords = {3D geometry,Face reenactment,Speech-driven,Talking head,speech-driven,talking head},
mendeley-tags = {3D geometry,Face reenactment,speech-driven,talking head},
month = {jan},
number = {1},
pages = {60--68},
publisher = {Institute of Image Information and Television Engineers},
title = {{Speech-driven face reenactment for a video sequence}},
volume = {8},
year = {2020}
}
@inproceedings{Otani2017a,
author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkil{\"{a}}, Janne and Yokoya, Naokazu},
booktitle = {画像の認識・理解シンポジウム(MIRU2017)論文集},
title = {{Unsupervised Video Summarization using Deep Video Features}},
volume = {2017},
year = {2017}
}
@inproceedings{Garcia2020a,
abstract = {We propose a novel video understanding task by fusing knowledge-based and video question answering. First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer pairs about a popular sitcom. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered. Second, we propose a video understanding model by combining the visual and textual video content with specific knowledge about the show. Our main findings are: (i) the incorporation of knowledge produces outstanding improvements for VQA in video, and (ii) the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations.},
archivePrefix = {arXiv},
arxivId = {1910.10706},
author = {Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta},
booktitle = {Proceedings - 2020 AAAI Conference on Artificial Intelligence},
eprint = {1910.10706},
file = {:Users/n-yuta/Documents/Mendeley Desktop//Garcia et al. - 2020 - KnowIT VQA Answering knowledge-based questions about videos.pdf:pdf},
keywords = {kvqa},
mendeley-tags = {kvqa},
month = {feb},
pages = {10826--10834},
shorttitle = {KnowIT VQA},
title = {{KnowIT VQA: Answering knowledge-based questions about videos}},
url = {http://arxiv.org/abs/1910.10706 https://aaai.org/ojs/index.php/AAAI/article/view/6713/6567},
year = {2020}
}
@article{Ngo2019,
abstract = {Reflectance and shape are two important components in visually perceiving the real world. Inferring the reflectance and shape of an object through cameras is a fundamental research topic in the field of computer vision. While three-dimensional shape recovery is pervasive with varieties of approaches and practical applications, reflectance recovery has only emerged recently. Reflectance recovery is a challenging task that is usually conducted in controlled environments, such as a laboratory environment with a special apparatus. However, it is desirable that the reflectance be recovered in the field with a handy camera so that reflectance can be jointly recovered with the shape. To that end, we present a solution that simultaneously recovers the reflectance and shape (i.e., dense depth and normal maps) of an object under natural illumination with commercially available handy cameras. We employ a light field camera to capture one light field image of the object, and a 360-degree camera to capture the illumination. The proposed method provides positive results in both simulation and real-world experiments.},
author = {Ngo, Thanh Trung and Nagahara, Hajime and Nishino, Ko and ichiro Taniguchi, Rin and Yagi, Yasushi},
doi = {10.1007/s11263-019-01149-5},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Light field camera,Natural illumination,Reflectance,Shape from shading},
month = {dec},
number = {11-12},
pages = {1707--1722},
publisher = {Springer New York LLC},
title = {{Reflectance and Shape Estimation with a Light Field Camera Under Natural Illumination}},
volume = {127},
year = {2019}
}
@inproceedings{Ohsaki2017,
abstract = {{\textcopyright} 2017 SPIE. Hyperspectral imaging is used in various fields because it can obtain much more information than imaging by conventional RGB cameras. Hyperspectral imaging systems using active illumination, prisms, gratings, or narrowband filters have been proposed. Active illumination systems can obtain two-dimensional (2D) spectral images rapidly, and the device can be low-cost and small because of the use of LEDs. However, flicker can occur when different colors of LEDs are switched. The other methods do not have the flicker problem because they use passive imaging. However, these systems take a long time to acquire the 2D spectral images, or they tend to be high-cost or large. In our research, we propose a flickerless active LED illumination system for hyperspectral imaging. This system acquires images while switching the illumination. The switching illumination consists of many narrowband LEDs that have different spectrums. The spectral images of each LED are reconstructed from the acquired images. The switching illumination is designed to reduce the flicker based on human visual characteristics. We reduce the color changes of the switching illumination while maintaining its spectral differences. In the experiment, we obtain the optimal design of a flickerless illumination system for measuring oxygen saturation. To show the feasibility of our system, we clearly show the difference in saturation using the spectral images obtained by a prototype designed using the proposed method.},
author = {Ohsaki, Makoto and Nagahara, Hajime and Ikeda, Tetsuo and Taniguchi, Rin-ichiro},
booktitle = {Thirteenth International Conference on Quality Control by Artificial Vision 2017},
doi = {10.1117/12.2266765},
isbn = {9781510611214},
issn = {1996756X},
month = {mar},
pages = {103380Z},
publisher = {SPIE},
title = {{Hyperspectral imaging using flickerless active LED illumination}},
volume = {10338},
year = {2017}
}
@inproceedings{Kajiwara2018,
abstract = {We introduce the TMU systems for the Complex Word Identification (CWI) Shared Task 2018. TMU systems use random forest classifiers and regressors whose features are the number of characters, the number of words, and the frequency of target words in various corpora. Our simple systems performed best on 5 tracks out of 12 tracks. Our ablation analysis revealed the usefulness of a learner corpus for CWI task.},
address = {Stroudsburg, PA, USA},
author = {Kajiwara, Tomoyuki and Komachi, Mamoru},
booktitle = {Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 13)},
doi = {10.18653/v1/W18-0521},
pages = {195--199},
publisher = {Association for Computational Linguistics},
title = {{Complex word identification based on frequency in a learner corpus}},
url = {http://aclweb.org/anthology/W18-0521},
year = {2018}
}
@inproceedings{Chu2018,
author = {Chu, Chenhui and Otani, Mayu and Nakashima, Yuta},
booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
keywords = {vgp},
mendeley-tags = {vgp},
pages = {3479--3492},
title = {{Visually grounded paraphrase extraction}},
year = {2018}
}
@inproceedings{Sayo2019,
abstract = {Reconstructing the entire body of moving human in a computer is important for various applications, such as tele-presence, virtual try-on, etc. For the purpose, realistic representation of loose clothes or non-rigid body deformation is a challenging and important task. Recent approaches for full-body reconstruction use a statistical shape model, which is built upon accurate full-body scans of people in skin-tight clothes. Such a model can be fitted to a point cloud of a person wearing loose clothes, however, it cannot represent the detailed shape of loose clothes, such as wrinkles and/or folds. In this paper, we propose a method that reconstructs 3D model of full-body human with loose clothes by reproducing the deformations as displacements from the skin-tight body mesh. We take advantage of a statistical shape model as base shape of full-body human mesh, and then, obtain displacements from the base mesh by non-rigid registration. To efficiently represent such displacements, we use lower dimensional embeddings of the deformations. This enables us to regress the coefficients corresponding to the small number of bases. We also propose a method to reconstruct shape only from a single 3D scanner, which is realized by shape fitting to only visible meshes as well as intra-frame shape interpolation. Our experiments with both unknown scene and partial body scans confirm the reconstruction ability of our proposed method.},
author = {Sayo, Akihiko and Onizuka, Hayato and Thomas, Diego and Nakashima, Yuta and Kawasaki, Hiroshi and Ikeuchi, Katsushi},
booktitle = {Proceedings - Pacific-Rim Symposium on Image and Video Technology},
doi = {10.1007/978-3-030-34879-3_18},
isbn = {978-3-030-34879-3},
issn = {16113349},
keywords = {Eigen-deformation,Human shape reconstruction,Neural network,Non-rigid registration},
language = {en},
mendeley-tags = {Eigen-deformation,Human shape reconstruction,Neural network,Non-rigid registration},
month = {nov},
pages = {225--239},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Human shape reconstruction with loose clothes from partially observed data by pose specific deformation}},
year = {2019}
}
@inproceedings{Ashihara2019b,
abstract = {Topic modeling is a key component to computational legal science. Network analysis is also very important to further understand the structure of references in legal documents. In this paper, we improve topic modeling for legal case documents by using homophily networks derived from two families of references: prior cases and statute laws. We perform a detailed analysis on a rich legal case dataset in order to create these networks. The use of the reference-induced homophily topic modeling improves on prior methods.},
author = {Ashihara, Kazuki and Chu, Chenhui and Renoust, Benjamin and Okubo, Noriko and Takemura, Noriko and Nakashima, Yuta and Nagahara, Hajime},
booktitle = {Proceedings - International Conference on Complex Networks and Their Applications},
doi = {10.1007/978-3-030-36683-4_3},
isbn = {978-3-030-36683-4},
keywords = {Homophily,Network of legal documents,Topic modeling},
language = {en},
mendeley-tags = {Homophily,Network of legal documents,Topic modeling},
month = {nov},
pages = {28--39},
publisher = {Springer International Publishing},
shorttitle = {Legal Information as a Complex Network},
title = {{Legal information as a complex network: Improving topic modeling through homophily}},
year = {2019}
}
@inproceedings{Terai2020,
abstract = {Drowsiness is a major factor that hinders learning. To improve learning efficiency, it is important to understand students' physical status such as wakefulness during online coursework. In this study, we have proposed a drowsiness estimation method based on learners' head and facial movements while viewing video lectures. To examine the effectiveness of head and facial movements in drowsiness estimation, we collected learner video data recorded during e-learning and applied a deep learning approach under the following conditions: (a) using only facial movement data, (b) using only head movement data, and (c) using both facial and head movement data.We achieved an average F1-macro score of 0.74 in personalized models for detecting learner drowsiness using both facial and head movement data.},
author = {Terai, Shogo and Shirai, Shizuka and Alizadeh, Mehrasa and Kawamura, Ryosuke and Takemura, Noriko and Uranishi, Yuki and Takemura, Haruo and Nagahara, Hajime},
booktitle = {International Conference on Intelligent User Interfaces, Proceedings IUI},
doi = {10.1145/3379336.3381500},
isbn = {9781450375139},
keywords = {Drowsiness estimation,E-learning,Facial movements,Head movements},
month = {mar},
pages = {124--125},
publisher = {Association for Computing Machinery},
title = {{Detecting learner drowsiness based on facial expressions and head movements in online courses}},
year = {2020}
}
@inproceedings{Nagahara2018,
abstract = {Most conventional digital video cameras face a fundamental trade-off between spatial resolution, temporal resolution and dynamic range (i.e., brightness resolution) because of a limited bandwidth for data transmission. A few recent studies have shown that with non-uniform space-time sampling, such as that implemented with pixel-wise coded exposure, one can go beyond this trade-off and achieve high efficiency for scene capture. However, in these studies, the sampling schemes were pre-defined and independent of the target scene content. In this paper, we propose an adaptive space-time-brightness sampling method to further improve the efficiency of video capture. The proposed method adaptively updates a pixel-wise coded exposure pattern using the information analyzed from previously captured frames. We built a prototype camera that enables adaptive coding of patterns online to show the feasibility of the proposed adaptive coded exposure method. Simulation and experimental results show that the adaptive space-time-brightness sampling scheme achieves more accurate video reconstruction results and high dynamic range with less computational cost, than previous method. To the best of our knowledge, our prototype is the first implementation of an adaptive pixel-wise coded exposure camera.},
author = {Nagahara, Hajime and Liu, Dengyu and Sonoda, Toshiki and Gu, Jinwei},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2018.00237},
isbn = {9781538661000},
issn = {21607516},
month = {dec},
pages = {1915--1923},
publisher = {IEEE Computer Society},
title = {{Space-time-brightness sampling using an adaptive pixel-wise coded exposure}},
volume = {2018-June},
year = {2018}
}
@inproceedings{Otani2019a,
abstract = {Video summarization is a technique to create a short skim of the original video while preserving the main stories/content. There exists a substantial interest in automatizing this process due to the rapid growth of the available material. The recent progress has been facilitated by public benchmark datasets, which enable easy and fair comparison of methods. Currently the established evaluation protocol is to compare the generated summary with respect to a set of reference summaries provided by the dataset. In this paper, we will provide in-depth assessment of this pipeline using two popular benchmark datasets. Surprisingly, we observe that randomly generated summaries achieve comparable or better performance to the state-of-the-art. In some cases, the random summaries outperform even the human generated summaries in leave-one-out experiments. Moreover, it turns out that the video segmentation, which is often considered as a fixed pre-processing method, has the most significant impact on the performance measure. Based on our observations, we propose alternative approaches for assessing the importance scores as well as an intuitive visualization of correlation between the estimated scoring and human annotations.},
archivePrefix = {arXiv},
arxivId = {1903.11328},
author = {Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkil{\"{a}}, Janne and Heikkila, Janne},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.00778},
eprint = {1903.11328},
isbn = {9781728132938},
issn = {10636919},
keywords = {Datasets and Evaluation,Vision Applications and Systems},
title = {{Rethinking the evaluation of video summaries}},
year = {2019}
}
@inproceedings{Verma2020,
abstract = {Human pose estimation is a well-known problem in computer vision to locate joint positions. Existing datasets for the learning of poses are observed to be not challenging enough in terms of pose diversity, object occlusion, and viewpoints. This makes the pose annotation process relatively simple and restricts the application of the models that have been trained on them. To handle more variety in human poses, we propose the concept of fine-grained hierarchical pose classification, in which we formulate the pose estimation as a classification task, and propose a dataset, Yoga-82, for large-scale yoga pose recognition with 82 classes. Yoga-82 consists of complex poses where fine annotations may not be possible. To resolve this, we provide hierarchical labels for yoga poses based on the body configuration of the pose. The dataset contains a three-level hierarchy including body positions, variations in body positions, and the actual pose names. We present the classification accuracy of the state-of-the-art convolutional neural network architectures on Yoga-82. We also present several hierarchical variants of DenseNet in order to utilize the hierarchical labels.},
archivePrefix = {arXiv},
arxivId = {2004.10362},
author = {Verma, Manisha and Kumawat, Sudhakar and Nakashima, Yuta and Raman, Shanmuganathan},
booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
eprint = {2004.10362},
month = {apr},
title = {{Yoga-82: a new dataset for fine-grained classification of human poses}},
url = {http://openaccess.thecvf.com/content{\_}CVPRW{\_}2020/papers/w70/Verma{\_}Yoga-82{\_}A{\_}New{\_}Dataset{\_}for{\_}Fine-Grained{\_}Classification{\_}of{\_}Human{\_}Poses{\_}CVPRW{\_}2020{\_}paper.pdf},
year = {2020}
}
@inproceedings{Verma2019,
abstract = {Deep convolutional neural networks (CNNs) have established their feet in the ground of computer vision and machine learning, used in various applications. In this work, an attempt is made to learn a CNN for a task of facial expression recognition (FER). Our network has convolutional layers linked with an FC layer with a skip-connection to the classification layer. Motivation behind this design is that lower layers of a CNN are responsible for lower level features, and facial expressions can be mainly encoded in low-to-mid level features. Hence, in order to leverage the responses from lower layers, all convo-lutional layers are integrated via FC layers. Moreover, a network with shared parameters is used to extract landmark motion trajectory features. These visual and landmark features are fused to improve the performance. Our method is evaluated on the CK+ and Oulu-CASIA facial expression datasets.},
address = {Taipei},
author = {Verma, Manisha and Kobori, Hirokazu and Nakashima, Yuta and Takemura, Noriko and Nagahara, Hajime},
booktitle = {Proceedings - IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2019.8803396},
isbn = {9781538662496},
pages = {51--55},
publisher = {IEEE},
title = {{Facial expression recognition with skip-connection to leverage low-level features}},
url = {https://ieeexplore.ieee.org/abstract/document/8803396},
year = {2019}
}
@inproceedings{Yoda2016,
abstract = {Photometric stereo enables the estimation of surface normals from images that were captured using different known lighting directions. The classical photometric stereo method requires at least three images to determine the normals of a given scene. This method therefore cannot be applied to a dynamic scene, because it is assumed that the scene should remain static while the required images are captured. We present a dynamic photometric stereo method to estimate the surface normals in a dynamic scene. We use a multi-tap complementary metal-oxide-semiconductor (CMOS) image sensor to capture the input images for the photometric stereo method. The image sensor can divide the electrons from the photodiode of a single pixel into different taps of exposures, and can therefore capture multiple images under different lighting conditions with almost the same timing. We implemented a prototype camera that was synchronized with a lighting system, and subsequently realized photometric stereo of a dynamic scene.},
author = {Yoda, Takuya and Nagahara, Hajime and Taniguchi, Rin Ichiro and Kagawa, Keiichiro and Yasutomi, Keita and Kawahito, Shoji},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2016.7899988},
isbn = {9781509048472},
issn = {10514651},
month = {jan},
pages = {2356--2361},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Dynamic photometric stereo method using multi-tap CMOS image sensor}},
volume = {0},
year = {2016}
}
@article{Kawai2017a,
abstract = {Augmented reality (AR) marker hiding is a technique to visually remove AR markers in a real-time video stream. A conventional approach transforms a background image with a homography matrix calculated on the basis of a camera pose and overlays the transformed image on an AR marker region in a real-time frame, assuming that the AR marker is on a planar surface. However, this approach may cause discontinuities in textures around the boundary between the marker and its surrounding area when the planar surface assumption is not satisfied. This paper proposes a method for AR marker hiding without discontinuities around texture boundaries even under nonplanar background geometry without measuring it. For doing this, our method estimates the dense motion in the marker's background by analyzing the motion of sparse feature points around it, together with a smooth motion assumption, and deforms the background image according to it. Our experiments demonstrate the effectiveness of the proposed method in various environments with different background geometries and textures.},
author = {Kawai, Norihiko and Sato, Tomokazu and Nakashima, Yuta and Yokoya, Naokazu},
doi = {10.1109/TVCG.2016.2617325},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Marker hiding,diminished reality,texture deformation},
month = {oct},
number = {10},
pages = {2288--2300},
pmid = {27775521},
publisher = {IEEE Computer Society},
title = {{Augmented reality marker hiding with texture deformation}},
volume = {23},
year = {2017}
}
@article{Yagi2018,
abstract = {A light field, which is often understood as a set of dense multi-view images, has been utilized in various 2D/3D applications. Efficient light field acquisition using a coded aperture camera is the target problem considered in this paper. Specifically, the entire light field, which consists of many images, should be reconstructed from only a few images that are captured through different aperture patterns. In previous work, this problem has often been discussed from the context of compressed sensing (CS), where sparse representations on a pre-trained dictionary or basis are explored to reconstruct the light field. In contrast, we formulated this problem from the perspective of principal component analysis (PCA) and non-negative matrix factorization (NMF), where only a small number of basis vectors are selected in advance based on the analysis of the training dataset. From this formulation, we derived optimal non-negative aperture patterns and a straight-forward reconstruction algorithm. Even though our method is based on conventional techniques, it has proven to be more accurate and much faster than a state-of-the-art CS-based method.},
author = {Yagi, Yusuke and Takahashi, Keita and Fujii, Toshiaki and Sonoda, Toshiki and Nagahara, Hajime},
doi = {10.1587/transinf.2017PCP0007},
issn = {17451361},
journal = {IEICE Transactions on Information and Systems},
keywords = {Coded aperture,Light field,Nonnegative matrix factorization,Principal component analysis},
month = {sep},
number = {9},
pages = {2190--2200},
publisher = {Institute of Electronics, Information and Communication, Engineers, IEICE},
title = {{Designing coded aperture camera based on PCA and NMF for light field acquisition}},
volume = {E101D},
year = {2018}
}
@inproceedings{NoaGarciaChenhuiChu2019,
abstract = {In this work, we address knowledge-based visual question answering in videos. First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer pairs that combines visual, textual and temporal coherence reasoning together with knowledge-based questions. Second, we propose a video understanding model by combining the visual and textual video information with specific knowledge about the dataset. We find that the incorporation of knowledge produces outstanding improvements for VQA in video. However, the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations.},
author = {Garcia, Noa and Chu, Chenhui and Otani, Mayu and Nakashima, Yuta},
booktitle = {画像の認識・理解シンポジウム(MIRU2019)論文集},
title = {{Video meets knowledge in visual question answering}},
volume = {2019},
year = {2019}
}
@article{Hirose2018a,
abstract = {{\textcopyright} 2018 Optical Society of America. Label-free visualization of nerves and nervous plexuses will improve the preservation of neurological functions in nerve-sparing robot-assisted surgery. We have developed a coherent anti-Stokes Raman scattering (CARS) rigid endoscope to distinguish nerves from other tissues during surgery. The developed endoscope, which has a tube with a diameter of 12 mm and a length of 270 mm, achieved 0.91{\%} image distortion and 8.6{\%} non-uniformity of CARS intensity in the whole field of view (650 µm diameter). We demonstrated CARS imaging of a rat sciatic nerve and visualization of the fine structure of nerve fibers.},
author = {Hirose, K. and Aoki, T. and Furukawa, T. and Fukushima, S. and Niioka, H. and Deguchi, S. and Hashimoto, M.},
doi = {10.1364/BOE.9.000387},
issn = {21567085},
journal = {Biomedical Optics Express},
number = {2},
title = {{Coherent anti-stokes Raman scattering rigid endoscope toward robot-assisted surgery}},
volume = {9},
year = {2018}
}
@inproceedings{Shirai2019,
author = {Shirai, Shizuka and Takemura, Noriko and Nakashima, Yuta and Nagahara, Hajime and Takemura, Haruo},
booktitle = {Companion Proceedings of the 9th International Conference on Learning Analytics {\&} Knowledge},
keywords = {sensing},
mendeley-tags = {sensing},
month = {mar},
pages = {170--171},
title = {{Multimodal learning analytics: Society 5.0 project in Japan}},
year = {2019}
}
@inproceedings{Dayrit2017,
abstract = {We propose ReMagicMirror, a system to help people learn actions (e.g., martial arts, dances). We first capture the motions of a teacher performing the action to learn, using two RGB-D cameras. Next, we fit a parametric human body model to the depth data and texture it using the color data, reconstructing the teacher's motion and appearance. The learner is then shown the ReMagicMirror system, which acts as a mirror. We overlay the teacher's reconstructed body on top of this mirror in an augmented reality fashion. The learner is able to intuitively manipulate the reconstruction's viewpoint by simply rotating her body, allowing for easy comparisons between the learner and the teacher. We perform a user study to evaluate our system's ease of use, effectiveness, quality, and appeal.},
author = {Dayrit, Fabian Lorenzo and Kimura, Ryosuke and Nakashima, Yuta and Blanco, Ambrosio and Kawasaki, Hiroshi and Ikeuchi, Katsushi and Sato, Tomokazu and Yokoya, Naokazu},
booktitle = {Proceedings - International Conference on Multimedia Modeling (MMM)},
doi = {10.1007/978-3-319-51811-4_25},
isbn = {9783319518107},
issn = {16113349},
keywords = {3D human reconstruction,Human reenactment,RGB-D sensors,sensing},
mendeley-tags = {sensing},
pages = {303--315},
publisher = {Springer Verlag},
title = {{ReMagicMirror: Action learning using human reenactment with the mirror metaphor}},
year = {2017}
}
@inproceedings{Ma2017,
abstract = {{\textcopyright} 2017 SPIE. An infrared (IR) camera captures the temperature distribution of an object as an IR image. Because facial temperature is almost constant, an IR camera has the potential to be used in detecting facial regions in IR images. However, in detecting faces, a simple temperature thresholding does not always work reliably. The standard face detection algorithm used is AdaBoost with local features, such as Haar-like, MB-LBP, and HOG features in the visible images. However, there are few studies using these local features in IR image analysis. In this paper, we propose an AdaBoost-based training method to mix these local features for face detection in thermal images. In an experiment, we captured a dataset from 20 participants, comprising 14 males and 6 females, with 10 variations in camera distance, 21 poses, and participants with and without glasses. Using leave-one-out cross-validation, we show that the proposed mixed features have an advantage over all the regular local features.},
author = {Ma, C. and Trung, N.T. and Uchiyama, H. and Nagahara, H. and Shimada, A. and Taniguchi, R.-I.},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.2266836},
isbn = {9781510611214},
issn = {1996756X},
keywords = {Face detection,Haar-like,Histogram of oriented gradient,Local-binary pattern,Mixed features,Thermal image},
title = {{Mixed features for face detection in thermal image}},
volume = {10338},
year = {2017}
}
@inproceedings{NguyenCanh2019,
abstract = {Detection followed by projection in conventional privacy cameras is vulnerable to software attacks that threaten to expose image sensor data. By multiplexing the incoming light with a coded mask, a FlatCam camera removes the spatial correlation and captures visually protected images. However, FlatCam imaging suffers from poor reconstruction quality and pays no attention to the privacy of visual information. In this paper, we propose a deep learning-based compressive sensing approach to reconstruct and protect sensitive regions from secured FlatCam measurements. We predict sensitive regions via facial segmentation and separate them from the captured measurements. Our deep compressive sensing network was trained with simulated data, and was tested on both simulated and real FlatCam data.},
author = {{Nguyen Canh}, Thuong and Nagahara, Hajime},
booktitle = {Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019},
doi = {10.1109/ICCVW.2019.00492},
isbn = {9781728150239},
keywords = {Compressive sensing,Deep compressive sensing,Deep learning,Visual privacy protection},
month = {oct},
pages = {3978--3986},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Deep compressive sensing for visual privacy protection in flatcam imaging}},
year = {2019}
}
@inproceedings{Maruyama2019,
abstract = {We propose an efficient pipeline from input to output for a tensor light-field display. Conventionally, a dense light field (i.e., tens of images taken with narrow viewpoint intervals) is required as an input in such displays. However, obtaining dense light fields is a challenging task for real scenes. To make the acquisition process more efficient, we adopted a coded-aperture camera as an input device, which is suitable for acquiring dense light fields in a compressive manner. Moreover, we modeled the entire process from acquisition to display using a convolutional neural network. As a result of training the network on a massive light field data, we can reproduce the whole light field on the display from only a few images taken with the camera. Both simulative and real experiments were conducted to show the effectiveness of our method.},
author = {Maruyama, Keita and Inagaki, Yasutaka and Takahashi, Keita and Fujii, Toshiaki and Nagahara, Hajime},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2019.8803741},
isbn = {9781538662496},
issn = {15224880},
keywords = {3-D display,coded-aperture camera,light field},
month = {sep},
pages = {1064--1068},
publisher = {IEEE Computer Society},
title = {{A 3-D Display Pipeline from Coded-Aperture Camera to Tensor Light-Field Display Through CNN}},
volume = {2019-Septe},
year = {2019}
}
@inproceedings{Ashihara2019,
abstract = {Lexical substitution ranks substitution candidates from the viewpoint of paraphrasability for a target word in a given sentence. There are two major approaches for lexical substitution: (1) generating contextualized word embeddings by assigning multiple embeddings to one word and (2) generating context embeddings using the sentence. Herein we propose a method that combines these two approaches to contextualize word embeddings for lexical substitution. Experiments demonstrate that our method outperforms the current state-of-the-art method. We also create CEFR-LP, a new evaluation dataset for the lexical substitution task. It has a wider coverage of substitution candidates than previous datasets and assigns English proficiency levels to all target words and substitution candidates.},
address = {Stroudsburg, PA, USA},
author = {Ashihara, Kazuki and Kajiwara, Tomoyuki and Arase, Yuki and Uchida, Satoru},
booktitle = {Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)},
doi = {10.18653/v1/D19-5552},
pages = {397--406},
publisher = {Association for Computational Linguistics},
title = {{Contextualized context2vec}},
url = {https://www.aclweb.org/anthology/D19-5552},
year = {2019}
}
@article{Miyake2018,
abstract = {{\textcopyright} 2018 The Author(s) Alleles of human leukocyte antigen (HLA)-A DNAs are classified and expressed graphically by using artificial intelligence “Deep Learning (Stacked autoencoder)”. Nucleotide sequence data corresponding to the length of 822 bp, collected from the Immuno Polymorphism Database, were compressed to 2-dimensional representation and were plotted. Profiles of the two-dimensional plots indicate that the alleles can be classified as clusters are formed. The two-dimensional plot of HLA-A DNAs gives a clear outlook for characterizing the various alleles.},
author = {Miyake, J. and Kaneshita, Y. and Asatani, S. and Tagawa, S. and Niioka, H. and Hirano, T.},
doi = {10.1007/s13577-017-0194-6},
issn = {17490774},
journal = {Human Cell},
keywords = {Allele,Artificial intelligence,Autoencoder,Deep learning,HLA},
title = {{Graphical classification of DNA sequences of HLA alleles by deep learning}},
year = {2018}
}
@article{Tanaka2018,
abstract = {Image completion is a technique to fill missing regions in a damaged or redacted image. A patch-based approach is one of major approaches, which solves an optimization problem that involves pixel values in missing regions and similar image patch search. One major problem of this approach is that it sometimes duplicates implausible texture in the image or overly smooths down a missing region when the algorithm cannot find better patches. As a practical remedy, the user may provide an interaction to identify such regions and re-apply image completion iteratively until she/he gets a desirable result. In this work, inspired by this idea, we propose a framework of human-in-the-loop style image completion with automatic failure detection using a deep neural network instead of human interaction. Our neural network takes small patches extracted from multiple feature maps obtained from the completion process as input for the automated interaction process, which is iterated several times. We experimentally show that our neural network outperforms a conventional linear support vector machine. Our subjective evaluation demonstrates that our method drastically improves the visual quality of resulting images compared to non-iterative application.},
author = {Tanaka, Takahiro and Kawai, Norihiko and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
doi = {10.1016/j.jvcir.2018.05.015},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Convolutional neural network,Failure detection,Image completion,Image inpainting},
month = {aug},
pages = {56--66},
publisher = {Academic Press Inc.},
title = {{Iterative applications of image completion with CNN-based failure detection}},
volume = {55},
year = {2018}
}
@article{Ma2017a,
abstract = {A thermal camera captures the temperature distribution of a scene as a thermal image. In thermal images, facial appearances of different people under different lighting conditions are similar. This is because facial temperature distribution is generally constant and not affected by lighting condition. This similarity in face appearances is advantageous for face detection. To detect faces in thermal images, cascade classifiers with Haar-like features are generally used. However, there are few studies exploring the local features for face detection in thermal images. In this paper, we introduce two approaches relying on local features for face detection in thermal images. First, we create new feature types by extending Multi-Block LBP. We consider a margin around the reference and the generally constant distribution of facial temperature. In this way, we make the features more robust to image noise and more effective for face detection in thermal images. Second, we propose an AdaBoost-based training method to get cascade classifiers with multiple types of local features. These feature types have different advantages. In this way we enhance the description power of local features. We did a hold-out validation experiment and a field experiment. In the hold-out validation experiment, we captured a dataset from 20 participants, comprising 14 males and 6 females. For each participant, we captured 420 images with 10 variations in camera distance, 21 poses, and 2 appearances (participant with/without glasses). We compared the performance of cascade classifiers trained by different sets of the features. The experiment results showed that the proposed approaches effectively improve the performance of face detection in thermal images. In the field experiment, we compared the face detection performance in realistic scenes using thermal and RGB images, and gave discussion based on the results.},
author = {Ma, Chao and Trung, Ngo Thanh and Uchiyama, Hideaki and Nagahara, Hajime and Shimada, Atsushi and Taniguchi, Rin Ichiro},
doi = {10.3390/s17122741},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {AdaBoost,Face detection,Haar-like,Histogram of oriented gradient,Local binary pattern,Local ternary pattern,Mixed features,Thermal image},
month = {dec},
number = {12},
publisher = {MDPI AG},
title = {{Adapting local features for face detection in thermal image}},
volume = {17},
year = {2017}
}
@article{Dayrit2017a,
abstract = {Standard video does not capture the 3D aspect of human motion, which is important for comprehension of motion that may be ambiguous. In this paper, we apply augmented reality (AR) techniques to give viewers insight into 3D motion by allowing them to manipulate the viewpoint of a motion sequence of a human actor using a handheld mobile device. The motion sequence is captured using a single RGB-D sensor, which is easier for a general user, but presents the unique challenge of synthesizing novel views using images captured from a single viewpoint. To address this challenge, our proposed system reconstructs a 3D model of the actor, then uses a combination of the actor's pose and viewpoint similarity to find appropriate images to texture it. The system then renders the 3D model on the mobile device using visual SLAM to create a map in order to use it to estimate the mobile device's camera pose relative to the original capturing environment. We call this novel view of a moving human actor a reenactment, and evaluate its usefulness and quality with an experiment and a survey.},
author = {Dayrit, Fabian Lorenzo and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
doi = {10.1007/s11042-015-3116-1},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Augmented reality,Mobile,Novel view synthesis,Reenactment},
month = {jan},
number = {1},
pages = {1291--1312},
publisher = {Springer New York LLC},
title = {{Increasing pose comprehension through augmented reality reenactment}},
volume = {76},
year = {2017}
}
@inproceedings{Shimanaka2018a,
abstract = {Sentence representations can capture a wide range of information that cannot be captured by local features based on character or word N-grams. This paper examines the usefulness of universal sentence representations for evaluating the quality of machine translation. Al-though it is difficult to train sentence representations using small-scale translation datasets with manual evaluation, sentence representations trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. Experimental results of the WMT-2016 dataset show that the proposed method achieves state-of-the-art performance with sentence representation features only.},
address = {Stroudsburg, PA, USA},
author = {Shimanaka, Hiroki and Kajiwara, Tomoyuki and Komachi, Mamoru},
booktitle = {Proceedings of the NAACL 2018 Student Research Workshop (NAACL 2018 SRW)},
doi = {10.18653/v1/N18-4015},
pages = {106--111},
publisher = {Association for Computational Linguistics},
title = {{Metric for automatic machine translation evaluation based on universal sentence representations}},
url = {http://aclweb.org/anthology/N18-4015},
year = {2018}
}
@book{Hamasaki2019,
abstract = {{\textcopyright} 2019, Springer Nature Switzerland AG. Barcodes and 2D codes are widely used for various purposes, such as electronic payments and product management. Special code readers, and consumer smartphones can be used to scan codes; thus concerns about fraud and authenticity are important. Embedding watermarks in 2D codes, which allows simultaneous recognition and tamper detection by simply analyzing the captured pattern without requiring an additional device is considered a promising solution. However, smartphone cameras frequently suffer misfocus especially if the target object is too close to the lens, which makes the captured image defocused and results in failure to read watermarks. In this paper, we propose the use of a coded aperture imaging technique to recover watermarks. We have designed a coded aperture that is robust against defocus blur by optimizing the aperture pattern using a genetic algorithm. In addition, we have developed a programmable coded aperture that includes an actual optical process that works in an optimization loop; thus, the complicated effects of the optical aberrations can be considered. Experimental results demonstrate that the proposed method can extend the depth of field for watermark extraction to 3.1 times wider than that of a general circular aperture.},
author = {Hamasaki, H. and Takeshita, S. and Nakai, K. and Sonoda, T. and Kawasaki, H. and Nagahara, H. and Ono, S.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-20876-9_15},
isbn = {9783030208752},
issn = {16113349},
keywords = {Coded aperture,Device-based optimization,Digital image watermark,Extended depth of field,Genetic algorithm,Two-dimensional code},
title = {{A Coded Aperture for Watermark Extraction from Defocused Images}},
volume = {11366 LNCS},
year = {2019}
}
@inproceedings{Yang2020,
abstract = {Visual question answering (VQA) aims at answering questions about the visual content of an image or a video. Currently, most work on VQA is focused on image-based question answering, and less attention has been paid into answering questions about videos. However, VQA in video presents some unique challenges that are worth studying: it not only requires to model a sequence of visual features over time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a sequential modelling technique based on Transformers, to encode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pretrained language-based Transformer. In our experiments, we exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two well-known video VQA datasets: TVQA and Pororo.},
author = {Yang, Zekun and Garcia, Noa and Chu, Chenhui and Otani, Mayu and Nakashima, Yuta and Takemura, Haruo},
booktitle = {Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020},
doi = {10.1109/WACV45572.2020.9093596},
isbn = {9781728165530},
keywords = {kvqa},
mendeley-tags = {kvqa},
title = {{BERT representations for video question answering}},
year = {2020}
}
@article{Garcia2020,
abstract = {{\textcopyright} 2019, The Author(s). In automatic art analysis, models that besides the visual elements of an artwork represent the relationships between the different artistic attributes could be very informative. Those kinds of relationships, however, usually appear in a very subtle way, being extremely difficult to detect with standard convolutional neural networks. In this work, we propose to capture contextual artistic information from fine-art paintings with a specific ContextNet network. As context can be obtained from multiple sources, we explore two modalities of ContextNets: one based on multitask learning and another one based on knowledge graphs. Once the contextual information is obtained, we use it to enhance visual representations computed with a neural network. In this way, we are able to (1) capture information about the content and the style with the visual representations and (2) encode relationships between different artistic attributes with the ContextNet. We evaluate our models on both painting classification and retrieval, and by visualising the resulting embeddings on a knowledge graph, we can confirm that our models represent specific stylistic aspects present in the data.},
author = {Garcia, Noa and Renoust, Benjamin and Nakashima, Yuta},
doi = {10.1007/s13735-019-00189-4},
journal = {International Journal of Multimedia Information Retrieval},
keywords = {Art classification,Knowledge graphs,Multi-modal retrieval,Multitask learning,Visualisation,buddha,kvqa},
mendeley-tags = {buddha,kvqa},
number = {1},
title = {{ContextNet: representation and exploration for painting classification and retrieval in context}},
volume = {9},
year = {2020}
}
@inproceedings{Garcia2019a,
abstract = {{\textcopyright} 2019 Association for Computing Machinery. Automatic art analysis aims to classify and retrieve artistic representations from a collection of images by using computer vision and machine learning techniques. In this work, we propose to enhance visual representations from neural networks with contextual artistic information. Whereas visual representations are able to capture information about the content and the style of an artwork, our proposed context-aware embeddings additionally encode relationships between different artistic attributes, such as author, school, or historical period. We design two different approaches for using context in automatic art analysis. In the first one, contextual data is obtained through a multi-task learning model, in which several attributes are trained together to find visual relationships between elements. In the second approach, context is obtained through an art-specific knowledge graph, which encodes relationships between artistic attributes. An exhaustive evaluation of both of our models in several art analysis problems, such as author identification, type classification, or cross-modal retrieval, show that performance is improved by up to 7.3{\%} in art classification and 37.24{\%} in retrieval when context-aware embeddings are used.},
author = {Garcia, Noa and Renoust, Benjamin and Nakashima, Yuta},
booktitle = {Proceedings of the 2019 ACM International Conference on Multimedia Retrieval (ICMR)},
doi = {10.1145/3323873.3325028},
isbn = {9781450367653},
keywords = {Art classification,Knowledge graphs,Multi-modal retrieval,buddha,kbqa},
mendeley-tags = {buddha,kbqa},
title = {{Context-aware embeddings for automatic art analysis}},
year = {2019}
}
@inproceedings{Li2020a,
abstract = {Retinal imaging serves as a valuable tool for diagnosis of various diseases. However, reading retinal images is a difficult and time-consuming task even for experienced specialists. The fundamental step towards automated retinal image analysis is vessel segmentation and artery/vein classification, which provide various information on potential disorders. To improve the performance of the existing automated methods for retinal image analysis, we propose a two-step vessel classification. We adopt a UNet-based model, SeqNet, to accurately segment vessels from the background and make prediction on the vessel type. Our model does segmentation and classification sequentially, which alleviates the problem of label distribution bias and facilitates training. To further refine classification results, we post-process them considering the structural information among vessels to propagate highly confident prediction to surrounding vessels. Our experiments show that our method improves AUC to 0.98 for segmentation and the accuracy to 0.92 in classification over DRIVE dataset.},
archivePrefix = {arXiv},
arxivId = {2005.13337},
author = {Li, Liangzhi and Verma, Manisha and Nakashima, Yuta and Kawasaki, Ryo and Nagahara, Hajime},
booktitle = {Medical Imaging with Deep Learning (MIDL)},
eprint = {2005.13337},
file = {::},
keywords = {Medical imaging,computer vision,deep learning,retina images,vessel classification,vessel segmentation},
month = {jan},
title = {{Joint learning of vessel segmentation and artery/vein classification with post-processing}},
url = {https://www.liangzhili.com/publication/li-2020-joint/},
year = {2020}
}
@article{Kimura2020,
abstract = {{\textcopyright} Copyright {\textcopyright} 2020 Kimura, Takemura, Nakashima, Kobori, Nagahara, Numao and Shinohara. Climate change is one of the most important issues for humanity. To defuse this problem, it is considered necessary to improve energy efficiency, make energy sources cleaner, and reduce energy consumption in urban areas. The Japanese government has recommended an air conditioner setting of 28°C in summer and 20°C in winter since 2005. The aim of this setting is to save energy by keeping room temperatures constant. However, it is unclear whether this is an appropriate temperature for workers and students. This study examined whether thermal environments influence task performance over time. To examine whether the relationship between task performance and thermal environments influences the psychological states of participants, we recorded their subjective rating of mental workload along with their working memory score, electroencephalogram (EEG), heart rate variability, skin conductance level (SCL), and tympanum temperature during the task and compared the results among different conditions. In this experiment, participants were asked to read some texts and answer questions related to those texts. Room temperature (18, 22, 25, or 29°C) and humidity (50{\%}) were manipulated during the task and participants performed the task at these temperatures. The results of this study showed that the temporal cost of task and theta power of EEG, which is an index for concentration, decreased over time. However, subjective mental workload increased with time. Moreover, the low frequency to high frequency ratio and SCL increased with time and heat (25 and 29°C). These results suggest that mental workload, especially implicit mental workload, increases in warmer environments, even if learning efficiency is facilitated. This study indicates integrated evidence for relationships among task performance, psychological state, and thermal environment by analyzing behavioral, subjective, and physiological indexes multidirectionally.},
author = {Kimura, T. and Takemura, N. and Nakashima, Y. and Kobori, H. and Nagahara, H. and Numao, M. and Shinohara, K.},
doi = {10.3389/fpsyg.2020.00568},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {EEG,autonomic nervous system,learning efficiency,mental workload,thermal environment},
title = {{Warmer Environments Increase Implicit Mental Workload Even If Learning Efficiency Is Enhanced}},
volume = {11},
year = {2020}
}
@inproceedings{Rongsirigul2017,
abstract = {The proliferation of off-the-shelf head-mounted displays (HMDs) let end-users enjoy virtual reality applications, some of which render a real-world scene using a novel view synthesis (NVS) technique. View-dependent texture mapping (VDTM) has been studied for NVS due to its photo-realistic quality. The VDTM technique renders a novel view by adaptively selecting textures from the most appropriate images. However, this process is computationally expensive because VDTM scans every captured image. For stereoscopic HMDs, the situation is much worse because we need to render novel views once for each eye, almost doubling the cost. This paper proposes light-weight VDTM tailored for an HMD. In order to reduce the computational cost in VDTM, our method leverages the overlapping fields of view between a stereoscopic pair of HMD images and pruning the images to be scanned. We show that the proposed method drastically accelerates the VDTM process without spoiling the image quality through a user study.},
author = {Rongsirigul, Thiwat and Nakashima, Yuta and Sato, Tomokazu and Yokoya, Naokazu},
booktitle = {Proceedings - IEEE International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2017.8019417},
isbn = {9781509060672},
issn = {1945788X},
keywords = {Head-mounted displays,Image-based rendering,Novel view synthesis},
month = {aug},
pages = {703--708},
publisher = {IEEE Computer Society},
title = {{Novel view synthesis with light-weight view-dependent texture mapping for a stereoscopic HMD}},
year = {2017}
}
@article{Ashihara2019a,
abstract = {Currently, distributed word representations are employed in many natural language processing tasks. However, when generating one representation for each word, the meanings of a polysemous word cannot be differentiated because the meanings are integrated into one representation. Therefore, several attempts have been made to generate different representations per meaning based on parts of speech or the topic of a sentence. However, these methods are too unrefined to deal with polysemy. In this paper, we proposed two methods to generate more subtle multiple word representations. The first method involves generating multiple word representations using the word in a dependency relationship as a clue. The second approach involves employing a bi-directional language model in which a word representation that considers all the words in the context is generated. The results of the extensive evaluation of the Lexical Substitution task and Context-Aware Word Similarity task confirmed the effectiveness of our approaches to generate more subtle multiple word representations.},
author = {Ashihara, Kazuki and Kajiwara, Tomoyuki and Arase, Yuki and Uchida, Satoru},
doi = {10.5715/jnlp.26.689},
journal = {Journal of Natural Language Processing},
month = {dec},
number = {4},
pages = {689--710},
title = {{Contextualized multi-sense word embedding}},
url = {https://www.jstage.jst.go.jp/article/jnlp/26/4/26{\_}689/{\_}article/-char/ja/},
volume = {26},
year = {2019}
}
@inproceedings{Kajiwara2020,
abstract = {Paraphrase generation can be regarded as monolingual translation. Unlike bilingual machine translation, paraphrase generation rewrites only a limited portion of an input sentence. Hence, previous methods based on machine translation often perform conservatively to fail to make necessary rewrites. To solve this problem, we propose a neural model for paraphrase generation that first identifies words in the source sentence that should be paraphrased. Then, these words are paraphrased by the negative lexically constrained decoding that avoids outputting these words as they are. Experiments on text simplification and formality transfer show that our model improves the quality of paraphrasing by making necessary rewrites to an input sentence.},
address = {Florence, Italy},
author = {Kajiwara, Tomoyuki},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)},
doi = {10.18653/v1/P19-1607},
isbn = {9781950737482},
pages = {6047--6052},
publisher = {Association for Computational Linguistics},
title = {{Negative lexically constrained decoding for paraphrase generation}},
url = {https://www.aclweb.org/anthology/P19-1607/},
year = {2019}
}
@article{Yanagawa2019,
author = {Yanagawa, Masahiro and Niioka, Hirohiko and Hata, Akinori and Kikuchi, Noriko and Honda, Osamu and Kurakami, Hiroyuki and Morii, Eiichi and Noguchi, Masayuki and Watanabe, Yoshiyuki and Miyake, Jun and Tomiyama, Noriyuki},
doi = {10.1097/MD.0000000000016119},
issn = {0025-7974},
journal = {Medicine},
month = {jun},
number = {25},
pages = {e16119},
title = {{Application of deep learning (3-dimensional convolutional neural network) for the prediction of pathological invasiveness in lung adenocarcinoma}},
url = {http://journals.lww.com/00005792-201906210-00044},
volume = {98},
year = {2019}
}
