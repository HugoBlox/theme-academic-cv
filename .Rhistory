fit_text_tokenizer(text)
#num reviews (after removing rating=3)
print(tokenizer$document_count)
#list of integers for reviews
text_seqs<- texts_to_sequences(tokenizer, text)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$positive
length(y_train)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$five_star
length(y_train)
# Network parameters
batch_size <- 32
embedding_dims <- 50
filters <- 64
kernel_size <- 3
hidden_dims <- 50
epochs <- 10
#vocab size same as max_feat (total vocabulary count used for reviews)
vocab_size<-100
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = vocab_size, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% summary()
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
rdf_bin<-rdf %>% mutate(review_num = row_number()) %>%
mutate(five_star = case_when(rating!=5~ 0,TRUE~1)) %>% select(-rating,-stay_date)
rdf_bin %>% ggplot(aes(x=five_star)) +
geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
scale_y_continuous(labels=scales::percent) + scale_x_discrete()
ylab("percent")
# get text of review
text<-rdf_bin$review
max_features <- 500
tokenizer <- text_tokenizer(num_words = max_features)
#fit tokenizer to actual reviews
tokenizer %>%
fit_text_tokenizer(text)
#num reviews
print(tokenizer$document_count)
#list of integers for reviews
text_seqs<- texts_to_sequences(tokenizer, text)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$five_star
length(y_train)
# Network parameters
batch_size <- 32
embedding_dims <- 50
filters <- 64
kernel_size <- 3
hidden_dims <- 50
epochs <- 10
#vocab size same as max_feat (total vocabulary count used for reviews)
vocab_size<-100
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = vocab_size, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% summary()
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
# get text of review
text<-rdf_bin$review
max_features <- 1000
tokenizer <- text_tokenizer(num_words = max_features)
#fit tokenizer to actual reviews
tokenizer %>%
fit_text_tokenizer(text)
#num reviews
print(tokenizer$document_count)
#list of integers for reviews
text_seqs<- texts_to_sequences(tokenizer, text)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$five_star
length(y_train)
# Network parameters
batch_size <- 32
embedding_dims <- 50
filters <- 64
kernel_size <- 3
hidden_dims <- 50
epochs <- 10
# get text of review
text<-rdf_bin$review
max_features <- 500
tokenizer <- text_tokenizer(num_words = max_features)
#fit tokenizer to actual reviews
tokenizer %>%
fit_text_tokenizer(text)
#num reviews
print(tokenizer$document_count)
#list of integers for reviews
text_seqs<- texts_to_sequences(tokenizer, text)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$five_star
length(y_train)
# Network parameters
batch_size <- 32
embedding_dims <- 50
filters <- 64
kernel_size <- 3
hidden_dims <- 50
epochs <- 10
#input max_feat (total vocabulary count used for reviews)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% summary()
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
plot(hist)
#increase num words for embedding
max_features<-1000
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = 100)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = 3) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_small,
y_train,
shuffle=TRUE,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
#increase num words for embedding
max_features<-1000
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = 100)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_small,
y_train,
shuffle=TRUE,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
hist <- model %>%
fit(
x_train_small,
y_train,
batch_size = 32,
epochs = 50,
validation_split = 0.2
)
hist <- model %>%
fit(
x_train_small,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
rdf_bin<-rdf %>% mutate(review_num = row_number()) %>%
mutate(five_star = case_when(rating!=5~ 0,TRUE~1)) %>% select(-rating,-stay_date)
rdf_bin %>% ggplot(aes(x=five_star)) +
geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
scale_y_continuous(labels=scales::percent) + scale_x_discrete()
ylab("percent")
rdf_bin<-rdf %>% mutate(review_num = row_number()) %>%
mutate(five_star = case_when(rating!=5~ 0,TRUE~1)) %>% select(-rating,-stay_date)
rdf_bin %>% ggplot(aes(x=five_star)) +
geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
scale_y_continuous(labels=scales::percent) + scale_x_discrete()+
ylab("percent")
# get text of review
text<-rdf_bin$review
max_features <- 500
tokenizer <- text_tokenizer(num_words = max_features)
#fit tokenizer to actual reviews
tokenizer %>%
fit_text_tokenizer(text)
#num reviews
print(tokenizer$document_count)
#list of integers for reviews
text_seqs<- texts_to_sequences(tokenizer, text)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$five_star
length(y_train)
# Network parameters
batch_size <- 32
embedding_dims <- 50
filters <- 64
kernel_size <- 5
hidden_dims <- 50
epochs <- 10
#input max_feat (total vocabulary count used for reviews:500)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% summary()
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
rdf_bin<-rdf %>% mutate(review_num = row_number()) %>%
mutate(five_star = case_when(rating!=5~ 0,TRUE~1)) %>% select(-rating,-stay_date)
rdf_bin %>% ggplot(aes(x=five_star)) +
geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
scale_y_continuous(labels=scales::percent) + scale_x_discrete()+
ylab("percent")
# get text of review
text<-rdf_bin$review
max_features <- 1000
tokenizer <- text_tokenizer(num_words = max_features)
#fit tokenizer to actual reviews
tokenizer %>%
fit_text_tokenizer(text)
#num reviews
print(tokenizer$document_count)
#list of integers for reviews
text_seqs<- texts_to_sequences(tokenizer, text)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$five_star
length(y_train)
# Network parameters
batch_size <- 32
embedding_dims <- 50
filters <- 64
kernel_size <- 5
hidden_dims <- 50
epochs <- 10
#input max_feat (total vocabulary count used for reviews:500)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% summary()
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
plot(hist)
#increase num words for embedding
max_features<-1000
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = 100)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
#increase num words for embedding
max_features<-5000
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = 400)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_small,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
#increase num words for embedding
max_features<-5000
maxlen<-400
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = maxlen)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_small,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
plot(hist)
#increase num words for embedding
max_features<-5000
maxlen<-400
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = maxlen)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_small,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
plot(hist)
library(blogdown)
serve_site()
