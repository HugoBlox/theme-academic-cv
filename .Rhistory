#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = 100)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = 3) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_small,
y_train,
shuffle=TRUE,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
#increase num words for embedding
max_features<-1000
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = 100)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_small,
y_train,
shuffle=TRUE,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
hist <- model %>%
fit(
x_train_small,
y_train,
batch_size = 32,
epochs = 50,
validation_split = 0.2
)
hist <- model %>%
fit(
x_train_small,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
rdf_bin<-rdf %>% mutate(review_num = row_number()) %>%
mutate(five_star = case_when(rating!=5~ 0,TRUE~1)) %>% select(-rating,-stay_date)
rdf_bin %>% ggplot(aes(x=five_star)) +
geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
scale_y_continuous(labels=scales::percent) + scale_x_discrete()
ylab("percent")
rdf_bin<-rdf %>% mutate(review_num = row_number()) %>%
mutate(five_star = case_when(rating!=5~ 0,TRUE~1)) %>% select(-rating,-stay_date)
rdf_bin %>% ggplot(aes(x=five_star)) +
geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
scale_y_continuous(labels=scales::percent) + scale_x_discrete()+
ylab("percent")
# get text of review
text<-rdf_bin$review
max_features <- 500
tokenizer <- text_tokenizer(num_words = max_features)
#fit tokenizer to actual reviews
tokenizer %>%
fit_text_tokenizer(text)
#num reviews
print(tokenizer$document_count)
#list of integers for reviews
text_seqs<- texts_to_sequences(tokenizer, text)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$five_star
length(y_train)
# Network parameters
batch_size <- 32
embedding_dims <- 50
filters <- 64
kernel_size <- 5
hidden_dims <- 50
epochs <- 10
#input max_feat (total vocabulary count used for reviews:500)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% summary()
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
rdf_bin<-rdf %>% mutate(review_num = row_number()) %>%
mutate(five_star = case_when(rating!=5~ 0,TRUE~1)) %>% select(-rating,-stay_date)
rdf_bin %>% ggplot(aes(x=five_star)) +
geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
scale_y_continuous(labels=scales::percent) + scale_x_discrete()+
ylab("percent")
# get text of review
text<-rdf_bin$review
max_features <- 1000
tokenizer <- text_tokenizer(num_words = max_features)
#fit tokenizer to actual reviews
tokenizer %>%
fit_text_tokenizer(text)
#num reviews
print(tokenizer$document_count)
#list of integers for reviews
text_seqs<- texts_to_sequences(tokenizer, text)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$five_star
length(y_train)
# Network parameters
batch_size <- 32
embedding_dims <- 50
filters <- 64
kernel_size <- 5
hidden_dims <- 50
epochs <- 10
#input max_feat (total vocabulary count used for reviews:500)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% summary()
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
plot(hist)
#increase num words for embedding
max_features<-1000
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = 100)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
#increase num words for embedding
max_features<-5000
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = 400)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_small,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
#increase num words for embedding
max_features<-5000
maxlen<-400
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = maxlen)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_small,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
plot(hist)
#increase num words for embedding
max_features<-5000
maxlen<-400
#increase length of review considered
x_train_small <- pad_sequences(text_seqs, maxlen = maxlen)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_small,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
plot(hist)
library(blogdown)
serve_site()
library(tidyverse)
library(zoo)
library(knitr)
library(tidytext)
library(SnowballC)
library(keras)
library(kableExtra)
#load data
rdf<-readRDS("first_3000.Rda")
str(rdf)
#rename & change variable type
rdf <- rdf %>% rename(stay_date = id_date,
review = reviews) %>%
mutate_at(vars(stay_date, review),
~ as.character(.)) %>%
mutate(rating = as.factor(rating))
#change stay_date variable from character to date type
rdf$stay_date <- gsub("Date of stay: ", "", rdf$stay_date)
rdf$stay_date <- as.yearmon(rdf$stay_date, "%b %Y")
kable(head(rdf,3)) %>% kable_styling(bootstrap_options = "striped", full_width = F,font_size = 8)
#set seed
set.seed(42)
#shuffle dataframe
rows <- sample(nrow(rdf))
rdf <- rdf[rows, ]
rdf_bin <- rdf %>% mutate(review_num = row_number()) %>%
mutate(five_star = case_when(rating != 5 ~ 0, TRUE ~ 1)) %>% select(-rating, -stay_date)
rdf_bin %>% ggplot(aes(x = five_star)) +
geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat = "count") +
scale_y_continuous(labels = scales::percent) + ylab("percent") +
scale_x_discrete()
# get text of review
text<-rdf_bin$review
max_features <- 1000
tokenizer <- text_tokenizer(num_words = max_features)
#fit tokenizer to actual reviews
tokenizer %>%
fit_text_tokenizer(text)
#num reviews
print(tokenizer$document_count)
#list of integers for reviews
text_seqs<- texts_to_sequences(tokenizer, text)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$five_star
length(y_train)
# Network parameters
embedding_dims <- 50
filters <- 64
kernel_size <- 5
hidden_dims <- 50
epochs <- 10
#increase num words for embedding
max_features<-5000
maxlen<-400
#increase length of review considered
x_train_long <- pad_sequences(text_seqs, maxlen = maxlen)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_long,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
str(hist)
plot(hist)
print(hist)
hist <- model %>%
fit(
x_train,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
# get text of review
text<-rdf_bin$review
max_features <- 1000
tokenizer <- text_tokenizer(num_words = max_features)
#fit tokenizer to actual reviews
tokenizer %>%
fit_text_tokenizer(text)
#num reviews
print(tokenizer$document_count)
#list of integers for reviews
text_seqs<- texts_to_sequences(tokenizer, text)
#cut reviews off after 100 words
maxlen <- 100
#define train data
x_train <- pad_sequences(text_seqs, maxlen = maxlen)
dim(x_train)
y_train <- rdf_bin$five_star
length(y_train)
# Network parameters
embedding_dims <- 50
filters <- 64
kernel_size <- 5
hidden_dims <- 50
epochs <- 10
#input max_feat (total vocabulary count used for reviews:1000)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% summary()
model %>% compile(
loss = "binary_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
print(hist)
plot(hist)
#increase num words for embedding
max_features<-5000
maxlen<-400
#increase length of review considered
x_train_long <- pad_sequences(text_seqs, maxlen = maxlen)
model <- keras_model_sequential() %>%
#embedding layer
layer_embedding(input_dim = max_features, output_dim =embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units=hidden_dims, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units=1,activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
hist <- model %>%
fit(
x_train_long,
y_train,
batch_size = 32,
epochs = 10,
validation_split = 0.2
)
print(hist)
plot(hist)
library(blogdown)
serve_site()
install.packages('data.table')
?fread
?fread()
getwd()
orders <- fread('../instacart/orders.csv',stringsAsFactors=TRUE)
library(data.table)
install.packages("data.table")
install.packages("data.table", type = "source",
repos = "http://Rdatatable.github.io/data.table")
install.packages("data.table", repos="https://Rdatatable.github.io/data.table")
getwd()
getwd()
