waitLabels <- mrides_long_numeric %>% mutate(wait_time=as.numeric(wait_time)) %>% select(wait_time)
waitLabels<-as.vector(waitLabels$wait_time)
#one-hot encoding for categorical variables (holidayn,hour,month, etc. )
holiday_name <- model.matrix(~holidayn-1, mrides_long)
hour_mat <- model.matrix(~hour-1, mrides_long)
mon_mat <- model.matrix(~month-1, mrides_long)
year_mat <- model.matrix(~year-1, mrides_long)
ride_mat<- model.matrix(~ride-1, mrides_long)
#column bind existing numeric data with one-hot encoded categorical variables
mrides_long_numeric<- cbind(mrides_long_numeric, holiday_name,hour_mat,mon_mat,year_mat,ride_mat)
#remove outcome variable
mrides_long_num_no_resp<-mrides_long_numeric %>% select(-wait_time)
#convert to numeric & create matrix
mrides_long_num_no_resp<-mrides_long_num_no_resp %>% mutate_if(is.integer,as.numeric)
mrides_matrix <- data.matrix(mrides_long_num_no_resp)
library(mlr)
library(caret)
# Split df into train/test
names(mrides_long_numeric)<-gsub(x=names(mrides_long_numeric),"\\|","_")
idTrain=createDataPartition(y=mrides_long_numeric$wait_time,p=0.7,list=FALSE)
train_df=mrides_long_numeric[idTrain,]
test_df=mrides_long_numeric[-idTrain,]
#convert characters to factors
# fact_col <- colnames(train_df)[sapply(train_df,is.character)]
# for(i in fact_col)
#         set(train_df,j=i,value = factor(train_df[[i]]))
# for(i in fact_col)
#         set(test_df,j=i,value = factor(test_df[[i]]))
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
str(mrides_long_numeric)
meta1$holidayn<-fct_explicit_na(meta1$holidayn, "no_holiday")
mrides<-rides1 %>%
merge(meta1,by="date")%>%
arrange(date,hour) %>% mutate(
hour=as.factor(hour),
month=as.factor(month(date)),
year=as.factor(year(date)))
kable(head(mrides)) %>%
kable_styling(bootstrap_options = "striped", full_width = F,font_size = 8)
library (xgboost)
set.seed(1234)
#select numeric features
mrides_long<-mrides %>% gather(key="ride",value="wait_time",toy_story_mania:soarin) %>% mutate_if(is.factor,as.character)
mrides_long_numeric<-mrides_long %>% select(-date) %>% select_if(is.numeric)
#get vector of training labels
waitLabels <- mrides_long_numeric %>% mutate(wait_time=as.numeric(wait_time)) %>% select(wait_time)
waitLabels<-as.vector(waitLabels$wait_time)
#one-hot encoding for categorical variables (holidayn,hour,month, etc. )
holiday_name <- model.matrix(~holidayn-1, mrides_long)
hour_mat <- model.matrix(~hour-1, mrides_long)
mon_mat <- model.matrix(~month-1, mrides_long)
year_mat <- model.matrix(~year-1, mrides_long)
ride_mat<- model.matrix(~ride-1, mrides_long)
#column bind existing numeric data with one-hot encoded categorical variables
mrides_long_numeric<- cbind(mrides_long_numeric, holiday_name,hour_mat,mon_mat,year_mat,ride_mat)
#remove outcome variable
mrides_long_num_no_resp<-mrides_long_numeric %>% select(-wait_time)
#convert to numeric & create matrix
mrides_long_num_no_resp<-mrides_long_num_no_resp %>% mutate_if(is.integer,as.numeric)
mrides_matrix <- data.matrix(mrides_long_num_no_resp)
library(mlr)
library(caret)
# Split df into train/test
names(mrides_long_numeric)<-gsub(x=names(mrides_long_numeric),"\\|","_")
idTrain=createDataPartition(y=mrides_long_numeric$wait_time,p=0.7,list=FALSE)
train_df=mrides_long_numeric[idTrain,]
test_df=mrides_long_numeric[-idTrain,]
#convert characters to factors
# fact_col <- colnames(train_df)[sapply(train_df,is.character)]
# for(i in fact_col)
#         set(train_df,j=i,value = factor(train_df[[i]]))
# for(i in fact_col)
#         set(test_df,j=i,value = factor(test_df[[i]]))
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
str(mrides_long_numeric)
library(tidyverse)
library(lubridate)
library(skimr)
library(ModelMetrics)
library(janitor)
library(broom)
library(forcats)
library(kableExtra)
library(stringr)
library(tidyverse)
library(lubridate)
library(skimr)
library(ModelMetrics)
library(janitor)
library(broom)
library(forcats)
library(kableExtra)
library(stringr)
rides1<-merged_rides%>%
mutate(hour=hour(datetime),
date=date(datetime)) %>%
group_by(hour,date)%>% filter(hour<21 & hour>8) %>%
summarize_at(
vars(toy_story_mania:soarin),
~mean(.,na.rm =TRUE)
)
avg_by_month <- rides1 %>%
mutate(month = month(date, label = TRUE)) %>%
group_by(month) %>%
summarize_at(vars(toy_story_mania:soarin),
~ mean(., na.rm = TRUE))
avg_by_month %>% gather(key="ride",value="avg_wait",-month,) %>%
ggplot(aes(x=month,y=avg_wait,color=ride)) + geom_line(aes(group = ride)) +
geom_point() + xlab("Month")
##selecting features to explore
meta1<-meta %>% select(DATE, contains('event'),-contains('eventN'),contains('HOLIDAY'),-contains('DAYS'),-contains('STREAK'),-HOLIDAYJ)
meta1$date<-as.character(meta1$DATE)
meta1$date <-ymd(mdy(meta1$date))
meta1$DATE<-NULL
meta1<-meta1 %>% clean_names()
meta1$holidayn<-fct_explicit_na(meta1$holidayn, "no_holiday")
mrides<-rides1 %>%
merge(meta1,by="date")%>%
arrange(date,hour) %>% mutate(
hour=as.factor(hour),
month=as.factor(month(date)),
year=as.factor(year(date)))
kable(head(mrides)) %>%
kable_styling(bootstrap_options = "striped", full_width = F,font_size = 8)
library (xgboost)
set.seed(1234)
#select numeric features
mrides_long<-mrides %>% gather(key="ride",value="wait_time",toy_story_mania:soarin) %>% mutate_if(is.factor,as.character)
mrides_long_numeric<-mrides_long %>% select(-date) %>% select_if(is.numeric)
#get vector of training labels
waitLabels <- mrides_long_numeric %>% mutate(wait_time=as.numeric(wait_time)) %>% select(wait_time)
waitLabels<-as.vector(waitLabels$wait_time)
#one-hot encoding for categorical variables (holidayn,hour,month, etc. )
holiday_name <- model.matrix(~holidayn-1, mrides_long)
hour_mat <- model.matrix(~hour-1, mrides_long)
mon_mat <- model.matrix(~month-1, mrides_long)
year_mat <- model.matrix(~year-1, mrides_long)
ride_mat<- model.matrix(~ride-1, mrides_long)
#column bind existing numeric data with one-hot encoded categorical variables
mrides_long_numeric<- cbind(mrides_long_numeric, holiday_name,hour_mat,mon_mat,year_mat,ride_mat)
#remove outcome variable
mrides_long_num_no_resp<-mrides_long_numeric %>% select(-wait_time)
#convert to numeric & create matrix
mrides_long_num_no_resp<-mrides_long_num_no_resp %>% mutate_if(is.integer,as.numeric)
mrides_matrix <- data.matrix(mrides_long_num_no_resp)
str(mrides_long_numeric)
library(mlr)
library(caret)
# Split df into train/test
names(mrides_long_numeric)<-gsub(x=names(mrides_long_numeric),"\\|","_")
idTrain=createDataPartition(y=mrides_long_numeric$wait_time,p=0.7,list=FALSE)
train_df=mrides_long_numeric[idTrain,]
test_df=mrides_long_numeric[-idTrain,]
str(mrides_long_numeric)
#convert characters to factors
# fact_col <- colnames(train_df)[sapply(train_df,is.character)]
# for(i in fact_col)
#         set(train_df,j=i,value = factor(train_df[[i]]))
# for(i in fact_col)
#         set(test_df,j=i,value = factor(test_df[[i]]))
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
test_task = makeRegrTask(data = test_df, target = "wait_time")
#learner
xgb_learner <- makeLearner(
"regr.xgboost", predict.type = "response", eval_metric="mae")
#tune eta hyper-param
xgb_params <- makeParamSet(makeIntegerParam("nrounds", lower = 100, upper = 300),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
control <- makeTuneControlRandom(maxit = 1)
# Choose a resampling strategy
resample_desc = makeResampleDesc("CV", iters = 2)
# tuning
tuned_params <- tuneParams(
learner = xgb_learner,
task = train_task,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
xgb_tuned_param <- setHyperPars(
learner = xgb_learner,
par.vals = tuned_params$x
)
print(xgb_tuned_param)
result <- predict(xgb_model, test_task)
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
test_task = makeRegrTask(data = test_df, target = "wait_time")
#learner
xgb_learner <- makeLearner(
"regr.xgboost", predict.type = "response", eval_metric="mae")
xgb_model <- train(xgb_learner, task = train_task)
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
test_task = makeRegrTask(data = test_df, target = "wait_time")
#learner
xgb_learner <- makeLearner(
"regr.xgboost", predict.type = "response", eval_metric="mae")
xgb_model <- mlr::train(xgb_learner, task = train_task)
result <- predict(xgb_model, test_task)
performance(result,measures = mae)
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
test_task = makeRegrTask(data = test_df, target = "wait_time")
#learner
xgb_learner <- makeLearner(
"regr.xgboost", predict.type = "response", eval_metric="mae")
xgb_model <- mlr::train(xgb_learner, task = train_task)
result <- predict(xgb_model, test_task)
performance(result,measures = mae)
print(xgb_learner$par.vals)
print(xgb_learner$par.vals)
getHyperPars(xgb_learner)
getHyperPars(xgb_learner)
# To see all the parameters of the xgboost classifier
getParamSet("regr.xgboost")
# To see all the parameters of the xgboost classifier
str(getParamSet("regr.xgboost"))
# To see all the parameters of the xgboost regr
getParamSet("regr.xgboost")
xgb_params <- makeParamSet(makeIntegerParam("nrounds", lower = 100, upper = 300),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
control <- makeTuneControlRandom(maxit = 1)
# Choose a resampling strategy
resample_desc = makeResampleDesc("CV", iters = 2)
# tuning
tuned_params <- tuneParams(
learner = xgb_learner,
task = train_task,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
xgb_tuned_learner<- setHyperPars(
learner = xgb_learner,
par.vals = tuned_params$x
)
print(xgb_tuned_learner)
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
test_task = makeRegrTask(data = test_df, target = "wait_time")
#learner
xgb_learner <- makeLearner(
"regr.xgboost", predict.type = "response", eval_metric="mae",verbose=1)
xgb_model <- mlr::train(xgb_learner, task = train_task)
result <- predict(xgb_model, test_task)
performance(result,measures = mae)
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
test_task = makeRegrTask(data = test_df, target = "wait_time")
#learner
xgb_learner <- makeLearner(
"regr.xgboost", predict.type = "response", eval_metric="mae",verbose=2)
xgb_model <- mlr::train(xgb_learner, task = train_task)
result <- predict(xgb_model, test_task)
performance(result,measures = mae)
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
test_task = makeRegrTask(data = test_df, target = "wait_time")
#learner
xgb_learner <- makeLearner(
"regr.xgboost", predict.type = "response", eval_metric="mae")
xgb_model <- mlr::train(xgb_learner, task = train_task)
result <- predict(xgb_model, test_task)
performance(result,measures = mae)
library(mlr)
library(caret)
set.seed(1)
# Split df into train/test
names(mrides_long_numeric)<-gsub(x=names(mrides_long_numeric),"\\|","_")
idTrain=createDataPartition(y=mrides_long_numeric$wait_time,p=0.7,list=FALSE)
train_df=mrides_long_numeric[idTrain,]
test_df=mrides_long_numeric[-idTrain,]
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
test_task = makeRegrTask(data = test_df, target = "wait_time")
#learner
xgb_learner <- makeLearner(
"regr.xgboost", predict.type = "response", eval_metric="mae")
xgb_model <- mlr::train(xgb_learner, task = train_task)
result <- predict(xgb_model, test_task)
performance(result,measures = mae)
xgb_params <- makeParamSet(makeIntegerParam("nrounds", lower = 100, upper = 300),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
control <- makeTuneControlRandom(maxit = 1)
# Choose a resampling strategy
resample_desc = makeResampleDesc("CV", iters = 2)
# tuning
tuned_params <- tuneParams(
learner = xgb_learner,
task = train_task,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
xgb_tuned_learner<- setHyperPars(
learner = xgb_learner,
par.vals = tuned_params$x
)
print(xgb_tuned_learner)
# Re-train parameters using tuned hyperparameters (and full training set)
xgb_tuned_model <- mlr::train(xgb_tuned_learner, train_task)
# Make a new prediction
result_tuned <- predict(xgb_tuned_model, test_task)
performance(result,measures = mae)
result_tuned <- predict(xgb_tuned_model, test_task)
performance(result,measures = mae)
result_tuned <- predict(xgb_tuned_model, test_task)
performance(result_tuned,measures = mae)
hyp_effect<-generateHyperParsEffectData(tuned_params)
hyp_effect<-generateHyperParsEffectData(tuned_params,partial.dep = TRUE)
plotHyperParsEffect(hyp_effect)
hyp_effect
hyp_effect<-generateHyperParsEffectData(tuned_params,partial.dep = TRUE)
hyp_effect
plotHyperParsEffect(hyp_effect,x="nrounds",y="mse.test.mean")
?plotHyperParsEffect
hyp_effect<-generateHyperParsEffectData(tuned_params,partial.dep = TRUE)
plotHyperParsEffect(hyp_effect,x="nrounds",y="mse.test.mean",partial.dep.learn = xgb_tuned_learner)
install.packages("mmpf")
hyp_effect<-generateHyperParsEffectData(tuned_params,partial.dep = TRUE)
plotHyperParsEffect(hyp_effect,x="nrounds",y="mse.test.mean",partial.dep.learn = xgb_tuned_learner)
hyp_effect<-generateHyperParsEffectData(tuned_params,partial.dep = TRUE)
plotHyperParsEffect(hyp_effect,x=hyp_effect$data,y="mse.test.mean",partial.dep.learn = xgb_tuned_learner)
hyp_effect<-generateHyperParsEffectData(tuned_params,partial.dep = TRUE)
plotHyperParsEffect(hyp_effect,x="eta",y="mse.test.mean",partial.dep.learn = xgb_tuned_learner)
library(blogdown)
build_site()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(skimr)
library(ModelMetrics)
library(janitor)
library(broom)
library(forcats)
library(kableExtra)
library(stringr)
library (xgboost)
set.seed(1234)
#select numeric features
mrides_long<-mrides %>% gather(key="ride",value="wait_time",toy_story_mania:soarin) %>% mutate_if(is.factor,as.character)
mrides_long_numeric<-mrides_long %>% select(-date) %>% select_if(is.numeric)
#get vector of training labels
waitLabels <- mrides_long_numeric %>% mutate(wait_time=as.numeric(wait_time)) %>% select(wait_time)
waitLabels<-as.vector(waitLabels$wait_time)
#one-hot encoding for categorical variables (holidayn,hour,month, etc. )
holiday_name <- model.matrix(~holidayn-1, mrides_long)
hour_mat <- model.matrix(~hour-1, mrides_long)
mon_mat <- model.matrix(~month-1, mrides_long)
year_mat <- model.matrix(~year-1, mrides_long)
ride_mat<- model.matrix(~ride-1, mrides_long)
#column bind existing numeric data with one-hot encoded categorical variables
mrides_long_numeric<- cbind(mrides_long_numeric, holiday_name,hour_mat,mon_mat,year_mat,ride_mat)
#remove outcome variable
mrides_long_num_no_resp<-mrides_long_numeric %>% select(-wait_time)
#convert to numeric & create matrix
mrides_long_num_no_resp<-mrides_long_num_no_resp %>% mutate_if(is.integer,as.numeric)
mrides_matrix <- data.matrix(mrides_long_num_no_resp)
library(mlr)
library(caret)
set.seed(1)
# Split df into train/test
names(mrides_long_numeric)<-gsub(x=names(mrides_long_numeric),"\\|","_")
idTrain=createDataPartition(y=mrides_long_numeric$wait_time,p=0.7,list=FALSE)
train_df=mrides_long_numeric[idTrain,]
test_df=mrides_long_numeric[-idTrain,]
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
##selecting features to explore
meta1<-meta %>% select(DATE, contains('event'),-contains('eventN'),contains('HOLIDAY'),-contains('DAYS'),-contains('STREAK'),-HOLIDAYJ)
meta1$date<-as.character(meta1$DATE)
meta1$date <-ymd(mdy(meta1$date))
meta1$DATE<-NULL
meta1<-meta1 %>% clean_names()
meta1$holidayn<-fct_explicit_na(meta1$holidayn, "no_holiday")
mrides<-rides1 %>%
merge(meta1,by="date")%>%
arrange(date,hour) %>% mutate(
hour=as.factor(hour),
month=as.factor(month(date)),
year=as.factor(year(date)))
kable(head(mrides)) %>%
kable_styling(bootstrap_options = "striped", full_width = F,font_size = 8)
library (xgboost)
set.seed(1234)
#select numeric features
mrides_long<-mrides %>% gather(key="ride",value="wait_time",toy_story_mania:soarin) %>% mutate_if(is.factor,as.character)
mrides_long_numeric<-mrides_long %>% select(-date) %>% select_if(is.numeric)
#get vector of training labels
waitLabels <- mrides_long_numeric %>% mutate(wait_time=as.numeric(wait_time)) %>% select(wait_time)
waitLabels<-as.vector(waitLabels$wait_time)
#one-hot encoding for categorical variables (holidayn,hour,month, etc. )
holiday_name <- model.matrix(~holidayn-1, mrides_long)
hour_mat <- model.matrix(~hour-1, mrides_long)
mon_mat <- model.matrix(~month-1, mrides_long)
year_mat <- model.matrix(~year-1, mrides_long)
ride_mat<- model.matrix(~ride-1, mrides_long)
#column bind existing numeric data with one-hot encoded categorical variables
mrides_long_numeric<- cbind(mrides_long_numeric, holiday_name,hour_mat,mon_mat,year_mat,ride_mat)
#remove outcome variable
mrides_long_num_no_resp<-mrides_long_numeric %>% select(-wait_time)
#convert to numeric & create matrix
mrides_long_num_no_resp<-mrides_long_num_no_resp %>% mutate_if(is.integer,as.numeric)
mrides_matrix <- data.matrix(mrides_long_num_no_resp)
library(mlr)
library(caret)
set.seed(1)
# Split df into train/test
names(mrides_long_numeric)<-gsub(x=names(mrides_long_numeric),"\\|","_")
idTrain=createDataPartition(y=mrides_long_numeric$wait_time,p=0.7,list=FALSE)
train_df=mrides_long_numeric[idTrain,]
test_df=mrides_long_numeric[-idTrain,]
#define task: regression, predict wait_time
train_task = makeRegrTask(data = train_df, target = "wait_time")
test_task = makeRegrTask(data = test_df, target = "wait_time")
#learner
xgb_learner <- makeLearner(
"regr.xgboost", predict.type = "response", eval_metric="mae")
xgb_model <- mlr::train(xgb_learner, task = train_task)
result <- predict(xgb_model, test_task)
performance(result,measures = mae)
xgb_params <- makeParamSet(makeIntegerParam("nrounds", lower = 100, upper = 300),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
control <- makeTuneControlRandom(maxit = 1)
# Choose a resampling strategy
resample_desc = makeResampleDesc("CV", iters = 2)
# tuning
tuned_params <- tuneParams(
learner = xgb_learner,
task = train_task,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
xgb_tuned_learner<- setHyperPars(
learner = xgb_learner,
par.vals = tuned_params$x
)
print(xgb_tuned_learner)
# Re-train parameters using tuned hyperparameters (and full training set)
xgb_tuned_model <- mlr::train(xgb_tuned_learner, train_task)
# Make a new prediction
result_tuned <- predict(xgb_tuned_model, test_task)
performance(result_tuned,measures = mae)
getFeatureImportance(xgb_tuned_model)
feat_im<-getFeatureImportance(xgb_tuned_model)
feat_im %>% gather(key="feature",value="importance")
feat_im<-getFeatureImportance(xgb_tuned_model)
getFeatureImportance(xgb_tuned_model)
feat_imp<-getFeatureImportance(xgb_tuned_model)
feat_imp
feat_imp$data.frame
feat_imp<-getFeatureImportance(xgb_tuned_model)
feat_imp$res
feat_imp$res %>% gather(key="feature",value="importance")
feat_imp<-getFeatureImportance(xgb_tuned_model)
feat_imp$res %>% gather(key="feature",value="importance") %>% ggplot(x=feature,y=importance) %>% geom_col()
feat_imp$res %>% gather(key="feature",value="importance") %>% ggplot(aes(x=feature,y=importance)) %>% geom_col()
feat_imp$res %>% gather(key="feature",value="importance") %>% ggplot(aes(x=feature,y=importance)) + geom_col()
feat_imp$res %>% gather(key="feature",value="importance") %>% ggplot(aes(x=feature,y=importance)) + geom_col()+coord_flip()
feat_imp$res %>% gather(key="feature",value="importance") %>% arrange(importance) %>% top_n(20,importance) %>% ggplot(aes(x=feature,y=importance)) + geom_col()+coord_flip()
feat_imp$res %>% gather(key="feature",value="importance") %>% top_n(20,importance) %>% ggplot(aes(x=feature,y=reorder(importance,importance))) + geom_col()+coord_flip()
feat_imp<-getFeatureImportance(xgb_tuned_model)
feat_imp$res %>% gather(key="feature",value="importance") %>% top_n(20,importance) %>% ggplot(aes(x=feature,y=importance)) + geom_col()+coord_flip()
feat_imp<-getFeatureImportance(xgb_tuned_model)
#plot top 20 features
feat_imp$res %>% gather(key="feature",value="importance") %>% top_n(20,importance) %>% ggplot(aes(x=fct_reorder(feature,importance))) + geom_col()+coord_flip()
feat_imp<-getFeatureImportance(xgb_tuned_model)
#plot top 20 features
feat_imp$res %>% gather(key="feature",value="importance") %>% top_n(20,importance) %>% ggplot(aes(x=fct_reorder(feature,importance))) + geom_col(aes(y=importance))+coord_flip()
library(blogdown)
build_site()
blogdown:::new_post_addin()
getwd()
library(tidyverse)
library(zoo)
library(knitr)
rdf<-readRDS("first_3000.Rda")
library(tidyverse)
library(zoo)
library(knitr)
rdf<-readRDS("first_3000.Rda")
library(blogdown)
serve_site()
